{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/ComparingNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/projects/project-notebooks/ComparingNetworks.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "# Comparing networks\n",
    "\n",
    "***Comparing networks: Characterizing computational similarity in task-trained recurrent neural networks***\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "\n",
    "__Content creators:__ Chris Versteeg\n",
    "\n",
    "__Content reviewers:__ Chris Versteeg, Hannah Choi, Eva Dyer\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Researchers training artificial networks to perform tasks (e.g., image classification, motor control) have found that the activity in the artificial networks can resemble the activity of biological neurons from brain areas thought to perform similar tasks. Unfortunately, it is unclear whether a superficial similarity in neural activation necessarily translates to a conserved computational strategy. We need ways to assess how well different models are able to capture the computational principles, which will require datasets where the ground-truth computations are known, and we can analyze the similarity between artificial and natural systems. The aim of this project is to explore ways to measure alignment in dynamical systems, and to study different approaches to quantify the changes in representations across different tasks and across different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Project Background\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'KpXu34cvWPE'), ('Bilibili', '')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Project slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "print(\"If you want to download the slides: https://osf.io/download/vb3tw/\")\n",
    "IFrame(src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/vb3tw/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Project Template\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/ComputationalSimilarityTemplate.png?raw=true\"\n",
    "\n",
    "display(Image(url=url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this notebook, we are going to provide code to get you started on Q1-Q3 of this project! \n",
    "\n",
    "The basic outline looks like this:\n",
    "\n",
    "- Section 1: Preparing the environment.\n",
    "- Section 2: Overview of the available tasks.\n",
    "- Section 3: Understanding the Three-Bit Flip-Flop task (3BFF).\n",
    "- Section 4: Training a model to perform 3BFF.\n",
    "- Section 5: Inspecting the performance of trained models.\n",
    "    - Part 1: Visualizing latent activity\n",
    "    - Part 2: Quantifying latent similarity with State R2.\n",
    "    - Part 3: Visualizing Fixed-Point architectures.\n",
    "- Section 6: Introduction to Random Target task.\n",
    "\n",
    "**Importantly, we've put landmarks in the notebook to indicate**: \n",
    "\n",
    "*Interactive exercises*\n",
    "\n",
    "❓❓❓\n",
    "\n",
    "\n",
    "*Cells which will a decent amount of time to run (>5 mins)*\n",
    "\n",
    "⏳⏳⏳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial links**\n",
    "\n",
    "This project is mostly associated with the materials presented in [**W1D3**](https://neuroai.neuromatch.io/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Intro.html), on comparing activities of artificial and biological networks. One of the main techniques here, DSA, completes and empowers your toolbox by enabling dynamic analysis of activity patterns. You will find the [Tutorial 2](https://neuroai.neuromatch.io/tutorials/W1D1_Generalization/student/W1D1_Tutorial2.html) from the **W1D1** the most similar by the model's architecture and the idea of goal-oriented networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1: Preparing the environment\n",
    "\n",
    "**IF USING COLAB // KAGGLE**:\n",
    "\n",
    "Uncomment the lines of code below and run them in order. The top cell only needs to be run once, but the second cell (envStr) needs to be re-run if the Colab // Kaggle notebook crashes. These blocks install the needed dependencies and set up your environment. Notice that third cell content depends on whether you use Colab or Kaggle.\n",
    "\n",
    "⏳⏳⏳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @markdown Colab // Kaggle installation (Part 1)\n",
    "\n",
    "# ! git clone https://github.com/neuromatch/ComputationThruDynamicsBenchmark\n",
    "# %cd ComputationThruDynamicsBenchmark\n",
    "# ! pip install -e .\n",
    "\n",
    "## RUN THIS CELL, THEN RESTART SESSION AS PROMPTED (BUTTON AT BOTTOM OF THIS CELL'S FINISHED OUTPUT). DO NOT NEED TO RUN AGAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @markdown Colab // Kaggle installation (Part 2)\n",
    "\n",
    "# !pip uninstall -y torchaudio torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @markdown Colab // Kaggle installation (Part 3)\n",
    "\n",
    "## GET BACK TO THE DIRECTORY AND CONFIGURE .env\n",
    "\n",
    "################ COLAB #####################\n",
    "\n",
    "# %cd /content/ComputationThruDynamicsBenchmark/\n",
    "# envStr = \"\"\"HOME_DIR=/content/ComputationThruDynamicsBenchmark/\n",
    "# #Don't change these\n",
    "# TRAIN_INPUT_FILE=train_input.h5\\nEVAL_INPUT_FILE=eval_input.h5\n",
    "# EVAL_TARGET_FILE=eval_target.h5\n",
    "# \"\"\"\n",
    "\n",
    "#############################################\n",
    "\n",
    "################ KAGGLE #####################\n",
    "\n",
    "# %cd /kaggle/working/ComputationThruDynamicsBenchmark/\n",
    "# envStr = \"\"\"HOME_DIR=/kaggle/working/ComputationThruDynamicsBenchmark/\n",
    "# #Don't change these\n",
    "# TRAIN_INPUT_FILE=train_input.h5\\nEVAL_INPUT_FILE=eval_input.h5\n",
    "# EVAL_TARGET_FILE=eval_target.h5\n",
    "# \"\"\"\n",
    "\n",
    "##############################################\n",
    "\n",
    "# with open('.env','w') as f:\n",
    "#   f.write(envStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @markdown Colab // Kaggle installation (Part 4)\n",
    "\n",
    "# !git clone https://github.com/mitchellostrow/DSA\n",
    "# %cd DSA/\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**IF RUNNING LOCALLY**:\n",
    "\n",
    "Follow the instructions [here](https://github.com/snel-repo/ComputationThruDynamicsBenchmark) to setup the separatte environment for this project, or you can run the cell below for general installment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @markdown Local installation\n",
    "\n",
    "import contextlib\n",
    "import io\n",
    "import os\n",
    "\n",
    "dirname = \"ComputationThruDynamicsBenchmark\"\n",
    "\n",
    "with contextlib.redirect_stdout(io.StringIO()): #to suppress output\n",
    "    if not os.path.isdir(dirname):\n",
    "        ! git clone https://github.com/neuromatch/ComputationThruDynamicsBenchmark\n",
    "        %cd ComputationThruDynamicsBenchmark\n",
    "        ! pip install -e .\n",
    "\n",
    "envStr = \"\"\"HOME_DIR=ComputationThruDynamicsBenchmark/\n",
    "\n",
    "\n",
    "#Don't change these\n",
    "TRAIN_INPUT_FILE=train_input.h5\\nEVAL_INPUT_FILE=eval_input.h5\n",
    "EVAL_TARGET_FILE=eval_target.h5\n",
    "\"\"\"\n",
    "\n",
    "with open('.env','w') as f:\n",
    "  f.write(envStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2: Overview of the available tasks\n",
    "\n",
    "First, let's take a high-level look at the tasks that we are going to use to understand computation in artificial networks!\n",
    "\n",
    "We'll start by loading in some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed for reproducibility\n",
    "import random\n",
    "import dotenv\n",
    "import pathlib\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# comment the next three lines if you want to see all training logs\n",
    "pl_loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict if 'pytorch_lightning' in name]\n",
    "for pl_log in pl_loggers:\n",
    "    logging.getLogger(pl_log.name).setLevel(logging.WARNING)\n",
    "\n",
    "random.seed(2024)\n",
    "\n",
    "dotenv.load_dotenv(override=True)\n",
    "HOME_DIR = os.getenv(\"HOME_DIR\")\n",
    "if HOME_DIR is None:\n",
    "    HOME_DIR = \"\"\n",
    "print(HOME_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "The Computation-Thru-Dynamics Benchmark has three distinct behavioral tasks.\n",
    "\n",
    "These tasks are called:\n",
    "1. Three-Bit Flip-Flop (3BFF) (see [Sussillo & Barak 2013](https://pubmed.ncbi.nlm.nih.gov/23272922/))\n",
    "2. MultiTask (See [Driscoll et al. 2023](https://www.biorxiv.org/content/10.1101/2022.08.15.503870v1.full.pdf))\n",
    "3. RandomTarget (See [Codol et al. 2023](https://elifesciences.org/reviewed-preprints/88591v2/reviews))\n",
    "\n",
    "We chose these tasks because they represent a variety of task complexities. We have a pretty good understanding of how the simpler tasks operate (3BFF), but really are only starting to scratch the surface of more complex tasks (RandomTarget).\n",
    "\n",
    "\n",
    " \n",
    "Specificially, in the Random Target task, the actions that the model takes can affect the future inputs, making it an important test case for being able to understand the dynamics of interacting systems!\n",
    "\n",
    "<img src=\"https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/TaskComplexity-01.png?raw=true\" width=\"600\"/>\n",
    "\n",
    "Each task (which we call a \"task environment\") follows a standardized format that allows alternative task environments to be incorporated without any changes to the training pipeline.\n",
    "\n",
    "Here we'll take a walk through the two tasks in the project template (TBFF and RandomTarget) and inspect the behavior of networks trained on these environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3: Understanding the Three-Bit Flip-Flop task (3BFF)\n",
    "\n",
    "We're going to start out with the task that launched a thousand Nature papers, the 3-Bit Flip-Flop. [Sussillo & Barak 2013](https://pubmed.ncbi.nlm.nih.gov/23272922/) used the three-bit flip-flop their seminal attempts to understand how dynamics can give rise to computation! \n",
    "\n",
    "The code snippet below intantiates an \"TaskEnv\" object, which contains the logic for the NBFF task.\n",
    "\n",
    "\n",
    "❓❓❓\n",
    "\n",
    "The default parameters are shown in ./interpretability/task_modeling/configs/env_task/NBFF.yaml, but try changing the parameters below to see how that affects trials generated from the environment. Note that this task is modular in the number of bits as well, so it provides an easy way to scale the dimensionality of a very simple dynamical system.\n",
    "\n",
    "❓❓❓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctd.task_modeling.task_env.task_env import NBitFlipFlop\n",
    "\n",
    "n = 3 # The number of bits in the flip-flop (default: 3)\n",
    "trial_length = 500 # The number of time steps in each trial (default: 500)\n",
    "switch_prob = 0.015 # The probability of an input pulse (default: 0.015 pulses/channel / time step)\n",
    "noise = 0.15 # The standard deviation of the Gaussian noise added to the input (default: 0.15)\n",
    "\n",
    "# This line creates the NBitFlipFlop environment. See ctd.task_modeling.task_env.task_env.NBitFlipFlop for more information.\n",
    "env_3bff = NBitFlipFlop(\n",
    "    n = n,\n",
    "    n_timesteps=trial_length,\n",
    "    switch_prob=switch_prob,\n",
    "    noise=noise\n",
    "    )\n",
    "\n",
    "# Renders a random trial from the environment\n",
    "env_3bff.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Above, we are plotting the inputs and outputs of the 3BFF task. One trial is 500 time steps, each with a 1% probability of getting an \"up\" or \"down\" pulse on each of its 3 input channels. When the task receives an \"up\" pulse, the state corresponding to that input channel moves from zero to one (if possible), and if a state at one receives a \"down\" pulse, it goes to zero. In this way, this system acts as 3 bits of memory, encoding 8 potential system states (2^3 states). We add noise to the inputs of the system so that it better reflects realistic computations that a neural circuit might perform.\n",
    "\n",
    "\n",
    "**Try changing the parameters of your 3BFF environment to see how the behavior changes!**\n",
    "\n",
    "Another way to visualize this is to view the three states in 3D. Below you can see that the 8 potential states appear as the vertices of a cube. Each trial is plotted as column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_3bff.render_3d(n_trials=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we can see the basic logic of the task, let's do a basic overview of what task-training is!\n",
    "\n",
    "<img src=\"https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/TTModelExample-01.png?raw=true\" width=\"600\"/>\n",
    "\n",
    "For task-training, we are simply training a model (e.g., an RNN) to produce a set of outputs given a set of inputs. This input/output relationship defines the task that the model is performing. In the case of 3BFF, an input pulse should cause the model's output to change in a way that reflects the switching of a bit. \n",
    "\n",
    "**3BFF Training Objective:**\n",
    "\n",
    "3BFF models are trained to minimize the MSE between the desired output and the output of the model, with some other components that pressure the solution to be smooth. If you're interested in the specifics, the implementation of the loss function can be found as the NBFFLoss object in `ctd/task_modeling/task_env/loss_func.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 4: Training a model to perform 3BFF\n",
    "\n",
    "For this tutorial, we are using PyTorch Lightning to abstract much of the engineering away, allowing you to focus your full attention on the scientific questions you want to tackle!\n",
    "\n",
    "This segment takes a few minutes to train, so I'd recommend planning your runs accordingly!\n",
    "\n",
    "⏳⏳⏳\n",
    "\n",
    "The cell below will create a recurrent neural network (RNN) model and use the 3BFF environment to generate samples that the model will be trained on!\n",
    "\n",
    "Unfortunately, it generates a lot of output, so if you don't care to see the model progress, set enable_progress_bar to False below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/runner/work/NeuroAI_Course/NeuroAI_Course/ComputationThruDynamicsBenchmark/lightning_logs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 15:22:50.379600: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/cv2/../../lib64:/opt/hostedtoolcache/Python/3.9.19/x64/lib\n",
      "2024-05-21 15:22:50.379626: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | GRU_RNN | 51.6 K\n",
      "----------------------------------\n",
      "51.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "51.6 K    Total params\n",
      "0.206     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n"
     ]
    }
   ],
   "source": [
    "from ctd.task_modeling.model.rnn import GRU_RNN\n",
    "from ctd.task_modeling.datamodule.task_datamodule import TaskDataModule\n",
    "from ctd.task_modeling.task_wrapper.task_wrapper import TaskTrainedWrapper\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "enable_progress_bar = False\n",
    "\n",
    "# Step 1: Instantiate the model\n",
    "rnn = GRU_RNN(latent_size = 128) # Look in ctd/task_modeling/models for alternative choices!\n",
    "\n",
    "# Step 2: Instantiate the task environment\n",
    "task_env = env_3bff\n",
    "\n",
    "# Step 3: Instantiate the task datamodule\n",
    "task_datamodule = TaskDataModule(task_env, n_samples = 1000, batch_size = 1000)\n",
    "\n",
    "# Step 4: Instantiate the task wrapper\n",
    "task_wrapper = TaskTrainedWrapper(learning_rate=1e-3, weight_decay = 1e-8)\n",
    "\n",
    "# Step 5: Initialize the model with the input and output sizes (3 inputs, 3 outputs, in this case)\n",
    "rnn.init_model(\n",
    "    input_size = task_env.observation_space.shape[0],\n",
    "    output_size = task_env.action_space.shape[0]\n",
    "    )\n",
    "\n",
    "# Step 6:  Set the environment and model in the task wrapper\n",
    "task_wrapper.set_environment(task_env)\n",
    "task_wrapper.set_model(rnn)\n",
    "\n",
    "# Step 7: Define the PyTorch Lightning Trainer object\n",
    "trainer = Trainer(max_epochs=500, enable_progress_bar=enable_progress_bar)\n",
    "\n",
    "# Step 8: Fit the model\n",
    "trainer.fit(task_wrapper, task_datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we use pickle to save the trained model and datamodule for use in future analyses!\n",
    "\n",
    "❓❓❓\n",
    "\n",
    "**Once you get this model trained, feel try to try changing the hyperparameters to see if you can get the model to train faster!**\n",
    "\n",
    "❓❓❓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save model as .pkl\n",
    "save_dir = pathlib.Path(HOME_DIR) / \"models_GRU_128\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "with open(save_dir / \"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(task_wrapper, f)\n",
    "\n",
    "# save datamodule as .pkl\n",
    "with open(save_dir / \"datamodule_sim.pkl\", \"wb\") as f:\n",
    "    pickle.dump(task_datamodule, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So that we can start comparing our models, we're going to train a second GRU_RNN to perform the 3BFF task, except this time, we'll use an alternative model called a Neural ODE! \n",
    "\n",
    "Notice that we're using the same datamodule as for the first model, meaning that we can directly compare the two models trial-by-trial.\n",
    "\n",
    "Again, this will take a few minutes to train!\n",
    "\n",
    "⏳⏳⏳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | NODE | 4.8 K \n",
      "-------------------------------\n",
      "4.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.8 K     Total params\n",
      "0.019     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n"
     ]
    }
   ],
   "source": [
    "from ctd.task_modeling.model.node import NODE\n",
    "enable_progress_bar = False\n",
    "rnn = NODE(latent_size = 3, num_layers = 3, layer_hidden_size=64) # Look in ctd/task_modeling/models for alternative choices!\n",
    "task_wrapper = TaskTrainedWrapper(learning_rate=1e-3, weight_decay = 1e-10)\n",
    "rnn.init_model(\n",
    "    input_size = task_env.observation_space.shape[0],\n",
    "    output_size = task_env.action_space.shape[0]\n",
    "    )\n",
    "task_wrapper.set_environment(task_env)\n",
    "task_wrapper.set_model(rnn)\n",
    "trainer = Trainer(max_epochs=500, enable_progress_bar=enable_progress_bar)\n",
    "\n",
    "trainer.fit(task_wrapper, task_datamodule)\n",
    "\n",
    "save_dir = pathlib.Path(HOME_DIR) / \"models_NODE_3\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "with open(save_dir / \"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(task_wrapper, f)\n",
    "\n",
    "# save datamodule as .pkl\n",
    "with open(save_dir / \"datamodule_sim.pkl\", \"wb\") as f:\n",
    "    pickle.dump(task_datamodule, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 5: Inspecting the performance of trained models\n",
    "\n",
    "Now that the models have been trained, let's see if we can determine how similar their computational strategies are to each other!\n",
    "\n",
    "To make your life easier, we've provided an \"analysis\" object that abstracts away much of the data handling, allowing you to work more easily with the data from the models.\n",
    "\n",
    "The analysis object also offers visualization tools that can help to see how well the trained model learned to perform the task!\n",
    "\n",
    "For example, plot_trial_io is a function that plots (for a specified number of trials):\n",
    "- Latent activity\n",
    "- Controlled output\n",
    "- Target output\n",
    "- Noisy inputs to model\n",
    "\n",
    "❓❓❓\n",
    "\n",
    "**Try changing trials that are plotted. Do the models capture all of the states equally well?**\n",
    "\n",
    "❓❓❓\n",
    "\n",
    "### Part 1: Visualizing latent activity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctd.comparison.analysis.tt.tt import Analysis_TT\n",
    "\n",
    "fpath_GRU_128 = HOME_DIR + \"models_GRU_128/\"\n",
    "# Create the analysis object:\n",
    "analysis_GRU_128 = Analysis_TT(\n",
    "    run_name = \"GRU_128_3bff\",\n",
    "    filepath = fpath_GRU_128)\n",
    "\n",
    "analysis_GRU_128.plot_trial_io(num_trials = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_NODE = HOME_DIR + \"models_NODE_3/\"\n",
    "# Create the analysis object:\n",
    "analysis_NODE = Analysis_TT(\n",
    "    run_name = \"NODE_3_3bff\",\n",
    "    filepath = fpath_NODE)\n",
    "\n",
    "analysis_NODE.plot_trial_io(num_trials = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "There are also useful data visualization functions, such as visualizing a scree plot of the latent activity.\n",
    "\n",
    "A scree plot shows the % of variance in the highest principle component dimensions. From this plot we can see that the GRU has the majority of its variance in the first 3 PCs, but there remains significant variance in the lower PCs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_GRU_128.plot_scree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Importantly, the analysis object also provides functions that give access to the raw latent activity, predicted outputs, etc. of the trained models! All of these functions accept a \"phase\" variable that designates whether to return the training and/or validation datasets. \n",
    "These functions are:\n",
    "- `get_latents()`: Returns latent activity of the trained model\n",
    "- `get_inputs()`: Returns the inputs to the model (for 3BFF, the input pulses)\n",
    "- `get_model_output()`: Returns a dict that contains all model outputs:\n",
    "  - controlled - the variable that the model is controlling\n",
    "  - latents - the latent activity\n",
    "  - actions - the output from the model (for RandomTarget only)\n",
    "  - states - the state of the environment (for RandomTarget only)\n",
    "  - joints - Joint angles (for RandomTarget only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([800, 500, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data shape: torch.Size([200, 500, 128])\n"
     ]
    }
   ],
   "source": [
    "print(f\"All data shape: {analysis_GRU_128.get_latents().shape}\")\n",
    "print(f\"Train data shape: {analysis_GRU_128.get_latents(phase = 'train').shape}\")\n",
    "print(f\"Validation data shape: {analysis_GRU_128.get_latents(phase = 'val').shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Part 2: Using affine transformations to compare latent activity\n",
    "\n",
    "Now that we have the latent activity for the 64D and the 128D GRU models trained on 3BFf, we can investigate how similar their latent activity is.\n",
    "\n",
    "One problem: The models may be arbitrarily rotated, scaled, and translated relative to each other! \n",
    "\n",
    "This means that we need to find the best \"fit\" between the two models that doesn't fail when they are equivalent under an \"affine\" transformation (meaning a linear transformation and/or translation).\n",
    "\n",
    "Luckily, we have a tool that can solve this problem for us! Linear regression.\n",
    "\n",
    "In this code, we are:\n",
    "\n",
    "1. Getting the latent activity from each model\n",
    "2. Performing PCA on the latent activity (to get the dimensions ordered by their variance)\n",
    "3. Fit a linear regression from one set of latent activity to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "source = analysis_GRU_128\n",
    "target = analysis_NODE\n",
    "\n",
    "# Get the latent activity from the validation phase for each model:\n",
    "latents_source = source.get_latents(phase='train').detach().numpy()\n",
    "latents_targ = target.get_latents(phase='train').detach().numpy()\n",
    "\n",
    "latents_source_val = source.get_latents(phase='val').detach().numpy()\n",
    "latents_targ_val = target.get_latents(phase='val').detach().numpy()\n",
    "\n",
    "n_trials, n_timesteps, n_latent_source = latents_source.shape\n",
    "n_trials, n_timesteps, n_latent_targ = latents_targ.shape\n",
    "\n",
    "n_trials_val, n_timesteps_val, n_latent_source_val = latents_source_val.shape\n",
    "n_trials_val, n_timesteps_val, n_latent_targ_val = latents_targ_val.shape\n",
    "\n",
    "print(f\"Latent shape for source model: {latents_source.shape}\"\n",
    "      f\"\\nLatent shape for target model: {latents_targ.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on both latent spaces to find axes of highest variance\n",
    "pca_source = PCA()\n",
    "pca_targ = PCA()\n",
    "lats_source_pca = pca_source.fit_transform(latents_source.reshape(-1, n_latent_source)).reshape((n_trials, n_timesteps, -1))\n",
    "lats_source_pca_val = pca_source.transform(latents_source_val.reshape(-1, n_latent_source)).reshape((n_trials, n_timesteps, -1))\n",
    "\n",
    "lats_targ_pca = pca_targ.fit_transform(latents_targ.reshape(-1, n_latent_targ)).reshape((n_trials, n_timesteps, -1))\n",
    "lats_targ_pca_val = pca_targ.transform(latents_targ_val.reshape(-1, n_latent_targ_val)).reshape((n_trials_val, n_timesteps_val, -1))\n",
    "\n",
    "# Fit a linear regression model to predict the target latents from the source latents\n",
    "reg = LinearRegression().fit(lats_source_pca.reshape(-1, n_latent_source), lats_targ_pca.reshape(-1, n_latent_targ))\n",
    "# Get the R2 of the fit\n",
    "preds = reg.predict(lats_source_pca_val.reshape(-1, n_latent_source_val))\n",
    "r2s = r2_score(lats_targ_pca_val.reshape((-1, n_latent_targ_val)), preds,  multioutput = \"raw_values\")\n",
    "r2_var = r2_score(lats_targ_pca_val.reshape((-1, n_latent_targ_val)), preds, multioutput = \"variance_weighted\")\n",
    "print(f\"R2 of linear regression fit: {r2s}\")\n",
    "print(f\"Variance-weighted R2 of linear regression fit: {r2_var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So the variance weighted R2 from the source to the target is ~0.93.\n",
    "\n",
    "Importantly, we had to pick a \"direction\" to compute this R2 value. What happens if we switch the source and targets?\n",
    "\n",
    "❓❓❓\n",
    "\n",
    "**Try reversing the direction (the source and targets) and see how well the model fits!**\n",
    "\n",
    "❓❓❓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "One final tool that is provided to you is the comparison object, which does many of these direct comparisons within the object itself. Here is one example visualization that shows how similar the latent activity of two example trials are for these two models! \n",
    "\n",
    "This function has the affine transformation \"built-in\" so you don't this shows what your R2 value above looks like in the first 3 PCs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctd.comparison.comparison import Comparison\n",
    "comp = Comparison()\n",
    "comp.load_analysis(analysis_GRU_128, reference_analysis=True)\n",
    "comp.load_analysis(analysis_NODE)\n",
    "comp.plot_trials_3d_reference(num_trials=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Part 3: Fixed-point finding\n",
    "\n",
    "\n",
    "\n",
    "Finally, we can use fixed-point finding to inspect the linearized dynamics of the trained model.\n",
    "\n",
    "**What are fixed-points?**\n",
    "\n",
    "Fixed points are points in the dynamics for which the flow field is zero, meaning that points at that location do not move.\n",
    "\n",
    "The fixed point structure for the 3BFF task was first shown in the original [Sussillo and Barack paper](https://ccn.johndmurray.org/ccn_2013/materials/pdf/mante/sussillo_2013.pdf).\n",
    "\n",
    "<img src=\"https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/SussilloBarack.png?raw=true\" width=\"400\"/>\n",
    "\n",
    "\n",
    "We can see that the fixed-points are at the vertices of the cube above, drawing the activity towards them and keeping it there until an input pushes it out!\n",
    "\n",
    "We use a modified version of a fixed point finder released by [Golub et al. 2018](https://github.com/mattgolub/fixed-point-finder) to search the flow field for these zero points.\n",
    "\n",
    "❓❓❓\n",
    "\n",
    "**Try changing some of these parameters:** \n",
    "- How quickly are the fixed-points found in the model?\n",
    "- How many initializations are needed to find the fixed points?\n",
    "- Do the stability properties tell us anything about the underlying computation?\n",
    "\n",
    "❓❓❓\n",
    "\n",
    "Importantly from [Driscol et al. 2022](https://www.biorxiv.org/content/10.1101/2022.08.15.503870v1.full.pdf), we know that changes in the inputs can have large effects on the fixed point architecture, so we're going to set the inputs to zero in this optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with contextlib.redirect_stdout(io.StringIO()): #to suppress output\n",
    "    fps = analysis_GRU_128.plot_fps(\n",
    "        inputs= torch.zeros(3),\n",
    "        n_inits=1024,\n",
    "        learning_rate=1e-3,\n",
    "        noise_scale=0.0,\n",
    "        max_iters=20000,\n",
    "        seed=0,\n",
    "        compute_jacobians=True,\n",
    "        q_thresh=1e-5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "q_thesh = 1e-6\n",
    "q_vals = fps.qstar\n",
    "x_star = fps.xstar[q_vals < q_thesh]\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_star[:, 0], x_star[:, 1], x_star[:, 2], c='r', marker='o')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "❓❓❓\n",
    "\n",
    "What can you find out about the FPs of the trained models? Can you modify the FP finding hps to get more interpretable results?\n",
    "\n",
    "What can we learn about the computational solution built in this 3BFF network from these fixed-point architectures?\n",
    "\n",
    "❓❓❓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 6: Introducing the Random Target task\n",
    "\n",
    "Now that we've developed intuition on a simple, well understood task, let's move up the ladder of complexity!\n",
    "\n",
    "The second task is a random-target reaching task performed by an RNN controlling a 2-joint musculoskeletal model of an arm actuated by 6 Mujoco muscles. This environment was built using MotorNet, a package developed by [Oli Codol et al.](https://github.com/OlivierCodol/MotorNet) that provides environments for training RNNs to control biomechanical models!\n",
    "\n",
    "Here is a short clip of what this task looks like when performed by a well-trained model:\n",
    "\n",
    "<img src=\"https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/FinalGif.gif?raw=true\" width=\"300\"/>\n",
    "\n",
    "\n",
    "Behaviorally, the task has the following structure:\n",
    "1. A random initial hand position is sampled from a range of reachable locations; the model is instructed to maintain that hand position.\n",
    "2. A random target position is chosen from the range of reachable locations and fed to the model.\n",
    "3. After a random delay period, a go-cue is fed to the model, which prompts the model to generate muscle activations that drive the hand to the target location.\n",
    "4. On 20% of trials, the go-cue is never supplied (\"catch\" trials)\n",
    "5. On 50% of trials, a randomly directed bump perturbation (5-10 N, 150-300 ms duration) is applied to the hand.\n",
    "    - 50% of these bumps occur in a small window after the go-cue\n",
    "    - 50% of these bumps occur at a random time in the trial\n",
    "\n",
    "The model is trained to:\n",
    "1. Minimize the MSE between the hand position and the desired hand position\n",
    "2. Minimize the squared muscle activation\n",
    "\n",
    "with each loss term being weighted by a scalar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctd.task_modeling.task_env.task_env import RandomTarget\n",
    "from motornet.effector import RigidTendonArm26\n",
    "from motornet.muscle import MujocoHillMuscle\n",
    "# Create the analysis object:\n",
    "rt_task_env = RandomTarget(effector = RigidTendonArm26(muscle = MujocoHillMuscle()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "⏳⏳⏳\n",
    "\n",
    "Now to train the model! We use the same procedure as the 3BFF above; this model will take a bit longer to train however, and you will want to turn off the GPUs (if you have them). Because of the serial nature of this task, the parallelization allowed by GPUs doesn't help speed up our training!\n",
    "\n",
    "⏳⏳⏳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/runner/work/NeuroAI_Course/NeuroAI_Course/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_env/task_env.py:334: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(ang_targ, dtype=torch.float32, device=self.device)\n",
      "/home/runner/work/NeuroAI_Course/NeuroAI_Course/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_env/task_env.py:338: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(angs, dtype=torch.float32, device=self.device)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type         | Params\n",
      "------------------------------------------\n",
      "0 | task_env | RandomTarget | 84    \n",
      "1 | model    | GRU_RNN      | 57.4 K\n",
      "------------------------------------------\n",
      "57.4 K    Trainable params\n",
      "84        Non-trainable params\n",
      "57.4 K    Total params\n",
      "0.230     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n"
     ]
    }
   ],
   "source": [
    "from ctd.task_modeling.model.rnn import GRU_RNN\n",
    "from ctd.task_modeling.datamodule.task_datamodule import TaskDataModule\n",
    "from ctd.task_modeling.task_wrapper.task_wrapper import TaskTrainedWrapper\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# Step 1: Instantiate the model\n",
    "rnn = GRU_RNN(latent_size = 128) # Look in ctd/task_modeling/models for alternative choices!\n",
    "\n",
    "# Step 2: Instantiate the task environment\n",
    "task_env = rt_task_env\n",
    "\n",
    "# Step 3: Instantiate the task datamodule\n",
    "task_datamodule = TaskDataModule(task_env, n_samples = 1000, batch_size = 256)\n",
    "\n",
    "# Step 4: Instantiate the task wrapper\n",
    "task_wrapper = TaskTrainedWrapper(learning_rate=1e-3, weight_decay = 1e-8)\n",
    "\n",
    "# Step 5: Initialize the model with the input and output sizes\n",
    "rnn.init_model(\n",
    "    input_size = task_env.observation_space.shape[0] + task_env.context_inputs.shape[0],\n",
    "    output_size = task_env.action_space.shape[0]\n",
    "    )\n",
    "\n",
    "# Step 6:  Set the environment and model in the task wrapper\n",
    "task_wrapper.set_environment(task_env)\n",
    "task_wrapper.set_model(rnn)\n",
    "\n",
    "# Step 7: Define the PyTorch Lightning Trainer object (put `enable_progress_bar=True` to observe training progress)\n",
    "trainer = Trainer(accelerator= \"cpu\",max_epochs=500,enable_progress_bar=False)\n",
    "\n",
    "# Step 8: Fit the model\n",
    "trainer.fit(task_wrapper, task_datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Importantly, this task is distinct from the previous two tasks because the outputs of the model affect the subsequent inputs!\n",
    "\n",
    "<img src=\"https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/MotorNetIllustration-01.png?raw=true\" width=\"400\"/>\n",
    "\n",
    "\n",
    "Visualizing the latent dynamics of models trained on MotorNet tasks, we can see that there are complex features in the state space, but we'll leave that to you to figure out what they mean!\n",
    "\n",
    "<img src=\"https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/MotorNetGif.gif?raw=true\" width=\"300\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the later questions, we will ask you to modify the environments in MotorNet to test how well your models can generalize to new tasks!\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "That's it!\n",
    "\n",
    "To recap, in this tutorial we learned:\n",
    "1. The basics of two tasks, the Three-Bit Flip-Flop and the Random Target task.\n",
    "2. How to train recurrent neural network models on these tasks\n",
    "3. Methods of visualizng and quantifying differences between these task-trained models.\n",
    "\n",
    "As you begin to extend beyond this tutorial, you will likely need to make your own environments, or modify existing environments to test the ability of models to generalize. We've tried to document the code-base to make this as easy as possible, but feel free to reach out if you have any questions!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ComparingNetworks",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
