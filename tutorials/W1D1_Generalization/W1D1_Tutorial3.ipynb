{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c900bed-85b4-4e92-a7c5-d705f2d563bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown\n",
    "\n",
    "#!pip install ipywidgets matplotlib torch torchvision tqdm hashlib requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b5bef4-77ad-4413-85b1-50a269d464f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GNS and pyBPL\n",
    "#!pip install git+https://github.com/neuromatch/GNS-Modeling\n",
    "#!pip install git+https://github.com/neuromatch/pyBPL\n",
    "#!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f2870-3fd6-4a63-8360-36763673490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries for file operations, operating system operations, web requests, and others\n",
    "import hashlib\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "from importlib import reload\n",
    "from zipfile import ZipFile\n",
    "import zipfile\n",
    "# Data handling and visualization libraries\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning libraries\n",
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# IPython display & widgets for interactive controls in Jupyter notebooks\n",
    "from IPython.display import clear_output, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# pybpl and gns libraries for specific tasks\n",
    "import gns\n",
    "from gns import MODEL_SAVE_PATH\n",
    "from gns.inference.parsing import get_topK_parses\n",
    "from gns.omniglot.classification import ClassificationDataset\n",
    "from gns.rendering import Renderer\n",
    "from gns.type import TypeModel\n",
    "from gns.utils.experiments import mkdir, time_string\n",
    "import pybpl\n",
    "from pybpl import splines, parameters\n",
    "from pybpl.util import nested_map\n",
    "from pybpl.util.stroke import dist_along_traj\n",
    "\n",
    "# Additional utilities\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reload gns and its submodules if necessary\n",
    "reload(gns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870241d-73fb-455f-a8ef-c92f9b2bdaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452c1b0-d79d-4c17-8738-b0b342180a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "# @markdown\n",
    "\n",
    "def display_images(probe, options):\n",
    "    # Open the probe image and the option images\n",
    "    probe_image = Image.open(probe)\n",
    "    option_images = [Image.open(img_path) for img_path in options]\n",
    "    \n",
    "    # Create a figure with the probe and the 3x3 grid for the options directly below\n",
    "    fig = plt.figure(figsize=(15, 10))  # Adjust figure size as needed\n",
    "    \n",
    "    # Add the probe image to the top of the figure with a red border\n",
    "    ax_probe = fig.add_subplot(4, 3, (1, 3))  # Span the probe across the top 3 columns\n",
    "    ax_probe.imshow(probe_image)\n",
    "    ax_probe.axis('off')\n",
    "    rect = patches.Rectangle((0, 0), probe_image.width-1, probe_image.height-1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax_probe.add_patch(rect)\n",
    "    \n",
    "    # Position the 3x3 grid of option images directly below the probe image\n",
    "    for index, img in enumerate(option_images):\n",
    "        row = (index // 3) + 1  # Calculate row in the 3x3 grid, starting directly below the probe\n",
    "        col = (index % 3) + 1   # Calculate column in the 3x3 grid\n",
    "        ax_option = fig.add_subplot(4, 3, row * 3 + col)  # Adjust grid position to directly follow the probe\n",
    "        ax_option.imshow(img)\n",
    "        ax_option.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5deda-c9f0-4733-a01b-3d077a96bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Data retrieval for zip files\n",
    "# @markdown\n",
    "\n",
    "def handle_file_operations(fname, url, expected_md5, extract_to='data'):\n",
    "    \"\"\"Handles downloading, verifying, and extracting a file.\"\"\"\n",
    "    \n",
    "    # Define helper functions for download, verify, and extract operations\n",
    "    def download_file(url, filename):\n",
    "        \"\"\"Downloads file from the given URL and saves it locally.\"\"\"\n",
    "        try:\n",
    "            r = requests.get(url, stream=True)\n",
    "            r.raise_for_status()\n",
    "            with open(filename, \"wb\") as fid:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    fid.write(chunk)\n",
    "            print(\"Download successful.\")\n",
    "            return True\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"!!! Failed to download data: {e} !!!\")\n",
    "            return False\n",
    "    \n",
    "    def verify_file_md5(filename, expected_md5):\n",
    "        \"\"\"Verifies the file's MD5 checksum.\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(filename, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        if hash_md5.hexdigest() == expected_md5:\n",
    "            print(\"MD5 checksum verified.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"!!! Data download appears corrupted !!!\")\n",
    "            return False\n",
    "    \n",
    "    def extract_zip_file(filename, extract_to):\n",
    "        \"\"\"Extracts the ZIP file to the specified directory.\"\"\"\n",
    "        try:\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_to)\n",
    "            print(f\"File extracted successfully to {extract_to}\")\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\"!!! The ZIP file is corrupted or not a zip file !!!\")\n",
    "    \n",
    "    # Main operation\n",
    "    if not os.path.isfile(fname) or not verify_file_md5(fname, expected_md5):\n",
    "        if download_file(url, fname) and verify_file_md5(fname, expected_md5):\n",
    "            extract_zip_file(fname, extract_to)\n",
    "    else:\n",
    "        print(f\"File '{fname}' already exists and is verified. Proceeding to extraction.\")\n",
    "        extract_zip_file(fname, extract_to)\n",
    "\n",
    "# Example usage\n",
    "file_info = [\n",
    "    {\"fname\": \"one-shot-classification.zip\", \"url\": \"https://osf.io/aw6eq/download\", \"expected_md5\": \"9376412e7fb74d64f045644b51e641a6\"},\n",
    "    {\"fname\": \"omniglot-py.zip\", \"url\": \"https://osf.io/bazxp/download\", \"expected_md5\": \"f7a4011f5c25460c6d95ee1428e377ed\"}\n",
    "]\n",
    "\n",
    "for file in file_info:\n",
    "    handle_file_operations(**file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cf0b6-954a-414b-80b9-e356349bc934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Data retrieval\n",
    "# @markdown\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    Download a file from a given URL\n",
    "\n",
    "    Parameters:\n",
    "    url (str): URL to the file\n",
    "    filename (str): Local filename to save the file\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code.\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "def verify_checksum(filename, expected_checksum):\n",
    "    \"\"\"\n",
    "    Verify the MD5 checksum of a file\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Path to the file\n",
    "    expected_checksum (str): Expected MD5 checksum\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the checksum matches, False otherwise\n",
    "    \"\"\"\n",
    "    md5 = hashlib.md5()\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            md5.update(chunk)\n",
    "\n",
    "    return md5.hexdigest() == expected_checksum\n",
    "    \n",
    "def move_models_to_directory(model_files, destination_directory):\n",
    "    \"\"\"\n",
    "    Move model files to a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    model_files (list of str): Filenames of the models to move.\n",
    "    destination_directory (str): The destination directory where models will be moved.\n",
    "    \"\"\"\n",
    "    for model_file in model_files:\n",
    "        shutil.move(model_file, destination_directory + model_file)\n",
    "\n",
    "def load_models(model_files, directory, map_location='cpu'):\n",
    "    \"\"\"\n",
    "    Load multiple models from a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    model_files (list of str): Filenames of the models to load.\n",
    "    directory (str): Directory from which to load the models.\n",
    "    map_location (str): Device on which to load the models ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary of models, keyed by their filenames.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for model_file in model_files:\n",
    "        full_path = directory + model_file\n",
    "        models[model_file] = torch.load(full_path, map_location=map_location)\n",
    "    return models\n",
    "\n",
    "def verify_models_in_destination(model_files, destination_directory):\n",
    "    \"\"\"\n",
    "    Verify the presence of model files in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    model_files (list of str): Filenames of the models to verify.\n",
    "    destination_directory (str): The directory where the models are supposed to be.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if all models are found in the directory, False otherwise.\n",
    "    \"\"\"\n",
    "    missing_files = []\n",
    "    for model_file in model_files:\n",
    "        # Construct the full path to where the model should be\n",
    "        full_path = os.path.join(destination_directory, model_file)\n",
    "        # Check if the model exists at the location\n",
    "        if not os.path.exists(full_path):\n",
    "            missing_files.append(model_file)\n",
    "\n",
    "    if missing_files:\n",
    "        print(f\"Missing model files in destination: {missing_files}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"All models are correctly located in the destination directory.\")\n",
    "        return True\n",
    "        \n",
    "# URLs and checksums for the models\n",
    "models_info = {\n",
    "    'location_model.pt': ('https://osf.io/zmd7y/download', 'dfd51cf7c3a277777ad941c4fcc23813'),\n",
    "    'stroke_model.pt': ('https://osf.io/m6yc7/download', '511ea7bd12566245d5d11a85d5a0abb0'),\n",
    "    'terminate_model.pt': ('https://osf.io/dsmhc/download', '2f3e26cfcf36ce9f9172c15d8b1079d1')\n",
    "}\n",
    "\n",
    "destination_directory = '/home/samuele/.local/lib/python3.10/site-packages/gns/'\n",
    "\n",
    "for model_name, (url, checksum) in models_info.items():\n",
    "    download_file(url, model_name)\n",
    "    if verify_checksum(model_name, checksum):\n",
    "        print(f\"Successfully verified {model_name}\")\n",
    "    else:\n",
    "        print(f\"Checksum does not match for {model_name}. Download might be corrupted.\")\n",
    "        continue  \n",
    "\n",
    "# Define model filenames\n",
    "model_files = list(models_info.keys())\n",
    "\n",
    "# Move models to the specified directory\n",
    "move_models_to_directory(model_files, destination_directory)\n",
    "\n",
    "# Verify the presence of the models in the destination directory\n",
    "if verify_models_in_destination(model_files, destination_directory):\n",
    "    print(\"Verification successful: All models are in the correct directory.\")\n",
    "else:\n",
    "    print(\"Verification failed: Some models are missing from the destination directory.\")\n",
    "\n",
    "# Load the models from the new location\n",
    "models = load_models(model_files, destination_directory, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3609707-5107-4b50-8b0a-a607890adadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "# @markdown\n",
    "\n",
    "def select_random_images_within_alphabet(base_path, alphabet_path, exclude_character_path, num_images=8):\n",
    "    chosen_images = []\n",
    "    all_characters = [char for char in os.listdir(alphabet_path) if os.path.isdir(os.path.join(alphabet_path, char)) and os.path.join(alphabet_path, char) != exclude_character_path]\n",
    "    while len(chosen_images) < num_images:\n",
    "        if not all_characters:\n",
    "            break\n",
    "        character = random.choice(all_characters)\n",
    "        character_path = os.path.join(alphabet_path, character)\n",
    "        all_images = [img for img in os.listdir(character_path) if img.endswith('.png')]\n",
    "        if not all_images:\n",
    "            continue\n",
    "        image_file = random.choice(all_images)\n",
    "        image_path = os.path.join(character_path, image_file)\n",
    "        chosen_images.append(image_path)\n",
    "    return chosen_images\n",
    "\n",
    "def run_trial(base_path, num_trials):\n",
    "    for _ in range(num_trials):\n",
    "        languages = [lang for lang in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, lang))]\n",
    "        selected_language = random.choice(languages)\n",
    "        language_path = os.path.join(base_path, selected_language)\n",
    "        characters = [char for char in os.listdir(language_path) if os.path.isdir(os.path.join(language_path, char))]\n",
    "        selected_character = random.choice(characters)\n",
    "        character_path = os.path.join(language_path, selected_character)\n",
    "        images = [img for img in os.listdir(character_path) if img.endswith('.png')]\n",
    "        probe_image_path, correct_answer_image_path = random.sample(images, 2)\n",
    "        probe_image_path = os.path.join(character_path, probe_image_path)\n",
    "        correct_answer_image_path = os.path.join(character_path, correct_answer_image_path)\n",
    "        wrong_answers = select_random_images_within_alphabet(base_path, language_path, character_path, num_images=8)\n",
    "        options = wrong_answers\n",
    "        options.insert(random.randint(0, len(options)), correct_answer_image_path)\n",
    "        display_images(probe_image_path, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8caf54d-c1b2-4f10-8f70-a96e12620dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base_path = \"data/omniglot-py/images_background\"\n",
    "\n",
    "# Running the trial\n",
    "run_trial(base_path, num_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed022acf-0a4a-428b-af0a-8fa41c1787a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflection activity: How do you think you, as a human, are performing a task like Omniglot?\n",
    "# One-shot learning\n",
    "# Segmentation\n",
    "# Conditioned generation\n",
    "# Unconditioned generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f7499-c4b4-47e6-8dfe-137333c33c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_score_fn(model, parses):\n",
    "    drawings = nested_map(lambda x: splines.get_stk_from_bspline(x), parses)\n",
    "    if torch.cuda.is_available():\n",
    "        drawings = nested_map(lambda x: x.cuda(), drawings)\n",
    "        parses = nested_map(lambda x: x.cuda(), parses)\n",
    "    losses = model.losses_fn(parses, drawings, filter_small=False, denormalize=True)\n",
    "    return -losses.cpu()\n",
    "\n",
    "def collect_img_results(img_id, parses, log_probs, reverse):\n",
    "    \"\"\"Collects parses and log probabilities without saving to disk.\"\"\"\n",
    "    appendix = 'test' if reverse else 'train'\n",
    "    data = {\n",
    "        'img_id': img_id,\n",
    "        'appendix': appendix,\n",
    "        'parses': parses,\n",
    "        'log_probs': log_probs,\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def get_base_parses_in_memory(run_id, trials_per=800, reverse=False, dry_run=False):\n",
    "    print('run_id: %i' % run_id)\n",
    "    print('Loading model...')\n",
    "    # Correctly identify the base directory of the `gns` package\n",
    "    gns_base_dir = os.path.dirname(gns.__file__)\n",
    "    \n",
    "    # Specify the model save directory within the `gns` package\n",
    "    model_save_path = os.path.join(gns_base_dir, 'model_saves')\n",
    "    \n",
    "    # Initialize TypeModel with the correct model save path\n",
    "    type_model = TypeModel(save_dir=model_save_path).eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        type_model = type_model.cuda()\n",
    "    score_fn = lambda parses: model_score_fn(type_model, parses)\n",
    "\n",
    "    # Use the dynamically determined path\n",
    "    osc_path = os.path.join(os.getcwd(), 'data/one-shot-classification')\n",
    "    print('Loading classification dataset...')\n",
    "    dataset = ClassificationDataset(osc_folder=osc_path)\n",
    "    run = dataset.runs[run_id]\n",
    "    imgs = run.test_imgs if reverse else run.train_imgs\n",
    "\n",
    "    collected_data = []  # In-memory storage for parses and log_probs\n",
    "\n",
    "    print('Collecting top-K parses for each train image...')\n",
    "    nimg = len(imgs)\n",
    "    for i in range(nimg):\n",
    "        start_time = time.time()\n",
    "        parses, log_probs = get_topK_parses(\n",
    "            imgs[i], k=5, score_fn=score_fn, configs_per=1,\n",
    "            trials_per=trials_per)\n",
    "        total_time = time.time() - start_time\n",
    "        print(f'image {i+1}/{nimg} took {time_string(total_time)}')\n",
    "        if dry_run:\n",
    "            continue\n",
    "        img_results = collect_img_results(i, parses, log_probs, reverse)\n",
    "        collected_data.append(img_results)\n",
    "\n",
    "    return collected_data\n",
    "\n",
    "# Example usage\n",
    "run_id = 10\n",
    "trials_per = 800\n",
    "reverse = False\n",
    "dry_run = False\n",
    "\n",
    "# Call the modified function\n",
    "collected_data = get_base_parses_in_memory(run_id=run_id, trials_per=trials_per, reverse=reverse, dry_run=dry_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9c406-e512-466e-b8ff-34acda11fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use retina mode for matplotlib\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Function to visualize parses\n",
    "def visualize_parses(collected_data, item, run):\n",
    "    # Load the classification labels to map test images to their corresponding training images\n",
    "    with open(f'./data/one-shot-classification/all_runs/run{run+1:02}/class_labels.txt') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    test_to_train = {}\n",
    "    for line in data:\n",
    "        left, right = line.split(' ')\n",
    "        test_to_train[left.split('/')[-1]] = right.split('/')[-1].strip()\n",
    "\n",
    "    plt.figure(figsize=(12.5, 2.5))\n",
    "    train_im = test_to_train[f'item{item:02}.png']\n",
    "\n",
    "    # Reference image visualization\n",
    "    plt.subplot(1, 6, 1)\n",
    "    plt.imshow(1 - plt.imread(f'./data/one-shot-classification/all_runs/run{run+1:02}/training/{train_im}'), cmap='gray')\n",
    "    plt.title('Reference image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Load parses from the collected data\n",
    "    for img_data in collected_data:\n",
    "        if img_data['img_id'] == item - 1:\n",
    "            parses = img_data['parses']\n",
    "            break\n",
    "\n",
    "    r = Renderer()\n",
    "    # Visualization of parses\n",
    "    for i, parse in enumerate(parses):\n",
    "        drawings = [splines.get_stk_from_bspline(x) for x in parse]\n",
    "        plt.subplot(1, 6, i+2)\n",
    "        plt.imshow(r(drawings), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Parse {i+1}')\n",
    "\n",
    "        colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n",
    "        for i, d in enumerate(drawings):\n",
    "            plt.plot(d[:, 0], -d[:, 1], colors[i], linewidth=1)\n",
    "            plt.plot(d[0, 0], -d[0, 1], 'w.', markersize=20, fillstyle='none')\n",
    "            plt.plot(d[-1, 0], -d[-1, 1], 'ws', markersize=10, fillstyle='none')\n",
    "\n",
    "# Example of visualizing parses for a specific item and run\n",
    "visualize_parses(collected_data, item=6, run=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c807c826-91c3-4adc-916d-04353ea2de92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
