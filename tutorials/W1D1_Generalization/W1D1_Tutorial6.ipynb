{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12bb9e08-6996-4d6b-8b80-a7a26191c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2baca459-5a6a-489b-90d3-dffa0a4893e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .mat file\n",
    "data = scipy.io.loadmat('data/condsForSimJ2moMuscles.mat')\n",
    "\n",
    "# Extract condsForSim struct\n",
    "conds_for_sim = data['condsForSim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c15d51-8e59-4305-a2bc-e270ac349085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_214548/1417502240.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  go_envelope_all.append(torch.tensor(go_envelope_condition, dtype=torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go Envelope Tensor Shape: torch.Size([216, 296, 1])\n",
      "Plan Tensor Shape: torch.Size([216, 296, 15])\n",
      "Muscle Tensor Shape: torch.Size([216, 296, 2])\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store data for all conditions\n",
    "go_envelope_all = []\n",
    "plan_all = []\n",
    "muscle_all = []\n",
    "\n",
    "# Get the number of conditions (rows) and delay durations (columns)\n",
    "num_conditions, num_delays = conds_for_sim.shape\n",
    "\n",
    "# Loop through each condition and extract data\n",
    "for i in range(num_conditions):  # 27 conditions\n",
    "    go_envelope_condition = []\n",
    "    plan_condition = []\n",
    "    muscle_condition = []\n",
    "\n",
    "    for j in range(num_delays):  # 8 delay durations\n",
    "        condition = conds_for_sim[i, j]\n",
    "\n",
    "        go_envelope = condition['goEnvelope']\n",
    "        plan = condition['plan']\n",
    "        muscle = condition['muscle']\n",
    "\n",
    "        # Select only muscles 5 and 6 \n",
    "        selected_muscle_data = muscle[:, [4, 5]]  # Select columns for muscle 5 and 6, which show the nicest multiphasic activity\n",
    "\n",
    "        go_envelope_condition.append(go_envelope)\n",
    "        plan_condition.append(plan)\n",
    "        muscle_condition.append(selected_muscle_data)\n",
    "\n",
    "    # Stack data for each condition\n",
    "    go_envelope_all.append(torch.tensor(go_envelope_condition, dtype=torch.float32))\n",
    "    plan_all.append(torch.tensor(plan_condition, dtype=torch.float32))\n",
    "    muscle_all.append(torch.tensor(muscle_condition, dtype=torch.float32))\n",
    "\n",
    "# Stack data for all conditions\n",
    "go_envelope_tensor = torch.stack(go_envelope_all)\n",
    "plan_tensor = torch.stack(plan_all)\n",
    "muscle_tensor = torch.stack(muscle_all)\n",
    "\n",
    "# Reshape to merge the first two dimensions\n",
    "go_envelope_tensor = go_envelope_tensor.reshape(-1, *go_envelope_tensor.shape[2:])\n",
    "plan_tensor = plan_tensor.reshape(-1, *plan_tensor.shape[2:])\n",
    "muscle_tensor = muscle_tensor.reshape(-1, *muscle_tensor.shape[2:])\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Go Envelope Tensor Shape: {go_envelope_tensor.shape}\")\n",
    "print(f\"Plan Tensor Shape: {plan_tensor.shape}\")\n",
    "print(f\"Muscle Tensor Shape: {muscle_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9515c13-74d5-4fb5-8614-53fbb8dcfe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Go Envelope Tensor Shape: torch.Size([216, 296, 1])\n",
      "Normalized Plan Tensor Shape: torch.Size([216, 296, 15])\n",
      "Normalized Muscle Tensor Shape: torch.Size([216, 296, 2])\n"
     ]
    }
   ],
   "source": [
    "# Normalize and standardize a tensor\n",
    "def normalize_and_standardize(tensor):\n",
    "    # Normalize: Scale to 0-1 range\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    tensor = (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "    # Standardize: Shift to zero mean and unit variance\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    tensor = (tensor - mean) / std\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# Apply the function to each tensor\n",
    "normalized_go_envelope_tensor = normalize_and_standardize(go_envelope_tensor)\n",
    "normalized_plan_tensor = normalize_and_standardize(plan_tensor)\n",
    "normalized_muscle_tensor = normalize_and_standardize(muscle_tensor)\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(f\"Normalized Go Envelope Tensor Shape: {normalized_go_envelope_tensor.shape}\")\n",
    "print(f\"Normalized Plan Tensor Shape: {normalized_plan_tensor.shape}\")\n",
    "print(f\"Normalized Muscle Tensor Shape: {normalized_muscle_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2b6a95-49a0-4f10-8f63-3b85a7f7d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, g, tau=50):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = tau  # Time constant\n",
    "\n",
    "        # Weight initialization\n",
    "        self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, input_size) / torch.sqrt(torch.tensor(input_size, dtype=torch.float)))\n",
    "        self.w = nn.Parameter(torch.zeros(output_size, hidden_size))\n",
    "        self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.bz = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "        # Nonlinearity\n",
    "        self.nonlinearity = torch.tanh\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x is of shape [batch_size, input_size]\n",
    "        # hidden is of shape [1, hidden_size] or [batch_size, hidden_size]\n",
    "    \n",
    "        # Matrix multiplication\n",
    "        hidden_update = torch.matmul(self.J, hidden.T)  # [hidden_size, batch_size]\n",
    "        input_update = torch.matmul(self.B, x.T)  # [hidden_size, batch_size]\n",
    "    \n",
    "        # Apply nonlinearity\n",
    "        new_hidden = self.nonlinearity(hidden_update + input_update + self.bx.unsqueeze(1))\n",
    "    \n",
    "        # Transpose back to [batch_size, hidden_size]\n",
    "        new_hidden = new_hidden.T\n",
    "    \n",
    "        output = torch.matmul(self.w, new_hidden.T) + self.bz.unsqueeze(1)\n",
    "        output = output.T  # Transpose to [batch_size, output_size]\n",
    "    \n",
    "        return output, new_hidden\n",
    "\n",
    "\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden state with an additional batch dimension\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 15\n",
    "hidden_size = 300\n",
    "output_size = 2  # Number of muscles\n",
    "g = 1.1  # Moderate g value for simpler dynamics\n",
    "\n",
    "# Model instantiation\n",
    "model = SimpleRNN(input_size, hidden_size, output_size, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "711bd5b4-d92c-43e4-b3c6-fb0a6fabb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "# Assuming that 'normalized_plan_tensor' is your input data and 'normalized_muscle_tensor' is your target data\n",
    "X_train = normalized_plan_tensor\n",
    "y_train = normalized_muscle_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c626ab3-d0fa-4180-a639-1e986caf435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to TensorDataset and DataLoader for batch processing\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64  # You can adjust this based on your data size and memory constraints\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72179104-fa01-47b6-87ea-22b40e5f544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46e0da9-238e-4bc8-a2be-a4dbc562731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.46963347494602203\n",
      "Epoch 2, Loss: 0.45373592525720596\n",
      "Epoch 3, Loss: 0.5241474062204361\n",
      "Epoch 4, Loss: 0.4954524487257004\n",
      "Epoch 5, Loss: 0.46555476635694504\n",
      "Epoch 6, Loss: 0.5544778257608414\n",
      "Epoch 7, Loss: 0.475787989795208\n",
      "Epoch 8, Loss: 0.4912019148468971\n",
      "Epoch 9, Loss: 0.4985133558511734\n",
      "Epoch 10, Loss: 0.4873269721865654\n",
      "Epoch 11, Loss: 0.4678957387804985\n",
      "Epoch 12, Loss: 0.465007059276104\n",
      "Epoch 13, Loss: 0.46721724420785904\n",
      "Epoch 14, Loss: 0.4783782735466957\n",
      "Epoch 15, Loss: 0.4777790829539299\n",
      "Epoch 16, Loss: 0.4955277368426323\n",
      "Epoch 17, Loss: 0.5189576596021652\n",
      "Epoch 18, Loss: 0.542348101735115\n",
      "Epoch 19, Loss: 0.5266738310456276\n",
      "Epoch 20, Loss: 0.5250164568424225\n",
      "Epoch 21, Loss: 0.48532481491565704\n",
      "Epoch 22, Loss: 0.49602771550416946\n",
      "Epoch 23, Loss: 0.5364746451377869\n",
      "Epoch 24, Loss: 0.5523156896233559\n",
      "Epoch 25, Loss: 0.472917839884758\n",
      "Epoch 26, Loss: 0.43356650322675705\n",
      "Epoch 27, Loss: 0.4699414595961571\n",
      "Epoch 28, Loss: 0.5136973261833191\n",
      "Epoch 29, Loss: 0.4594564586877823\n",
      "Epoch 30, Loss: 0.44343457743525505\n",
      "Epoch 31, Loss: 0.49207911640405655\n",
      "Epoch 32, Loss: 0.5197380855679512\n",
      "Epoch 33, Loss: 0.459350161254406\n",
      "Epoch 34, Loss: 0.49905166774988174\n",
      "Epoch 35, Loss: 0.4929628297686577\n",
      "Epoch 36, Loss: 0.46785519272089005\n",
      "Epoch 37, Loss: 0.4969536066055298\n",
      "Epoch 38, Loss: 0.5530929416418076\n",
      "Epoch 39, Loss: 0.4834858328104019\n",
      "Epoch 40, Loss: 0.5220358148217201\n",
      "Epoch 41, Loss: 0.5190033540129662\n",
      "Epoch 42, Loss: 0.4847152903676033\n",
      "Epoch 43, Loss: 0.511987991631031\n",
      "Epoch 44, Loss: 0.5186080411076546\n",
      "Epoch 45, Loss: 0.47020936757326126\n",
      "Epoch 46, Loss: 0.47429873794317245\n",
      "Epoch 47, Loss: 0.4637548550963402\n",
      "Epoch 48, Loss: 0.4743725433945656\n",
      "Epoch 49, Loss: 0.46297311037778854\n",
      "Epoch 50, Loss: 0.47162096202373505\n",
      "Epoch 51, Loss: 0.5101701021194458\n",
      "Epoch 52, Loss: 0.598898135125637\n",
      "Epoch 53, Loss: 0.5520266592502594\n",
      "Epoch 54, Loss: 0.5449180826544762\n",
      "Epoch 55, Loss: 0.4824734404683113\n",
      "Epoch 56, Loss: 0.6078428477048874\n",
      "Epoch 57, Loss: 0.5192313641309738\n",
      "Epoch 58, Loss: 0.46181727945804596\n",
      "Epoch 59, Loss: 0.568025752902031\n",
      "Epoch 60, Loss: 0.453085720539093\n",
      "Epoch 61, Loss: 0.482797808945179\n",
      "Epoch 62, Loss: 0.4433416500687599\n",
      "Epoch 63, Loss: 0.49189984798431396\n",
      "Epoch 64, Loss: 0.4187496155500412\n",
      "Epoch 65, Loss: 0.6586399078369141\n",
      "Epoch 66, Loss: 0.5530650839209557\n",
      "Epoch 67, Loss: 0.430595725774765\n",
      "Epoch 68, Loss: 0.5358163267374039\n",
      "Epoch 69, Loss: 0.5552168041467667\n",
      "Epoch 70, Loss: 0.39775630831718445\n",
      "Epoch 71, Loss: 0.39740492030978203\n",
      "Epoch 72, Loss: 0.43300916999578476\n",
      "Epoch 73, Loss: 0.3410692438483238\n",
      "Epoch 74, Loss: 0.3922875076532364\n",
      "Epoch 75, Loss: 0.3385555148124695\n",
      "Epoch 76, Loss: 0.2659924253821373\n",
      "Epoch 77, Loss: 0.427666112780571\n",
      "Epoch 78, Loss: 0.4099500924348831\n",
      "Epoch 79, Loss: 0.3954349532723427\n",
      "Epoch 80, Loss: 0.33748654276132584\n",
      "Epoch 81, Loss: 0.26088231429457664\n",
      "Epoch 82, Loss: 0.23675751686096191\n",
      "Epoch 83, Loss: 0.4463098719716072\n",
      "Epoch 84, Loss: 0.33799000084400177\n",
      "Epoch 85, Loss: 0.5714684128761292\n",
      "Epoch 86, Loss: 0.5774364024400711\n",
      "Epoch 87, Loss: 0.4940184950828552\n",
      "Epoch 88, Loss: 0.5087490081787109\n",
      "Epoch 89, Loss: 0.595876082777977\n",
      "Epoch 90, Loss: 0.44524000957608223\n",
      "Epoch 91, Loss: 0.5336085930466652\n",
      "Epoch 92, Loss: 0.47725484520196915\n",
      "Epoch 93, Loss: 0.4940311387181282\n",
      "Epoch 94, Loss: 0.4947659820318222\n",
      "Epoch 95, Loss: 0.4762004017829895\n",
      "Epoch 96, Loss: 0.48470622301101685\n",
      "Epoch 97, Loss: 0.447247676551342\n",
      "Epoch 98, Loss: 0.4919622018933296\n",
      "Epoch 99, Loss: 0.4354661963880062\n",
      "Epoch 100, Loss: 0.4652980715036392\n",
      "Epoch 101, Loss: 0.5475862696766853\n",
      "Epoch 102, Loss: 0.4633742868900299\n",
      "Epoch 103, Loss: 0.4697660207748413\n",
      "Epoch 104, Loss: 0.4953225404024124\n",
      "Epoch 105, Loss: 0.4886118620634079\n",
      "Epoch 106, Loss: 0.47653672099113464\n",
      "Epoch 107, Loss: 0.4786517918109894\n",
      "Epoch 108, Loss: 0.5221096277236938\n",
      "Epoch 109, Loss: 0.4839644953608513\n",
      "Epoch 110, Loss: 0.4629524424672127\n",
      "Epoch 111, Loss: 0.5202618390321732\n",
      "Epoch 112, Loss: 0.4858170226216316\n",
      "Epoch 113, Loss: 0.46405886858701706\n",
      "Epoch 114, Loss: 0.5171806588768959\n",
      "Epoch 115, Loss: 0.5115479081869125\n",
      "Epoch 116, Loss: 0.5215776264667511\n",
      "Epoch 117, Loss: 0.4757825583219528\n",
      "Epoch 118, Loss: 0.4772230386734009\n",
      "Epoch 119, Loss: 0.46174610406160355\n",
      "Epoch 120, Loss: 0.5060833543539047\n",
      "Epoch 121, Loss: 0.49919620156288147\n",
      "Epoch 122, Loss: 0.4565359205007553\n",
      "Epoch 123, Loss: 0.43797387555241585\n",
      "Epoch 124, Loss: 0.45790930837392807\n",
      "Epoch 125, Loss: 0.547719918191433\n",
      "Epoch 126, Loss: 0.4617205932736397\n",
      "Epoch 127, Loss: 0.49072103947401047\n",
      "Epoch 128, Loss: 0.5231565237045288\n",
      "Epoch 129, Loss: 0.4952729791402817\n",
      "Epoch 130, Loss: 0.4652858152985573\n",
      "Epoch 131, Loss: 0.5191705003380775\n",
      "Epoch 132, Loss: 0.5348651185631752\n",
      "Epoch 133, Loss: 0.5174143612384796\n",
      "Epoch 134, Loss: 0.45000604167580605\n",
      "Epoch 135, Loss: 0.57172591984272\n",
      "Epoch 136, Loss: 0.5041747540235519\n",
      "Epoch 137, Loss: 0.5138623714447021\n",
      "Epoch 138, Loss: 0.5401738658547401\n",
      "Epoch 139, Loss: 0.46580564230680466\n",
      "Epoch 140, Loss: 0.44683004915714264\n",
      "Epoch 141, Loss: 0.46975062042474747\n",
      "Epoch 142, Loss: 0.52919652312994\n",
      "Epoch 143, Loss: 0.45968518406152725\n",
      "Epoch 144, Loss: 0.4619065970182419\n",
      "Epoch 145, Loss: 0.4842997193336487\n",
      "Epoch 146, Loss: 0.4702714756131172\n",
      "Epoch 147, Loss: 0.4510669484734535\n",
      "Epoch 148, Loss: 0.4405450187623501\n",
      "Epoch 149, Loss: 0.5386695265769958\n",
      "Epoch 150, Loss: 0.530371218919754\n",
      "Epoch 151, Loss: 0.48761997371912\n",
      "Epoch 152, Loss: 0.5385226756334305\n",
      "Epoch 153, Loss: 0.488949179649353\n",
      "Epoch 154, Loss: 0.4969712495803833\n",
      "Epoch 155, Loss: 0.5371783971786499\n",
      "Epoch 156, Loss: 0.49879442900419235\n",
      "Epoch 157, Loss: 0.49250368773937225\n",
      "Epoch 158, Loss: 0.487432561814785\n",
      "Epoch 159, Loss: 0.5306109488010406\n",
      "Epoch 160, Loss: 0.4908648878335953\n",
      "Epoch 161, Loss: 0.4716307893395424\n",
      "Epoch 162, Loss: 0.5637626349925995\n",
      "Epoch 163, Loss: 0.49678491801023483\n",
      "Epoch 164, Loss: 0.4795311763882637\n",
      "Epoch 165, Loss: 0.5305690616369247\n",
      "Epoch 166, Loss: 0.4941069185733795\n",
      "Epoch 167, Loss: 0.4945119321346283\n",
      "Epoch 168, Loss: 0.5396913066506386\n",
      "Epoch 169, Loss: 0.7400040626525879\n",
      "Epoch 170, Loss: 0.4783840999007225\n",
      "Epoch 171, Loss: 0.4801783114671707\n",
      "Epoch 172, Loss: 0.44998112320899963\n",
      "Epoch 173, Loss: 0.5216943398118019\n",
      "Epoch 174, Loss: 0.45081527531147003\n",
      "Epoch 175, Loss: 0.5466858223080635\n",
      "Epoch 176, Loss: 0.5005265176296234\n",
      "Epoch 177, Loss: 0.48748528957366943\n",
      "Epoch 178, Loss: 0.5391101613640785\n",
      "Epoch 179, Loss: 0.5144386142492294\n",
      "Epoch 180, Loss: 0.4599840044975281\n",
      "Epoch 181, Loss: 0.4700165465474129\n",
      "Epoch 182, Loss: 0.4895744249224663\n",
      "Epoch 183, Loss: 0.4792153537273407\n",
      "Epoch 184, Loss: 0.47148482501506805\n",
      "Epoch 185, Loss: 0.4648526459932327\n",
      "Epoch 186, Loss: 0.5169813632965088\n",
      "Epoch 187, Loss: 0.47456688433885574\n",
      "Epoch 188, Loss: 0.4772573783993721\n",
      "Epoch 189, Loss: 0.6001342684030533\n",
      "Epoch 190, Loss: 0.5043391287326813\n",
      "Epoch 191, Loss: 0.5563690587878227\n",
      "Epoch 192, Loss: 0.5091657638549805\n",
      "Epoch 193, Loss: 0.46711868047714233\n",
      "Epoch 194, Loss: 0.4675368443131447\n",
      "Epoch 195, Loss: 0.4897259548306465\n",
      "Epoch 196, Loss: 0.4979940429329872\n",
      "Epoch 197, Loss: 0.4686659201979637\n",
      "Epoch 198, Loss: 0.5073509067296982\n",
      "Epoch 199, Loss: 0.471648633480072\n",
      "Epoch 200, Loss: 0.5092797875404358\n",
      "Epoch 201, Loss: 0.5438637658953667\n",
      "Epoch 202, Loss: 0.4605286195874214\n",
      "Epoch 203, Loss: 0.49604371190071106\n",
      "Epoch 204, Loss: 0.4619353115558624\n",
      "Epoch 205, Loss: 0.4473148547112942\n",
      "Epoch 206, Loss: 0.5152267962694168\n",
      "Epoch 207, Loss: 0.4738626107573509\n",
      "Epoch 208, Loss: 0.4837687015533447\n",
      "Epoch 209, Loss: 0.4850320965051651\n",
      "Epoch 210, Loss: 0.4627813622355461\n",
      "Epoch 211, Loss: 0.4404994659125805\n",
      "Epoch 212, Loss: 0.5070204809308052\n",
      "Epoch 213, Loss: 0.467465303838253\n",
      "Epoch 214, Loss: 0.514321006834507\n",
      "Epoch 215, Loss: 0.5071211531758308\n",
      "Epoch 216, Loss: 0.45711997523903847\n",
      "Epoch 217, Loss: 0.47148220241069794\n",
      "Epoch 218, Loss: 0.4738708958029747\n",
      "Epoch 219, Loss: 0.45490002632141113\n",
      "Epoch 220, Loss: 0.5169514641165733\n",
      "Epoch 221, Loss: 0.5132652744650841\n",
      "Epoch 222, Loss: 0.5027129724621773\n",
      "Epoch 223, Loss: 0.4758838936686516\n",
      "Epoch 224, Loss: 0.5026426687836647\n",
      "Epoch 225, Loss: 0.46152549982070923\n",
      "Epoch 226, Loss: 0.467463456094265\n",
      "Epoch 227, Loss: 0.5108449682593346\n",
      "Epoch 228, Loss: 0.4694190248847008\n",
      "Epoch 229, Loss: 0.5193755403161049\n",
      "Epoch 230, Loss: 0.4890555813908577\n",
      "Epoch 231, Loss: 0.44530492275953293\n",
      "Epoch 232, Loss: 0.570029504597187\n",
      "Epoch 233, Loss: 0.46996115148067474\n",
      "Epoch 234, Loss: 0.4711027070879936\n",
      "Epoch 235, Loss: 0.5245246887207031\n",
      "Epoch 236, Loss: 0.5181880444288254\n",
      "Epoch 237, Loss: 0.4782167673110962\n",
      "Epoch 238, Loss: 0.4434692859649658\n",
      "Epoch 239, Loss: 0.4645437076687813\n",
      "Epoch 240, Loss: 0.5361569449305534\n",
      "Epoch 241, Loss: 0.46551626920700073\n",
      "Epoch 242, Loss: 0.5350442379713058\n",
      "Epoch 243, Loss: 0.4734271392226219\n",
      "Epoch 244, Loss: 0.5584626123309135\n",
      "Epoch 245, Loss: 0.4749901592731476\n",
      "Epoch 246, Loss: 0.49451493471860886\n",
      "Epoch 247, Loss: 0.49293939769268036\n",
      "Epoch 248, Loss: 0.504075214266777\n",
      "Epoch 249, Loss: 0.49240633845329285\n",
      "Epoch 250, Loss: 0.4645390585064888\n",
      "Epoch 251, Loss: 0.47083815187215805\n",
      "Epoch 252, Loss: 0.44616609439253807\n",
      "Epoch 253, Loss: 0.46144064515829086\n",
      "Epoch 254, Loss: 0.48940984159708023\n",
      "Epoch 255, Loss: 0.45055732131004333\n",
      "Epoch 256, Loss: 0.47108032554388046\n",
      "Epoch 257, Loss: 0.6190093159675598\n",
      "Epoch 258, Loss: 0.5184467285871506\n",
      "Epoch 259, Loss: 0.4750164672732353\n",
      "Epoch 260, Loss: 0.4707699790596962\n",
      "Epoch 261, Loss: 0.489817813038826\n",
      "Epoch 262, Loss: 0.4773516431450844\n",
      "Epoch 263, Loss: 0.4782306030392647\n",
      "Epoch 264, Loss: 0.4741358160972595\n",
      "Epoch 265, Loss: 0.4930408000946045\n",
      "Epoch 266, Loss: 0.5162273347377777\n",
      "Epoch 267, Loss: 0.4495733454823494\n",
      "Epoch 268, Loss: 0.48598070442676544\n",
      "Epoch 269, Loss: 0.5153486281633377\n",
      "Epoch 270, Loss: 0.4615108221769333\n",
      "Epoch 271, Loss: 0.5032437667250633\n",
      "Epoch 272, Loss: 0.469721220433712\n",
      "Epoch 273, Loss: 0.5251536294817924\n",
      "Epoch 274, Loss: 0.457124225795269\n",
      "Epoch 275, Loss: 0.5833631902933121\n",
      "Epoch 276, Loss: 0.46618831902742386\n",
      "Epoch 277, Loss: 0.5122334808111191\n",
      "Epoch 278, Loss: 0.49447325617074966\n",
      "Epoch 279, Loss: 0.45637596398591995\n",
      "Epoch 280, Loss: 0.5017509609460831\n",
      "Epoch 281, Loss: 0.4734548553824425\n",
      "Epoch 282, Loss: 0.46590761095285416\n",
      "Epoch 283, Loss: 0.44828522577881813\n",
      "Epoch 284, Loss: 0.4714861065149307\n",
      "Epoch 285, Loss: 0.44964350014925003\n",
      "Epoch 286, Loss: 0.5175641998648643\n",
      "Epoch 287, Loss: 0.4447747729718685\n",
      "Epoch 288, Loss: 0.4654950946569443\n",
      "Epoch 289, Loss: 0.5225523710250854\n",
      "Epoch 290, Loss: 0.46786241233348846\n",
      "Epoch 291, Loss: 0.4703512638807297\n",
      "Epoch 292, Loss: 0.49565713852643967\n",
      "Epoch 293, Loss: 0.491158626973629\n",
      "Epoch 294, Loss: 0.4484308883547783\n",
      "Epoch 295, Loss: 0.48836807161569595\n",
      "Epoch 296, Loss: 0.47175872325897217\n",
      "Epoch 297, Loss: 0.47326933592557907\n",
      "Epoch 298, Loss: 0.5169887393712997\n",
      "Epoch 299, Loss: 0.44917625933885574\n",
      "Epoch 300, Loss: 0.47163084894418716\n",
      "Epoch 301, Loss: 0.46976396441459656\n",
      "Epoch 302, Loss: 0.5193421542644501\n",
      "Epoch 303, Loss: 0.44364170357584953\n",
      "Epoch 304, Loss: 0.5196172818541527\n",
      "Epoch 305, Loss: 0.47909310460090637\n",
      "Epoch 306, Loss: 0.5395536199212074\n",
      "Epoch 307, Loss: 0.496878057718277\n",
      "Epoch 308, Loss: 0.4968373477458954\n",
      "Epoch 309, Loss: 0.448210459202528\n",
      "Epoch 310, Loss: 0.5380112677812576\n",
      "Epoch 311, Loss: 0.541456051170826\n",
      "Epoch 312, Loss: 0.4835514575242996\n",
      "Epoch 313, Loss: 0.44392846152186394\n",
      "Epoch 314, Loss: 0.5111793875694275\n",
      "Epoch 315, Loss: 0.4826904311776161\n",
      "Epoch 316, Loss: 0.4343980588018894\n",
      "Epoch 317, Loss: 0.4612830653786659\n",
      "Epoch 318, Loss: 0.4385351575911045\n",
      "Epoch 319, Loss: 0.46399810165166855\n",
      "Epoch 320, Loss: 0.49582476168870926\n",
      "Epoch 321, Loss: 0.497542567551136\n",
      "Epoch 322, Loss: 0.4891592562198639\n",
      "Epoch 323, Loss: 0.45243309810757637\n",
      "Epoch 324, Loss: 0.5118344202637672\n",
      "Epoch 325, Loss: 0.5065418407320976\n",
      "Epoch 326, Loss: 0.4748079404234886\n",
      "Epoch 327, Loss: 0.4695814996957779\n",
      "Epoch 328, Loss: 0.5426873192191124\n",
      "Epoch 329, Loss: 0.4765733480453491\n",
      "Epoch 330, Loss: 0.47928861528635025\n",
      "Epoch 331, Loss: 0.48096752911806107\n",
      "Epoch 332, Loss: 0.5195384323596954\n",
      "Epoch 333, Loss: 0.4559320658445358\n",
      "Epoch 334, Loss: 0.4623034670948982\n",
      "Epoch 335, Loss: 0.46231314539909363\n",
      "Epoch 336, Loss: 0.46112556755542755\n",
      "Epoch 337, Loss: 0.44233566150069237\n",
      "Epoch 338, Loss: 0.4944603741168976\n",
      "Epoch 339, Loss: 0.44048312306404114\n",
      "Epoch 340, Loss: 0.48553115129470825\n",
      "Epoch 341, Loss: 0.4717100262641907\n",
      "Epoch 342, Loss: 0.5187907889485359\n",
      "Epoch 343, Loss: 0.49445749819278717\n",
      "Epoch 344, Loss: 0.49250132590532303\n",
      "Epoch 345, Loss: 0.46271320432424545\n",
      "Epoch 346, Loss: 0.4603527784347534\n",
      "Epoch 347, Loss: 0.4850134924054146\n",
      "Epoch 348, Loss: 0.5029123201966286\n",
      "Epoch 349, Loss: 0.5207099169492722\n",
      "Epoch 350, Loss: 0.5403036847710609\n",
      "Epoch 351, Loss: 0.5013786405324936\n",
      "Epoch 352, Loss: 0.4974915459752083\n",
      "Epoch 353, Loss: 0.5346597284078598\n",
      "Epoch 354, Loss: 0.5034563690423965\n",
      "Epoch 355, Loss: 0.47462525963783264\n",
      "Epoch 356, Loss: 0.49485691636800766\n",
      "Epoch 357, Loss: 0.4908469244837761\n",
      "Epoch 358, Loss: 0.5230224281549454\n",
      "Epoch 359, Loss: 0.46962249279022217\n",
      "Epoch 360, Loss: 0.47200171649456024\n",
      "Epoch 361, Loss: 0.5126972496509552\n",
      "Epoch 362, Loss: 0.49684809148311615\n",
      "Epoch 363, Loss: 0.47089722752571106\n",
      "Epoch 364, Loss: 0.4639587327837944\n",
      "Epoch 365, Loss: 0.4967243894934654\n",
      "Epoch 366, Loss: 0.4886167421936989\n",
      "Epoch 367, Loss: 0.4869999513030052\n",
      "Epoch 368, Loss: 0.49887412041425705\n",
      "Epoch 369, Loss: 0.46921516209840775\n",
      "Epoch 370, Loss: 0.444987665861845\n",
      "Epoch 371, Loss: 0.4389696381986141\n",
      "Epoch 372, Loss: 0.4670512154698372\n",
      "Epoch 373, Loss: 0.5771062076091766\n",
      "Epoch 374, Loss: 0.4797067567706108\n",
      "Epoch 375, Loss: 0.4924438148736954\n",
      "Epoch 376, Loss: 0.456839457154274\n",
      "Epoch 377, Loss: 0.5209139212965965\n",
      "Epoch 378, Loss: 0.5366856902837753\n",
      "Epoch 379, Loss: 0.45482610166072845\n",
      "Epoch 380, Loss: 0.43921423330903053\n",
      "Epoch 381, Loss: 0.474223792552948\n",
      "Epoch 382, Loss: 0.4462731368839741\n",
      "Epoch 383, Loss: 0.4881085976958275\n",
      "Epoch 384, Loss: 0.4862169474363327\n",
      "Epoch 385, Loss: 0.4613673835992813\n",
      "Epoch 386, Loss: 0.49715112894773483\n",
      "Epoch 387, Loss: 0.49805738776922226\n",
      "Epoch 388, Loss: 0.4678357467055321\n",
      "Epoch 389, Loss: 0.5775442570447922\n",
      "Epoch 390, Loss: 0.4749084487557411\n",
      "Epoch 391, Loss: 0.4780409559607506\n",
      "Epoch 392, Loss: 0.4989924728870392\n",
      "Epoch 393, Loss: 0.455564197152853\n",
      "Epoch 394, Loss: 0.461322546005249\n",
      "Epoch 395, Loss: 0.4683035761117935\n",
      "Epoch 396, Loss: 0.45774806290864944\n",
      "Epoch 397, Loss: 0.49169088155031204\n",
      "Epoch 398, Loss: 0.498555239289999\n",
      "Epoch 399, Loss: 0.49450694024562836\n",
      "Epoch 400, Loss: 0.517450287938118\n",
      "Epoch 401, Loss: 0.5268708541989326\n",
      "Epoch 402, Loss: 0.44430558755993843\n",
      "Epoch 403, Loss: 0.5190926343202591\n",
      "Epoch 404, Loss: 0.47481995820999146\n",
      "Epoch 405, Loss: 0.4976607635617256\n",
      "Epoch 406, Loss: 0.4421227499842644\n",
      "Epoch 407, Loss: 0.4733032286167145\n",
      "Epoch 408, Loss: 0.4586592763662338\n",
      "Epoch 409, Loss: 0.5079547613859177\n",
      "Epoch 410, Loss: 0.46183349937200546\n",
      "Epoch 411, Loss: 0.5099595189094543\n",
      "Epoch 412, Loss: 0.4994542971253395\n",
      "Epoch 413, Loss: 0.4637310728430748\n",
      "Epoch 414, Loss: 0.49237725883722305\n",
      "Epoch 415, Loss: 0.5102058053016663\n",
      "Epoch 416, Loss: 0.4733309671282768\n",
      "Epoch 417, Loss: 0.4923842325806618\n",
      "Epoch 418, Loss: 0.475765585899353\n",
      "Epoch 419, Loss: 0.4712640717625618\n",
      "Epoch 420, Loss: 0.5029465481638908\n",
      "Epoch 421, Loss: 0.514403834939003\n",
      "Epoch 422, Loss: 0.4781138598918915\n",
      "Epoch 423, Loss: 0.4765836372971535\n",
      "Epoch 424, Loss: 0.44579507037997246\n",
      "Epoch 425, Loss: 0.5541072711348534\n",
      "Epoch 426, Loss: 0.4988337382674217\n",
      "Epoch 427, Loss: 0.46165967732667923\n",
      "Epoch 428, Loss: 0.43810173869132996\n",
      "Epoch 429, Loss: 0.5330800488591194\n",
      "Epoch 430, Loss: 0.45525064319372177\n",
      "Epoch 431, Loss: 0.44440068677067757\n",
      "Epoch 432, Loss: 0.48240572959184647\n",
      "Epoch 433, Loss: 0.472702331840992\n",
      "Epoch 434, Loss: 0.5255373418331146\n",
      "Epoch 435, Loss: 0.4976147934794426\n",
      "Epoch 436, Loss: 0.4949624687433243\n",
      "Epoch 437, Loss: 0.4934696704149246\n",
      "Epoch 438, Loss: 0.5199133232235909\n",
      "Epoch 439, Loss: 0.4839368537068367\n",
      "Epoch 440, Loss: 0.46375512331724167\n",
      "Epoch 441, Loss: 0.47098344564437866\n",
      "Epoch 442, Loss: 0.4920915365219116\n",
      "Epoch 443, Loss: 0.4471172206103802\n",
      "Epoch 444, Loss: 0.4534207135438919\n",
      "Epoch 445, Loss: 0.5040069818496704\n",
      "Epoch 446, Loss: 0.43944020569324493\n",
      "Epoch 447, Loss: 0.46980955451726913\n",
      "Epoch 448, Loss: 0.43732530251145363\n",
      "Epoch 449, Loss: 0.44286151602864265\n",
      "Epoch 450, Loss: 0.4906334951519966\n",
      "Epoch 451, Loss: 0.5440223440527916\n",
      "Epoch 452, Loss: 0.4870016947388649\n",
      "Epoch 453, Loss: 0.5684031620621681\n",
      "Epoch 454, Loss: 0.4718286246061325\n",
      "Epoch 455, Loss: 0.5140955075621605\n",
      "Epoch 456, Loss: 0.5003661289811134\n",
      "Epoch 457, Loss: 0.5283265635371208\n",
      "Epoch 458, Loss: 0.5209613889455795\n",
      "Epoch 459, Loss: 0.4682294875383377\n",
      "Epoch 460, Loss: 0.5460256412625313\n",
      "Epoch 461, Loss: 0.5991993956267834\n",
      "Epoch 462, Loss: 0.629048079252243\n",
      "Epoch 463, Loss: 0.5316310375928879\n",
      "Epoch 464, Loss: 0.5317904874682426\n",
      "Epoch 465, Loss: 0.5495118424296379\n",
      "Epoch 466, Loss: 0.523376002907753\n",
      "Epoch 467, Loss: 0.46730755269527435\n",
      "Epoch 468, Loss: 0.5064716562628746\n",
      "Epoch 469, Loss: 0.47127803415060043\n",
      "Epoch 470, Loss: 0.46302685141563416\n",
      "Epoch 471, Loss: 0.5095648393034935\n",
      "Epoch 472, Loss: 0.4591021165251732\n",
      "Epoch 473, Loss: 0.49548789113759995\n",
      "Epoch 474, Loss: 0.5233602225780487\n",
      "Epoch 475, Loss: 0.44412001967430115\n",
      "Epoch 476, Loss: 0.5231409966945648\n",
      "Epoch 477, Loss: 0.4707300737500191\n",
      "Epoch 478, Loss: 0.5190644860267639\n",
      "Epoch 479, Loss: 0.4456601180136204\n",
      "Epoch 480, Loss: 0.5532798916101456\n",
      "Epoch 481, Loss: 0.49684441089630127\n",
      "Epoch 482, Loss: 0.4656401127576828\n",
      "Epoch 483, Loss: 0.4960436001420021\n",
      "Epoch 484, Loss: 0.5092974305152893\n",
      "Epoch 485, Loss: 0.499468170106411\n",
      "Epoch 486, Loss: 0.4666098281741142\n",
      "Epoch 487, Loss: 0.5151935592293739\n",
      "Epoch 488, Loss: 0.5391716510057449\n",
      "Epoch 489, Loss: 0.4869925230741501\n",
      "Epoch 490, Loss: 0.4732820764183998\n",
      "Epoch 491, Loss: 0.4926846772432327\n",
      "Epoch 492, Loss: 0.48425327241420746\n",
      "Epoch 493, Loss: 0.49069683998823166\n",
      "Epoch 494, Loss: 0.5337441340088844\n",
      "Epoch 495, Loss: 0.4909564256668091\n",
      "Epoch 496, Loss: 0.47958486527204514\n",
      "Epoch 497, Loss: 0.4724687412381172\n",
      "Epoch 498, Loss: 0.4780174493789673\n",
      "Epoch 499, Loss: 0.511587105691433\n",
      "Epoch 500, Loss: 0.44505488499999046\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 500  # The number of times the entire dataset is passed through the network\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        h = model.init_hidden()\n",
    "\n",
    "        # Process each time step\n",
    "        for t in range(inputs.shape[1]):  # iterate over time steps\n",
    "            output, h = model(inputs[:, t, :], h)\n",
    "\n",
    "        # Compute loss using the last output (if your task is many-to-one)\n",
    "        loss = criterion(output, targets[:, -1, :])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd9a5b-0e80-4464-aa8e-9ec54d225646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
