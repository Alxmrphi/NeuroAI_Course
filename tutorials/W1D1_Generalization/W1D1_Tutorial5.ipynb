{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12bb9e08-6996-4d6b-8b80-a7a26191c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2baca459-5a6a-489b-90d3-dffa0a4893e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .mat file\n",
    "data = scipy.io.loadmat('data/condsForSimJ2moMuscles.mat')\n",
    "\n",
    "# Extract condsForSim struct\n",
    "conds_for_sim = data['condsForSim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c15d51-8e59-4305-a2bc-e270ac349085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go Envelope Tensor Shape: torch.Size([216, 296, 1])\n",
      "Plan Tensor Shape: torch.Size([216, 296, 15])\n",
      "Muscle Tensor Shape: torch.Size([216, 296, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303238/1417502240.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  go_envelope_all.append(torch.tensor(go_envelope_condition, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store data for all conditions\n",
    "go_envelope_all = []\n",
    "plan_all = []\n",
    "muscle_all = []\n",
    "\n",
    "# Get the number of conditions (rows) and delay durations (columns)\n",
    "num_conditions, num_delays = conds_for_sim.shape\n",
    "\n",
    "# Loop through each condition and extract data\n",
    "for i in range(num_conditions):  # 27 conditions\n",
    "    go_envelope_condition = []\n",
    "    plan_condition = []\n",
    "    muscle_condition = []\n",
    "\n",
    "    for j in range(num_delays):  # 8 delay durations\n",
    "        condition = conds_for_sim[i, j]\n",
    "\n",
    "        go_envelope = condition['goEnvelope']\n",
    "        plan = condition['plan']\n",
    "        muscle = condition['muscle']\n",
    "\n",
    "        # Select only muscles 5 and 6 \n",
    "        selected_muscle_data = muscle[:, [4, 5]]  # Select columns for muscle 5 and 6, which show the nicest multiphasic activity\n",
    "\n",
    "        go_envelope_condition.append(go_envelope)\n",
    "        plan_condition.append(plan)\n",
    "        muscle_condition.append(selected_muscle_data)\n",
    "\n",
    "    # Stack data for each condition\n",
    "    go_envelope_all.append(torch.tensor(go_envelope_condition, dtype=torch.float32))\n",
    "    plan_all.append(torch.tensor(plan_condition, dtype=torch.float32))\n",
    "    muscle_all.append(torch.tensor(muscle_condition, dtype=torch.float32))\n",
    "\n",
    "# Stack data for all conditions\n",
    "go_envelope_tensor = torch.stack(go_envelope_all)\n",
    "plan_tensor = torch.stack(plan_all)\n",
    "muscle_tensor = torch.stack(muscle_all)\n",
    "\n",
    "# Reshape to merge the first two dimensions\n",
    "go_envelope_tensor = go_envelope_tensor.reshape(-1, *go_envelope_tensor.shape[2:])\n",
    "plan_tensor = plan_tensor.reshape(-1, *plan_tensor.shape[2:])\n",
    "muscle_tensor = muscle_tensor.reshape(-1, *muscle_tensor.shape[2:])\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Go Envelope Tensor Shape: {go_envelope_tensor.shape}\")\n",
    "print(f\"Plan Tensor Shape: {plan_tensor.shape}\")\n",
    "print(f\"Muscle Tensor Shape: {muscle_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9515c13-74d5-4fb5-8614-53fbb8dcfe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Go Envelope Tensor Shape: torch.Size([216, 296, 1])\n",
      "Normalized Plan Tensor Shape: torch.Size([216, 296, 15])\n",
      "Normalized Muscle Tensor Shape: torch.Size([216, 296, 2])\n"
     ]
    }
   ],
   "source": [
    "# Normalize and standardize a tensor\n",
    "def normalize_and_standardize(tensor):\n",
    "    # Normalize: Scale to 0-1 range\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    tensor = (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "    # Standardize: Shift to zero mean and unit variance\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    tensor = (tensor - mean) / std\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# Apply the function to each tensor\n",
    "normalized_go_envelope_tensor = normalize_and_standardize(go_envelope_tensor)\n",
    "normalized_plan_tensor = normalize_and_standardize(plan_tensor)\n",
    "normalized_muscle_tensor = normalize_and_standardize(muscle_tensor)\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(f\"Normalized Go Envelope Tensor Shape: {normalized_go_envelope_tensor.shape}\")\n",
    "print(f\"Normalized Plan Tensor Shape: {normalized_plan_tensor.shape}\")\n",
    "print(f\"Normalized Muscle Tensor Shape: {normalized_muscle_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2b6a95-49a0-4f10-8f63-3b85a7f7d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, g, h, tau=50):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = tau  # Time constant\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size)  # New linear layer\n",
    "\n",
    "        # Weight initialization\n",
    "        self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, input_size) * (h / torch.sqrt(torch.tensor(input_size, dtype=torch.float)))) \n",
    "        self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # Nonlinearity\n",
    "        self.nonlinearity = torch.tanh\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        hidden_prev = hidden.clone()\n",
    "        timestep = self.tau / 10  # Timestep for Euler integration\n",
    "    \n",
    "        # Process each time step in the batch\n",
    "        for t in range(x.size(1)):  # x.size(1) is the temporal dimension\n",
    "            x_t = x[:, t, :]  # Extract the data for the current time step\n",
    "    \n",
    "            for _ in range(int(1 / timestep)):  # Adjust the range for simulation duration\n",
    "                hidden_update = torch.matmul(self.J, hidden.transpose(0, 1))\n",
    "                input_update = torch.matmul(self.B, x_t.transpose(0, 1))\n",
    "                new_hidden = self.nonlinearity(hidden_update + input_update + self.bx.unsqueeze(1))\n",
    "                new_hidden = new_hidden.transpose(0, 1)\n",
    "    \n",
    "                # Euler integration for continuous-time update\n",
    "                hidden = hidden + (timestep / self.tau) * (-hidden + new_hidden)\n",
    "    \n",
    "        # Initialize output tensor\n",
    "        output = self.output_linear(hidden)  # Apply the output_linear layer\n",
    "    \n",
    "        # Calculate the sum of squares of the firing rates and dynamic regularization\n",
    "        firing_rate_reg = hidden.pow(2).sum()\n",
    "        dynamic_reg = (hidden - hidden_prev).pow(2).sum().sqrt()\n",
    "\n",
    "        return output, hidden, firing_rate_reg, dynamic_reg\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state with batch dimension\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 16\n",
    "hidden_size = 300\n",
    "output_size = 2  # Number of muscles\n",
    "g = 1.1  # g value\n",
    "h = 1.0  # h value\n",
    "\n",
    "# Model instantiation\n",
    "model = SimpleRNN(input_size, hidden_size, output_size, g, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de475624-9823-4fd4-a591-960795badf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ComplicatedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, g, h, tau=50):\n",
    "        super(ComplicatedRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = tau  # Time constant\n",
    "\n",
    "        # Modified weight initialization for a more chaotic regime (g >> 1)\n",
    "        self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, input_size) * (h / torch.sqrt(torch.tensor(input_size, dtype=torch.float))))\n",
    "        self.w = nn.Parameter(torch.zeros(output_size, hidden_size))\n",
    "        self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.bz = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "        # Nonlinearity remains the same\n",
    "        self.nonlinearity = torch.tanh\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        timestep = self.tau / 10  # Timestep for Euler integration\n",
    "        for _ in range(int(1 / timestep)):  # Adjust the range for simulation duration\n",
    "            hidden_update = torch.matmul(self.J, hidden.T)\n",
    "            input_update = torch.matmul(self.B, x.T)\n",
    "            new_hidden = hidden_update + input_update + self.bx.unsqueeze(1)  # Nonlinearity removed\n",
    "            new_hidden = new_hidden.T\n",
    "    \n",
    "            # Euler integration for continuous-time update\n",
    "            hidden = hidden + (timestep / self.tau) * (-hidden + new_hidden)\n",
    "        \n",
    "        output = torch.matmul(self.w, hidden.T) + self.bz.unsqueeze(1)\n",
    "        output = output.T\n",
    "    \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state with zeros and correct batch dimension\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "input_size = 16\n",
    "hidden_size = 300\n",
    "output_size = 2 # Number of muscles\n",
    "g = 10.0 # Significantly larger g value for complicated model\n",
    "h = 1.0 # h value remains the same\n",
    "\n",
    "complicated_model = ComplicatedRNN(input_size, hidden_size, output_size, g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "711bd5b4-d92c-43e4-b3c6-fb0a6fabb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to TensorDataset and DataLoader for batch processing\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Concatenate along the third dimension (dim=2)\n",
    "X_train = torch.cat((normalized_go_envelope_tensor, normalized_plan_tensor), dim=2)\n",
    "\n",
    "# y_train remains the same\n",
    "y_train = normalized_muscle_tensor\n",
    "\n",
    "batch_size = 64  # You can adjust this based on your data size and memory constraints\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72179104-fa01-47b6-87ea-22b40e5f544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # MSE Loss for regression tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85120248-46b3-44c7-b8dc-9408f493bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l2_regularization(parameters, alpha):\n",
    "    l2_reg = sum(p.pow(2.0).sum() for p in parameters)\n",
    "    return alpha * l2_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b46e0da9-238e-4bc8-a2be-a4dbc562731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 296, 16])\n",
      "torch.Size([64, 16])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 26\u001b[0m     output, h, firing_rate_reg, dynamic_reg \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     total_firing_rate_reg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m firing_rate_reg\n\u001b[1;32m     28\u001b[0m     total_dynamic_reg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dynamic_reg\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m, in \u001b[0;36mSimpleRNN.forward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Process each time step in the batch\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)):  \u001b[38;5;66;03m# x.size(1) is the temporal dimension\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     x_t \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Extract the data for the current time step\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m timestep)):  \u001b[38;5;66;03m# Adjust the range for simulation duration\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         hidden_update \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ, hidden\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters for regularization\n",
    "alpha = 0.001  # Example value, adjust as necessary\n",
    "beta = 0.001\n",
    "gamma = 0.001\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 250  # The number of times the entire dataset is passed through the network\n",
    "epoch_losses = []  # List to store average loss of each epoch\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        batch_size = inputs.size(0)\n",
    "        h = model.init_hidden(batch_size) # Initialize hidden state with the current batch size\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_firing_rate_reg = 0\n",
    "        total_dynamic_reg = 0\n",
    "\n",
    "        # Process each time step in the inputs\n",
    "        for t in range(inputs.shape[1]):\n",
    "            print(inputs.shape)\n",
    "            output, h, firing_rate_reg, dynamic_reg = model(inputs[:, t, :], h)\n",
    "            total_firing_rate_reg += firing_rate_reg\n",
    "            total_dynamic_reg += dynamic_reg\n",
    "    \n",
    "        # Compute loss using the last output and include regularization terms:\n",
    "        loss = criterion(output, targets[:, -1, :])\n",
    "        l2_reg = compute_l2_regularization(model.parameters(), alpha)\n",
    "        rfr_reg = beta * total_firing_rate_reg / inputs.shape[1]  # Average over time steps\n",
    "        rj_reg = gamma * total_dynamic_reg / inputs.shape[1]  # Average over time steps\n",
    "    \n",
    "        total_loss = loss + l2_reg + rfr_reg + rj_reg\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21dd9a5b-0e80-4464-aa8e-9ec54d225646",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SimpleRNN.init_hidden() missing 1 required positional argument: 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_hidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Process each time step\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):  \u001b[38;5;66;03m# iterate over time steps\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: SimpleRNN.init_hidden() missing 1 required positional argument: 'batch_size'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 250  # The number of times the entire dataset is passed through the network\n",
    "epoch_losses = []  # List to store average loss of each epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        h = model.init_hidden()\n",
    "\n",
    "        # Process each time step\n",
    "        for t in range(inputs.shape[1]):  # iterate over time steps\n",
    "            output, h = complicated_model(inputs[:, t, :], h)\n",
    "\n",
    "        # Compute loss using the last output (if your task is many-to-one)\n",
    "        loss = criterion(output, targets[:, -1, :])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f707ff9a-6208-41d9-b38b-66f3b6fd0277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
