{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12bb9e08-6996-4d6b-8b80-a7a26191c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85fea2ec-e1ee-4ebe-9dff-d3b253c3ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for file and download URL\n",
    "fname = \"condsForSimJ2moMuscles.mat\"  # The name of the file to be downloaded\n",
    "url = \"https://osf.io/wak7e/download\" # URL from where the file will be downloaded\n",
    "expected_md5 = \"257d16c4d92759d615bf5cac75dd9a1f\" # MD5 hash for verifying file integrity\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        # Attempt to download the file\n",
    "        r = requests.get(url) # Make a GET request to the specified URL\n",
    "    except requests.ConnectionError:\n",
    "        # Handle connection errors during the download\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        # No connection errors, proceed to check the response\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            # Check if the HTTP response status code indicates a successful download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "            # Verify the integrity of the downloaded file using MD5 checksum\n",
    "            print(\"!!! Data download appears corrupted !!!\")\n",
    "        else:\n",
    "            # If download is successful and data is not corrupted, save the file\n",
    "            with open(fname, \"wb\") as fid:\n",
    "                fid.write(r.content) # Write the downloaded content to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2baca459-5a6a-489b-90d3-dffa0a4893e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .mat file\n",
    "data = scipy.io.loadmat('condsForSimJ2moMuscles.mat')\n",
    "\n",
    "# Extract condsForSim struct\n",
    "conds_for_sim = data['condsForSim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c024154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaner but same as cell below.min\n",
    "# go_envelope_tensor = torch.Tensor([condition['goEnvelope'] for all_delayed_conditions in conds_for_sim for condition in all_delayed_conditions])\n",
    "# plan_tensor = torch.Tensor([condition['plan'] for all_delayed_conditions in conds_for_sim for condition in all_delayed_conditions])\n",
    "# muscle_tensor = torch.Tensor([condition['muscle'][:,[4,5]] for all_delayed_conditions in conds_for_sim for condition in all_delayed_conditions])\n",
    "# print(f\"Go Envelope Tensor Shape: {go_envelope_tensor.shape}\")\n",
    "# print(f\"Plan Tensor Shape: {plan_tensor.shape}\")\n",
    "# print(f\"Muscle Tensor Shape: {muscle_tensor.shape}\")\n",
    "\n",
    "# num_conditions = len(conds_for_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c15d51-8e59-4305-a2bc-e270ac349085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go Envelope Tensor Shape: torch.Size([216, 296, 1])\n",
      "Plan Tensor Shape: torch.Size([216, 296, 15])\n",
      "Muscle Tensor Shape: torch.Size([216, 296, 2])\n"
     ]
    }
   ],
   "source": [
    "# Load the .mat file\n",
    "data = scipy.io.loadmat('condsForSimJ2moMuscles.mat')\n",
    "\n",
    "# Extract condsForSim struct\n",
    "conds_for_sim = data['condsForSim']\n",
    "\n",
    "# Initialize lists to store data for all conditions\n",
    "go_envelope_all = []\n",
    "plan_all = []\n",
    "muscle_all = []\n",
    "\n",
    "# Get the number of conditions (rows) and delay durations (columns)\n",
    "num_conditions, num_delays = conds_for_sim.shape\n",
    "\n",
    "# Loop through each condition and extract data\n",
    "for i in range(num_conditions):  # 27 conditions\n",
    "    go_envelope_condition = []\n",
    "    plan_condition = []\n",
    "    muscle_condition = []\n",
    "\n",
    "    for j in range(num_delays):  # 8 delay durations\n",
    "        condition = conds_for_sim[i, j]\n",
    "\n",
    "        go_envelope = condition['goEnvelope']\n",
    "        plan = condition['plan']\n",
    "        muscle = condition['muscle']\n",
    "\n",
    "        # Select only muscles 5 and 6 \n",
    "        selected_muscle_data = muscle[:, [4, 5]]  # which show the nicest multiphasic activity\n",
    "\n",
    "        go_envelope_condition.append(go_envelope)\n",
    "        plan_condition.append(plan)\n",
    "        muscle_condition.append(selected_muscle_data)\n",
    "\n",
    "    # Convert lists of NumPy arrays to single NumPy arrays before conversion to tensors\n",
    "    go_envelope_np = np.array(go_envelope_condition)\n",
    "    plan_np = np.array(plan_condition)\n",
    "    muscle_np = np.array(muscle_condition)\n",
    "\n",
    "    # Convert the single NumPy arrays to PyTorch tensors\n",
    "    go_envelope_all.append(torch.tensor(go_envelope_np, dtype=torch.float32))\n",
    "    plan_all.append(torch.tensor(plan_np, dtype=torch.float32))\n",
    "    muscle_all.append(torch.tensor(muscle_np, dtype=torch.float32))\n",
    "\n",
    "# Stack data for all conditions\n",
    "go_envelope_tensor = torch.stack(go_envelope_all)\n",
    "plan_tensor = torch.stack(plan_all)\n",
    "muscle_tensor = torch.stack(muscle_all)\n",
    "\n",
    "# Reshape to merge the first two dimensions\n",
    "go_envelope_tensor = go_envelope_tensor.reshape(-1, *go_envelope_tensor.shape[2:])\n",
    "plan_tensor = plan_tensor.reshape(-1, *plan_tensor.shape[2:])\n",
    "muscle_tensor = muscle_tensor.reshape(-1, *muscle_tensor.shape[2:])\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Go Envelope Tensor Shape: {go_envelope_tensor.shape}\")\n",
    "print(f\"Plan Tensor Shape: {plan_tensor.shape}\")\n",
    "print(f\"Muscle Tensor Shape: {muscle_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "417463ca-29c8-4229-baed-58dcf390747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization and Standardization\n",
    "def normalize_and_standardize(tensor):\n",
    "\n",
    "    # Standardize: shift to zero mean and unit variance\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    tensor = (tensor - mean) / std\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# Apply normalization and standardization to go_envelope_tensor and plan_tensor\n",
    "normalized_go_envelope_tensor = normalize_and_standardize(go_envelope_tensor)\n",
    "normalized_plan_tensor = normalize_and_standardize(plan_tensor)\n",
    "normalized_muscle_tensor = normalize_and_standardize(muscle_tensor)\n",
    "\n",
    "# Concatenate normalized tensors along the last dimension\n",
    "X_train = torch.cat((normalized_go_envelope_tensor, normalized_plan_tensor), dim=2)\n",
    "y_train = normalized_muscle_tensor\n",
    "\n",
    "batch_size = 256  # You can adjust this based on your data size and memory constraints\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2b6a95-49a0-4f10-8f63-3b85a7f7d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Rectified Tanh activation function\n",
    "def rectified_tanh(x):\n",
    "    return torch.where(x > 0, torch.tanh(x), 0) # was torch.where(x > 0, x, torch.tanh(x))\n",
    "def grad_rectified_tanh(x):\n",
    "    return torch.where(x > 0, 1 - torch.tanh(x)**2, 0)\n",
    "def grad_tanh(x):\n",
    "    return 1 - torch.tanh(x)**2\n",
    "    \n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, g, h, tau=50):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = tau  # Time constant\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size)  # New linear layer\n",
    "\n",
    "        # Weight initialization\n",
    "        self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, input_size) * (h / torch.sqrt(torch.tensor(input_size, dtype=torch.float)))) \n",
    "        self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # Nonlinearity\n",
    "        self.nonlinearity = rectified_tanh \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        hidden_prev = hidden.clone()\n",
    "        timestep = self.tau / 10  # Timestep for Euler integration\n",
    "        # Update hidden state\n",
    "        firing_rate = self.nonlinearity(hidden)\n",
    "        hidden_update = torch.matmul(self.J, firing_rate.transpose(0, 1))\n",
    "\n",
    "        input_update = torch.matmul(self.B, x.transpose(0, 1))\n",
    "        new_hidden = hidden_update + input_update + self.bx.unsqueeze(1)\n",
    "        new_hidden = new_hidden.transpose(0, 1)\n",
    "\n",
    "        # Euler integration for continuous-time update\n",
    "        hidden = hidden + (timestep / self.tau) * (-hidden + new_hidden)\n",
    "\n",
    "        # Output calculation\n",
    "        output = self.output_linear(firing_rate)\n",
    "\n",
    "        # Regularization terms\n",
    "        firing_rate_reg = hidden.pow(2).sum() \n",
    "        \n",
    "        dynamic_reg = torch.linalg.norm(torch.matmul(self.J, grad_rectified_tanh(hidden.transpose(0, 1))), ord='fro', dim=(-2, -1)).sum()\n",
    "\n",
    "        return output, hidden, firing_rate_reg, dynamic_reg\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state with batch dimension\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "def compute_l2_regularization(parameters, alpha):\n",
    "    l2_reg = sum(p.pow(2.0).sum() for p in parameters)\n",
    "    return alpha * l2_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e0da9-238e-4bc8-a2be-a4dbc562731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.355607271194458\n",
      "Epoch 2, Loss: 1.1729286909103394\n",
      "Epoch 3, Loss: 1.095632791519165\n",
      "Epoch 4, Loss: 1.0333561897277832\n",
      "Epoch 5, Loss: 0.9650463461875916\n",
      "Epoch 6, Loss: 0.8963386416435242\n",
      "Epoch 7, Loss: 0.8355063199996948\n",
      "Epoch 8, Loss: 0.781886100769043\n",
      "Epoch 9, Loss: 0.7321465611457825\n",
      "Epoch 10, Loss: 0.6890197992324829\n",
      "Epoch 11, Loss: 0.6528289914131165\n",
      "Epoch 12, Loss: 0.6124275326728821\n",
      "Epoch 13, Loss: 0.5731521248817444\n",
      "Epoch 14, Loss: 0.5419652462005615\n",
      "Epoch 15, Loss: 0.5236414074897766\n",
      "Epoch 16, Loss: 0.5100806355476379\n",
      "Epoch 17, Loss: 0.4783361852169037\n",
      "Epoch 18, Loss: 0.4961172640323639\n",
      "Epoch 19, Loss: 0.44894516468048096\n",
      "Epoch 20, Loss: 0.4552558660507202\n",
      "Epoch 21, Loss: 0.44219475984573364\n",
      "Epoch 22, Loss: 0.4161531329154968\n",
      "Epoch 23, Loss: 0.423766165971756\n",
      "Epoch 24, Loss: 0.40242844820022583\n",
      "Epoch 25, Loss: 0.4015916585922241\n",
      "Epoch 26, Loss: 0.39356377720832825\n",
      "Epoch 27, Loss: 0.3842301368713379\n",
      "Epoch 28, Loss: 0.3834291696548462\n",
      "Epoch 29, Loss: 0.36995774507522583\n",
      "Epoch 30, Loss: 0.3684465289115906\n",
      "Epoch 31, Loss: 0.36136436462402344\n",
      "Epoch 32, Loss: 0.3532460331916809\n",
      "Epoch 33, Loss: 0.3534466028213501\n",
      "Epoch 34, Loss: 0.3431827127933502\n",
      "Epoch 35, Loss: 0.34133005142211914\n",
      "Epoch 36, Loss: 0.3364391326904297\n",
      "Epoch 37, Loss: 0.3309027850627899\n",
      "Epoch 38, Loss: 0.32797709107398987\n",
      "Epoch 39, Loss: 0.32271045446395874\n",
      "Epoch 40, Loss: 0.3201479911804199\n",
      "Epoch 41, Loss: 0.31433457136154175\n",
      "Epoch 42, Loss: 0.31313300132751465\n",
      "Epoch 43, Loss: 0.3068491816520691\n",
      "Epoch 44, Loss: 0.30521875619888306\n",
      "Epoch 45, Loss: 0.30083227157592773\n",
      "Epoch 46, Loss: 0.29835426807403564\n",
      "Epoch 47, Loss: 0.294179230928421\n",
      "Epoch 48, Loss: 0.29209280014038086\n",
      "Epoch 49, Loss: 0.28789976239204407\n",
      "Epoch 50, Loss: 0.2861512303352356\n",
      "Epoch 51, Loss: 0.2823951244354248\n",
      "Epoch 52, Loss: 0.2801801860332489\n",
      "Epoch 53, Loss: 0.2771151065826416\n",
      "Epoch 54, Loss: 0.27485737204551697\n",
      "Epoch 55, Loss: 0.2721487581729889\n",
      "Epoch 56, Loss: 0.269575834274292\n",
      "Epoch 57, Loss: 0.2678698003292084\n",
      "Epoch 58, Loss: 0.26660043001174927\n",
      "Epoch 59, Loss: 0.26543664932250977\n",
      "Epoch 60, Loss: 0.2640882134437561\n",
      "Epoch 61, Loss: 0.2592097818851471\n",
      "Epoch 62, Loss: 0.25712114572525024\n",
      "Epoch 63, Loss: 0.2564043700695038\n",
      "Epoch 64, Loss: 0.25259971618652344\n",
      "Epoch 65, Loss: 0.2515256404876709\n",
      "Epoch 66, Loss: 0.2506062984466553\n",
      "Epoch 67, Loss: 0.24751774966716766\n",
      "Epoch 68, Loss: 0.2466088831424713\n",
      "Epoch 69, Loss: 0.24509280920028687\n",
      "Epoch 70, Loss: 0.24322040379047394\n",
      "Epoch 71, Loss: 0.24402427673339844\n",
      "Epoch 72, Loss: 0.24636560678482056\n",
      "Epoch 73, Loss: 0.23906318843364716\n",
      "Epoch 74, Loss: 0.24032780528068542\n",
      "Epoch 75, Loss: 0.23678304255008698\n",
      "Epoch 76, Loss: 0.2366311252117157\n",
      "Epoch 77, Loss: 0.23369356989860535\n",
      "Epoch 78, Loss: 0.23304441571235657\n",
      "Epoch 79, Loss: 0.23089483380317688\n",
      "Epoch 80, Loss: 0.23017749190330505\n",
      "Epoch 81, Loss: 0.22754088044166565\n",
      "Epoch 82, Loss: 0.2279040515422821\n",
      "Epoch 83, Loss: 0.22506898641586304\n",
      "Epoch 84, Loss: 0.22487697005271912\n",
      "Epoch 85, Loss: 0.22235281765460968\n",
      "Epoch 86, Loss: 0.22261151671409607\n",
      "Epoch 87, Loss: 0.2202269732952118\n",
      "Epoch 88, Loss: 0.21961833536624908\n",
      "Epoch 89, Loss: 0.2177157998085022\n",
      "Epoch 90, Loss: 0.21776077151298523\n",
      "Epoch 91, Loss: 0.21637319028377533\n",
      "Epoch 92, Loss: 0.2152862548828125\n",
      "Epoch 93, Loss: 0.21329599618911743\n",
      "Epoch 94, Loss: 0.21289844810962677\n",
      "Epoch 95, Loss: 0.21220195293426514\n",
      "Epoch 96, Loss: 0.2112317681312561\n",
      "Epoch 97, Loss: 0.20926684141159058\n",
      "Epoch 98, Loss: 0.2083931714296341\n",
      "Epoch 99, Loss: 0.20791412889957428\n",
      "Epoch 100, Loss: 0.20698170363903046\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 16\n",
    "hidden_size = 300\n",
    "output_size = 2  # Number of muscles\n",
    "g = 1.5  # g value\n",
    "h_val = 1.0  # h value\n",
    "\n",
    "# Hyperparameters for regularization\n",
    "alpha = 1e-4  \n",
    "beta = 0.03\n",
    "gamma = 1e-4\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 100\n",
    "epoch_losses = [] \n",
    "\n",
    "# get available device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Model instantiation\n",
    "model = SimpleRNN(input_size, hidden_size, output_size, g, h_val)\n",
    "model.to(device)\n",
    "hidden_states_for_plot = []\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # MSE Loss for regression tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0) ### WD=0. Note: Paper uses Hessian-Free optimizer\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        h = model.init_hidden(batch_size).to(device) # Initialize hidden state with the current batch size\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_firing_rate_reg = 0\n",
    "        total_dynamic_reg = 0\n",
    "\n",
    "        for t in range(inputs.shape[1]):  # Iterate over time steps\n",
    "            output, h, firing_rate_reg, dynamic_reg = model(inputs[:, t, :], h)\n",
    "            hidden_states_for_plot.append(h.detach().cpu().numpy())\n",
    "            total_firing_rate_reg += firing_rate_reg\n",
    "            total_dynamic_reg += dynamic_reg\n",
    "   \n",
    "        # Compute loss using the last output and include regularization terms\n",
    "        loss = criterion(output, targets[:, -1, :])\n",
    "        l2_reg = compute_l2_regularization(model.parameters(), alpha)\n",
    "        rfr_reg = beta * total_firing_rate_reg / inputs.shape[1] / hidden_size /num_conditions  # Average over time steps\n",
    "        rj_reg = gamma * total_firing_rate_reg / inputs.shape[1] /num_conditions # Average over time steps\n",
    "    \n",
    "        total_loss = loss + l2_reg + rfr_reg + rj_reg\n",
    "\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "    \n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    avg_loss = running_loss / float(len(train_loader))\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# After training, convert the list of hidden states to a NumPy array for easier manipulation\n",
    "hidden_states_for_plot = np.array(hidden_states_for_plot)\n",
    "print(hidden_states_for_plot.shape)\n",
    "\n",
    "hidden_size = 300  # This should match the hidden size of your RNN\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de475624-9823-4fd4-a591-960795badf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  NEEDS UPDATING BASED ON CHANGES ABOVE ###\n",
    "\n",
    "\n",
    "\n",
    "# class ComplicatedRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, g, h, tau=50):\n",
    "#         super(ComplicatedRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.tau = tau  # Time constant\n",
    "\n",
    "#         # Modified weight initialization for a more chaotic regime (g >> 1)\n",
    "#         self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "#         self.B = nn.Parameter(torch.randn(hidden_size, input_size) * (h / torch.sqrt(torch.tensor(input_size, dtype=torch.float)))) \n",
    "#         self.w = nn.Parameter(torch.zeros(output_size, hidden_size))\n",
    "#         self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "#         self.bz = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "#         # Nonlinearity remains the same\n",
    "#         self.nonlinearity = torch.tanh\n",
    "    \n",
    "#     def forward(self, x, hidden):\n",
    "#         timestep = self.tau / 10  # Timestep for Euler integration\n",
    "#         for _ in range(int(1 / timestep)):  # Adjust the range for simulation duration\n",
    "#             hidden_update = torch.matmul(self.J, hidden.T)\n",
    "#             input_update = torch.matmul(self.B, x.T)\n",
    "#             new_hidden = self.nonlinearity(hidden_update + input_update + self.bx.unsqueeze(1))\n",
    "#             new_hidden = new_hidden.T\n",
    "\n",
    "#             # Euler integration for continuous-time update\n",
    "#             hidden = hidden + (timestep / self.tau) * (-hidden + new_hidden)\n",
    "        \n",
    "#         output = torch.matmul(self.w, hidden.T) + self.bz.unsqueeze(1)\n",
    "#         output = output.T\n",
    "    \n",
    "#         return output, hidden\n",
    "\n",
    "#     def init_hidden(self):\n",
    "#         # Initialize hidden state with an additional batch dimension\n",
    "#         return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "# input_size = 7\n",
    "# hidden_size = 300\n",
    "# output_size = 2 # Number of muscles\n",
    "# g = 4 # Significantly larger g value for complicated model\n",
    "# h = 1.0 # h value remains the same\n",
    "\n",
    "# complicated_model = ComplicatedRNN(input_size, hidden_size, output_size, g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd9a5b-0e80-4464-aa8e-9ec54d225646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training Loop\n",
    "# num_epochs = 250  # The number of epochs\n",
    "# epoch_losses = []  # To store average loss per epoch\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for inputs, targets in train_loader:\n",
    "#         optimizer2.zero_grad()\n",
    "#         batch_size = inputs.size(0)\n",
    "#         h = complicated_model.init_hidden()  # Initialize hidden state\n",
    "\n",
    "#         total_loss = 0.0  # Accumulate loss over time steps\n",
    "\n",
    "#         for t in range(inputs.shape[1]):\n",
    "#             output, h = complicated_model(inputs[:, t, :], h)\n",
    "#             loss = criterion(output, targets[:, t, :])  # Compute loss at each time step\n",
    "#             total_loss += loss\n",
    "\n",
    "#         total_loss.backward()  # Backpropagation on the accumulated loss\n",
    "#         optimizer2.step()\n",
    "\n",
    "#         running_loss += total_loss.item()\n",
    "\n",
    "#     avg_loss = running_loss / len(train_loader)\n",
    "#     epoch_losses.append(avg_loss)\n",
    "#     print(f'Epoch {epoch + 1}, Loss: {avg_loss}')\n",
    "\n",
    "# print('Finished Training')\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(1, num_epochs + 1), epoch_losses, label='Training Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss Over Epochs')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f707ff9a-6208-41d9-b38b-66f3b6fd0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_psth(data, title):\n",
    "    \"\"\"\n",
    "    Plot Peri-Stimulus Time Histogram (PSTH) for given data.\n",
    "    :param data: a tensor containing the neural data\n",
    "    :param title: a string for the plot title\n",
    "    \"\"\"\n",
    "    # Mean across trials\n",
    "    mean_data = data.mean(dim=0)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(mean_data)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Activity')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot PSTHs for brain data\n",
    "plot_psth(normalized_muscle_tensor, \"Brain Data PSTH\")\n",
    "\n",
    "def plot_rnn_psths(model, data_loader, title, max_time_steps, neurons=20):\n",
    "    \"\"\"\n",
    "    Generate and plot PSTH-like plots for RNN hidden units.\n",
    "    :param model: the RNN model\n",
    "    :param data_loader: DataLoader containing input data\n",
    "    :param title: a string for the plot title\n",
    "    :param max_time_steps: maximum number of time steps in the sequences\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to('cpu')\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computations, since we don't need them during evaluation\n",
    "        all_hidden_states = []  # List to collect hidden states from all batches\n",
    "\n",
    "        for inputs, _ in data_loader:  # Iterate over batches\n",
    "            h = model.init_hidden(inputs.size(0))  # Initialize the hidden state for this batch\n",
    "            # Create an array to store hidden states for each time step of the current batch\n",
    "            hidden_states = np.zeros((max_time_steps, h.shape[0], h.shape[1]))\n",
    "\n",
    "            for t in range(inputs.shape[1]):  # Iterate over each time step\n",
    "                _, h, _, _ = model(inputs[:, t, :], h)  # Forward pass for each time step, update hidden state\n",
    "                hidden_states[t, :, :] = h.detach().numpy()  # Detach and store the hidden state\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(200):  # Iterate over five hidden units\n",
    "    plt.plot(avg_hidden_states[:, i], label=f'Unit {i+1}')\n",
    "\n",
    "        # Stack all hidden states and compute their mean across batches and time steps\n",
    "        all_hidden_states_np = np.concatenate(all_hidden_states,0)\n",
    "        avg_hidden_states = np.mean(all_hidden_states_np, axis=1)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i in range(min(neurons,avg_hidden_states.shape[1])):\n",
    "            plt.plot(avg_hidden_states[:, i], label=f'Unit {i+1}')\n",
    "\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Hidden Unit Activation')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        return all_hidden_states_np\n",
    "\n",
    "# Determine the maximum number of time steps in your dataset\n",
    "max_time_steps = max([inputs.shape[1] for inputs, _ in train_loader])\n",
    "\n",
    "# Plot PSTH-like plots for RNN hidden units\n",
    "all_hidden_states_np= plot_rnn_psths(model, train_loader, \"RNN Hidden Units Activation\", max_time_steps)\n",
    "\n",
    "# Plot PSTH-like plots for RNN hidden units\n",
    "# plot_rnn_psths(complicated_model, train_loader, \"RNN Hidden Units Activation\", max_time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d4256-8aa3-423e-9e9a-24d3cb675808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add gaussian noise to the input data.\n",
    "def perturb_inputs(inputs, noise_level=0.1):\n",
    "    # Generating noise with the same shape as inputs\n",
    "    noise = torch.randn_like(inputs) * noise_level\n",
    "    # Adding the generated noise to the inputs and returning the noisy inputs\n",
    "    return inputs + noise\n",
    "\n",
    "# Function to add noise to the weights of a neural network model\n",
    "def perturb_weights(model, perturbation_factor=0.1):\n",
    "    # Iterating over all parameters (weights and biases) of the model\n",
    "    for param in model.parameters():\n",
    "        # Checking if the parameter is trainable \n",
    "        if param.requires_grad:\n",
    "            # Adding in-place Gaussian noise to the parameter\n",
    "            param.data.add_(torch.randn_like(param) * perturbation_factor)\n",
    "\n",
    "# Function to evaluate the model's performance on a dataset\n",
    "def evaluate_model(model, data_loader, criterion, complicated=False):\n",
    "    # Setting the model to evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    # Disabling gradient calculation as it's not needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterating through the dataset using the data loader\n",
    "        for inputs, targets in data_loader:\n",
    "            # Determining the batch size from the input shape\n",
    "            batch_size = inputs.size(0)\n",
    "            # Initializing the hidden state of the model for the current batch\n",
    "            # Iterating over each time step in the input sequence\n",
    "            for t in range(inputs.shape[1]):\n",
    "                # Conditionally handling the model's forward pass based on the 'complicated' flag\n",
    "                if complicated:\n",
    "                    h = model.init_hidden()\n",
    "                    # For a 'complicated' model, forward pass with the current timestep's input and hidden state.\n",
    "                    output, h = model(inputs[:, t, :], h)\n",
    "                else:\n",
    "                    h = model.init_hidden(batch_size)\n",
    "                    # For a simpler model, forward pass with additional unused outputs\n",
    "                    output, h, _, _ = model(inputs[:, t, :], h)\n",
    "                    # Calculating the loss between the model output and the target for the last timestep\n",
    "                    loss = criterion(output, targets[:, -1, :])\n",
    "                    # Accumulating the loss for the current batch\n",
    "                    total_loss += loss.item()\n",
    "                    # Returning the average loss across all batches in the data loader\n",
    "                return total_loss / len(data_loader)\n",
    "                \n",
    "def assess_robustness(model, data_loader, criterion, perturb_function, perturbation_level, complicated=False):\n",
    "    # Evaluate on non-perturbed data\n",
    "    original_loss = evaluate_model(model, data_loader, criterion, complicated)\n",
    "    # Perturb model\n",
    "    perturb_function(model, perturbation_level)\n",
    "    # Evaluate on perturbed data\n",
    "    perturbed_loss = evaluate_model(model, data_loader, criterion, complicated)\n",
    "    return original_loss, perturbed_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a58543-1e92-4ec5-a084-d56b53de7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and compare robustness\n",
    "simple_original_loss, simple_perturbed_loss = assess_robustness(model, train_loader, criterion, perturb_weights, 0.1)\n",
    "complex_original_loss, complex_perturbed_loss = assess_robustness(complicated_model, train_loader, criterion, perturb_weights, 0.1, complicated=True)\n",
    "\n",
    "print(f\"Simple Model - Original Loss: {simple_original_loss}, Perturbed Loss: {simple_perturbed_loss}\")\n",
    "print(f\"Complex Model - Original Loss: {complex_original_loss}, Perturbed Loss: {complex_perturbed_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
