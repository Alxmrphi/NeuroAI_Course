{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12bb9e08-6996-4d6b-8b80-a7a26191c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85fea2ec-e1ee-4ebe-9dff-d3b253c3ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for file and download URL\n",
    "fname = \"condsForSimJ2moMuscles.mat\"  # The name of the file to be downloaded\n",
    "url = \"https://osf.io/wak7e/download\" # URL from where the file will be downloaded\n",
    "expected_md5 = \"257d16c4d92759d615bf5cac75dd9a1f\" # MD5 hash for verifying file integrity\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        # Attempt to download the file\n",
    "        r = requests.get(url) # Make a GET request to the specified URL\n",
    "    except requests.ConnectionError:\n",
    "        # Handle connection errors during the download\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        # No connection errors, proceed to check the response\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            # Check if the HTTP response status code indicates a successful download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "            # Verify the integrity of the downloaded file using MD5 checksum\n",
    "            print(\"!!! Data download appears corrupted !!!\")\n",
    "        else:\n",
    "            # If download is successful and data is not corrupted, save the file\n",
    "            with open(fname, \"wb\") as fid:\n",
    "                fid.write(r.content) # Write the downloaded content to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c15d51-8e59-4305-a2bc-e270ac349085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go Envelope Tensor Shape: torch.Size([216, 296, 1])\n",
      "Plan Tensor Shape: torch.Size([216, 296, 15])\n",
      "Muscle Tensor Shape: torch.Size([216, 296, 2])\n"
     ]
    }
   ],
   "source": [
    "# Load the .mat file\n",
    "data = scipy.io.loadmat('condsForSimJ2moMuscles.mat')\n",
    "\n",
    "# Extract condsForSim struct\n",
    "conds_for_sim = data['condsForSim']\n",
    "\n",
    "# Initialize lists to store data for all conditions\n",
    "go_envelope_all = []\n",
    "plan_all = []\n",
    "muscle_all = []\n",
    "\n",
    "# Get the number of conditions (rows) and delay durations (columns)\n",
    "num_conditions, num_delays = conds_for_sim.shape\n",
    "\n",
    "# Loop through each condition and extract data\n",
    "for i in range(num_conditions):  # 27 conditions\n",
    "    go_envelope_condition = []\n",
    "    plan_condition = []\n",
    "    muscle_condition = []\n",
    "\n",
    "    for j in range(num_delays):  # 8 delay durations\n",
    "        condition = conds_for_sim[i, j]\n",
    "\n",
    "        go_envelope = condition['goEnvelope']\n",
    "        plan = condition['plan']\n",
    "        muscle = condition['muscle']\n",
    "\n",
    "        # Select only muscles 5 and 6 \n",
    "        selected_muscle_data = muscle[:, [4, 5]]  # which show the nicest multiphasic activity\n",
    "\n",
    "        go_envelope_condition.append(go_envelope)\n",
    "        plan_condition.append(plan)\n",
    "        muscle_condition.append(selected_muscle_data)\n",
    "\n",
    "    # Convert lists of NumPy arrays to single NumPy arrays before conversion to tensors\n",
    "    go_envelope_np = np.array(go_envelope_condition)\n",
    "    plan_np = np.array(plan_condition)\n",
    "    muscle_np = np.array(muscle_condition)\n",
    "\n",
    "    # Convert the single NumPy arrays to PyTorch tensors\n",
    "    go_envelope_all.append(torch.tensor(go_envelope_np, dtype=torch.float32))\n",
    "    plan_all.append(torch.tensor(plan_np, dtype=torch.float32))\n",
    "    muscle_all.append(torch.tensor(muscle_np, dtype=torch.float32))\n",
    "\n",
    "# Stack data for all conditions\n",
    "go_envelope_tensor = torch.stack(go_envelope_all)\n",
    "plan_tensor = torch.stack(plan_all)\n",
    "muscle_tensor = torch.stack(muscle_all)\n",
    "\n",
    "# Reshape to merge the first two dimensions\n",
    "go_envelope_tensor = go_envelope_tensor.reshape(-1, *go_envelope_tensor.shape[2:])\n",
    "plan_tensor = plan_tensor.reshape(-1, *plan_tensor.shape[2:])\n",
    "muscle_tensor = muscle_tensor.reshape(-1, *muscle_tensor.shape[2:])\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Go Envelope Tensor Shape: {go_envelope_tensor.shape}\")\n",
    "print(f\"Plan Tensor Shape: {plan_tensor.shape}\")\n",
    "print(f\"Muscle Tensor Shape: {muscle_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "417463ca-29c8-4229-baed-58dcf390747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization and Standardization\n",
    "def normalize_and_standardize(tensor):\n",
    "    # Normalize: scale to 0-1 range\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    tensor = (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "    # Standardize: shift to zero mean and unit variance\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    tensor = (tensor - mean) / std\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# Apply normalization and standardization to go_envelope_tensor and plan_tensor\n",
    "normalized_go_envelope_tensor = normalize_and_standardize(go_envelope_tensor)\n",
    "normalized_plan_tensor = normalize_and_standardize(plan_tensor)\n",
    "normalized_muscle_tensor = normalize_and_standardize(muscle_tensor)\n",
    "\n",
    "# Concatenate normalized tensors along the last dimension\n",
    "X_train = torch.cat((normalized_go_envelope_tensor, normalized_plan_tensor), dim=2)\n",
    "y_train = normalized_muscle_tensor\n",
    "\n",
    "batch_size = 54  # You can adjust this based on your data size and memory constraints\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2b6a95-49a0-4f10-8f63-3b85a7f7d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Rectified Tanh activation function\n",
    "def rectified_tanh(x):\n",
    "    return torch.where(x > 0, torch.tanh(x), 0)\n",
    "    \n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, g, h, tau=50):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = tau  # Time constant\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size)  # New linear layer\n",
    "\n",
    "        # Weight initialization\n",
    "        self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, input_size) * (h / torch.sqrt(torch.tensor(input_size, dtype=torch.float)))) \n",
    "        self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # Nonlinearity\n",
    "        self.nonlinearity = rectified_tanh    \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        hidden_prev = hidden.clone()\n",
    "        timestep = self.tau / 10  # Timestep for Euler integration\n",
    "        # Update hidden state\n",
    "        hidden_update = torch.matmul(self.J, hidden.transpose(0, 1))\n",
    "        input_update = torch.matmul(self.B, x.transpose(0, 1))\n",
    "        new_hidden = self.nonlinearity(hidden_update + input_update + self.bx.unsqueeze(1))\n",
    "        new_hidden = new_hidden.transpose(0, 1)\n",
    "\n",
    "        # Euler integration for continuous-time update\n",
    "        hidden = hidden + (timestep / self.tau) * (-hidden + new_hidden)\n",
    "\n",
    "        # Output calculation\n",
    "        output = self.output_linear(self.nonlinearity(hidden))\n",
    "\n",
    "        # Regularization terms\n",
    "        firing_rate_reg = hidden.pow(2).sum()\n",
    "        dynamic_reg = (hidden - hidden_prev).pow(2).sum().sqrt()\n",
    "\n",
    "        return output, hidden, firing_rate_reg, dynamic_reg\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state with batch dimension\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 16\n",
    "hidden_size = 300\n",
    "output_size = 2  # Number of muscles\n",
    "g = 1.5  # g value\n",
    "h = 1.0  # h value\n",
    "\n",
    "# Model instantiation\n",
    "model = SimpleRNN(input_size, hidden_size, output_size, g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e0da9-238e-4bc8-a2be-a4dbc562731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54, 300])\n",
      "Epoch 1, Loss: 60.293907165527344\n",
      "Epoch 2, Loss: 43.12694072723389\n",
      "Epoch 3, Loss: 34.68756818771362\n",
      "Epoch 4, Loss: 29.069705963134766\n",
      "Epoch 5, Loss: 24.985066890716553\n",
      "Epoch 6, Loss: 21.838332653045654\n",
      "Epoch 7, Loss: 19.357007026672363\n",
      "Epoch 8, Loss: 17.340847969055176\n",
      "Epoch 9, Loss: 15.66604733467102\n",
      "Epoch 10, Loss: 14.249650001525879\n",
      "Epoch 11, Loss: 13.042150735855103\n",
      "Epoch 12, Loss: 12.004640817642212\n",
      "Epoch 13, Loss: 11.101761817932129\n",
      "Epoch 14, Loss: 10.30933928489685\n",
      "Epoch 15, Loss: 9.610254287719727\n",
      "Epoch 16, Loss: 8.988394021987915\n",
      "Epoch 17, Loss: 8.42959713935852\n",
      "Epoch 18, Loss: 7.924638986587524\n",
      "Epoch 19, Loss: 7.466492056846619\n",
      "Epoch 20, Loss: 7.046846866607666\n",
      "Epoch 21, Loss: 6.6605366468429565\n",
      "Epoch 22, Loss: 6.304295182228088\n",
      "Epoch 23, Loss: 5.976369619369507\n",
      "Epoch 24, Loss: 5.674380779266357\n",
      "Epoch 25, Loss: 5.39538300037384\n",
      "Epoch 26, Loss: 5.136671423912048\n",
      "Epoch 27, Loss: 4.895117163658142\n",
      "Epoch 28, Loss: 4.66876757144928\n",
      "Epoch 29, Loss: 4.458338737487793\n",
      "Epoch 30, Loss: 4.261195063591003\n",
      "Epoch 31, Loss: 4.077601790428162\n",
      "Epoch 32, Loss: 3.908731698989868\n",
      "Epoch 33, Loss: 3.7540661096572876\n",
      "Epoch 34, Loss: 3.6102405190467834\n",
      "Epoch 35, Loss: 3.475272059440613\n",
      "Epoch 36, Loss: 3.3528387546539307\n",
      "Epoch 37, Loss: 3.239372193813324\n",
      "Epoch 38, Loss: 3.1349446177482605\n",
      "Epoch 39, Loss: 3.036838948726654\n",
      "Epoch 40, Loss: 2.9427024722099304\n",
      "Epoch 41, Loss: 2.8486981987953186\n",
      "Epoch 42, Loss: 2.7518250942230225\n",
      "Epoch 43, Loss: 2.6927719116210938\n",
      "Epoch 44, Loss: 2.673164129257202\n",
      "Epoch 45, Loss: 2.6238762736320496\n",
      "Epoch 46, Loss: 2.5540968775749207\n",
      "Epoch 47, Loss: 2.4892574846744537\n",
      "Epoch 48, Loss: 2.438850462436676\n",
      "Epoch 49, Loss: 2.3727177381515503\n",
      "Epoch 50, Loss: 2.3013639748096466\n",
      "Epoch 51, Loss: 2.237934112548828\n",
      "Epoch 52, Loss: 2.166595220565796\n",
      "Epoch 53, Loss: 2.116734594106674\n",
      "Epoch 54, Loss: 2.0834172070026398\n",
      "Epoch 55, Loss: 2.001721978187561\n",
      "Epoch 56, Loss: 1.9736903011798859\n",
      "Epoch 57, Loss: 1.9750177562236786\n"
     ]
    }
   ],
   "source": [
    "def compute_l2_regularization(parameters, alpha):\n",
    "    l2_reg = sum(p.pow(2.0).sum() for p in parameters)\n",
    "    return alpha * l2_reg\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # MSE Loss for regression tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
    "\n",
    "# Hyperparameters for regularization\n",
    "alpha = 1e-4  \n",
    "beta = 0.03\n",
    "gamma = 1e-4\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 500\n",
    "epoch_losses = []\n",
    "batch_size = 54\n",
    "\n",
    "# Initialize the tensor to accumulate hidden states\n",
    "accumulated_hidden_states = torch.zeros(batch_size, hidden_size)\n",
    "print(accumulated_hidden_states.shape)\n",
    "\n",
    "# Initialize a list to store hidden states for plotting\n",
    "hidden_states_for_plot = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        batch_size = inputs.size(0)\n",
    "        h = model.init_hidden(batch_size) # Initialize hidden state with the current batch size\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_firing_rate_reg = 0\n",
    "        total_dynamic_reg = 0\n",
    "\n",
    "        for t in range(inputs.shape[1]):  # Iterate over time steps\n",
    "            output, h, firing_rate_reg, dynamic_reg = model(inputs[:, t, :], h)\n",
    "            hidden_states_for_plot.append(h.detach().cpu().numpy())\n",
    "            total_firing_rate_reg += firing_rate_reg\n",
    "            total_dynamic_reg += dynamic_reg\n",
    "            \n",
    "        # Convert the list to a NumPy array\n",
    "        hidden_states_array = np.array(hidden_states_for_plot)\n",
    " \n",
    "        # Reshape to [num_batches, num_time_steps, num_hidden_units]\n",
    "        hidden_states_array = hidden_states_array.reshape(-1, inputs.shape[1], 300)\n",
    "        \n",
    "        # Average across batches\n",
    "        avg_hidden_states = np.mean(hidden_states_array, axis=0)\n",
    "\n",
    "        loss = criterion(output, targets[:, -1, :])\n",
    "        l2_reg = compute_l2_regularization(model.parameters(), alpha)\n",
    "        rfr_reg = beta * total_firing_rate_reg / inputs.shape[1]  # Average over time steps\n",
    "        rj_reg = gamma * total_dynamic_reg / inputs.shape[1]  # Average over time steps\n",
    "    \n",
    "        total_loss = loss + l2_reg + rfr_reg + rj_reg\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "    \n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# After training, convert the list of hidden states to a NumPy array for easier manipulation\n",
    "hidden_states_for_plot = np.array(hidden_states_for_plot)\n",
    "print(hidden_states_for_plot.shape)\n",
    "\n",
    "hidden_size = 300  # This should match the hidden size of your RNN\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de475624-9823-4fd4-a591-960795badf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Rectified Tanh activation function\n",
    "def rectified_tanh(x):\n",
    "    return torch.where(x > 0, torch.tanh(x), 0)\n",
    "    \n",
    "class ComplicatedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, g, h, tau=50):\n",
    "        super(ComplicatedRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = tau  # Time constant\n",
    "\n",
    "        # Modified weight initialization for a more chaotic regime (g >> 1)\n",
    "        self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, input_size) * (h / torch.sqrt(torch.tensor(input_size, dtype=torch.float)))) \n",
    "        self.w = nn.Parameter(torch.zeros(output_size, hidden_size))\n",
    "        self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.bz = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "        # Nonlinearity remains the same\n",
    "        self.nonlinearity = rectified_tanh\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        timestep = self.tau / 10  # Timestep for Euler integration\n",
    "        for _ in range(int(1 / timestep)):  # Adjust the range for simulation duration\n",
    "            hidden_update = torch.matmul(self.J, hidden.T)\n",
    "            input_update = torch.matmul(self.B, x.T)\n",
    "            new_hidden = self.nonlinearity(hidden_update + input_update + self.bx.unsqueeze(1))\n",
    "            new_hidden = new_hidden.T\n",
    "\n",
    "            # Euler integration for continuous-time update\n",
    "            hidden = hidden + (timestep / self.tau) * (-hidden + new_hidden)\n",
    "        \n",
    "        output = torch.matmul(self.w, hidden.T) + self.bz.unsqueeze(1)\n",
    "        output = output.T\n",
    "    \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "input_size = 7\n",
    "hidden_size = 300\n",
    "output_size = 2 # Number of muscles\n",
    "g = 4 # Significantly larger g value for complicated model\n",
    "h = 1.0 # h value remains the same\n",
    "\n",
    "complicated_model = ComplicatedRNN(input_size, hidden_size, output_size, g, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd9a5b-0e80-4464-aa8e-9ec54d225646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "num_epochs = 500  # The number of epochs\n",
    "epoch_losses = []  # To store average loss per epoch\n",
    "optimizer2 = optim.Adam(complicated_model.parameters(), lr=0.001, weight_decay=0)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer2.zero_grad()\n",
    "        batch_size = inputs.size(0)\n",
    "        h = complicated_model.init_hidden(batch_size)\n",
    "\n",
    "        total_loss = 0.0  # Accumulate loss over time steps\n",
    "\n",
    "        for t in range(inputs.shape[1]):\n",
    "            output, h = complicated_model(inputs[:, t, :], h)\n",
    "            loss = criterion(output, targets[:, t, :])  # Compute loss at each time step\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss.backward()  # Backpropagation on the accumulated loss\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer2.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad65166-cb7f-4d03-836d-c0a34c67f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hidden units of the Simple RNN\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(200):  # Iterate over five hidden units\n",
    "    plt.plot(avg_hidden_states[:, i], label=f'Unit {i+1}')\n",
    "\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Average Hidden State Activity')\n",
    "plt.title('Average Activity of Hidden Units Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d4256-8aa3-423e-9e9a-24d3cb675808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add gaussian noise to the input data.\n",
    "def perturb_inputs(inputs, noise_level=0.1):\n",
    "    # Generating noise with the same shape as inputs\n",
    "    noise = torch.randn_like(inputs) * noise_level\n",
    "    # Adding the generated noise to the inputs and returning the noisy inputs\n",
    "    return inputs + noise\n",
    "\n",
    "# Function to add noise to the weights of a neural network model\n",
    "def perturb_weights(model, perturbation_factor=0.1):\n",
    "    # Iterating over all parameters (weights and biases) of the model\n",
    "    for param in model.parameters():\n",
    "        # Checking if the parameter is trainable \n",
    "        if param.requires_grad:\n",
    "            # Adding in-place Gaussian noise to the parameter\n",
    "            param.data.add_(torch.randn_like(param) * perturbation_factor)\n",
    "\n",
    "# Function to evaluate the model's performance on a dataset\n",
    "def evaluate_model(model, data_loader, criterion, complicated=False):\n",
    "    # Setting the model to evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    # Disabling gradient calculation as it's not needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterating through the dataset using the data loader\n",
    "        for inputs, targets in data_loader:\n",
    "            # Determining the batch size from the input shape\n",
    "            batch_size = inputs.size(0)\n",
    "            # Initializing the hidden state of the model for the current batch\n",
    "            # Iterating over each time step in the input sequence\n",
    "            for t in range(inputs.shape[1]):\n",
    "                # Conditionally handling the model's forward pass based on the 'complicated' flag\n",
    "                if complicated:\n",
    "                    h = model.init_hidden(batch_size)\n",
    "                    # For a 'complicated' model, forward pass with the current timestep's input and hidden state.\n",
    "                    output, h = model(inputs[:, t, :], h)\n",
    "                else:\n",
    "                    h = model.init_hidden(batch_size)\n",
    "                    # For a simpler model, forward pass with additional unused outputs\n",
    "                    output, h, _, _ = model(inputs[:, t, :], h)\n",
    "                    # Calculating the loss between the model output and the target for the last timestep\n",
    "                    loss = criterion(output, targets[:, -1, :])\n",
    "                    # Accumulating the loss for the current batch\n",
    "                    total_loss += loss.item()\n",
    "                    # Returning the average loss across all batches in the data loader\n",
    "                return total_loss / len(data_loader)\n",
    "                \n",
    "def assess_robustness(model, data_loader, criterion, perturb_function, perturbation_level, complicated=False):\n",
    "    # Evaluate on non-perturbed data\n",
    "    original_loss = evaluate_model(model, data_loader, criterion, complicated)\n",
    "    # Perturb model\n",
    "    perturb_function(model, perturbation_level)\n",
    "    # Evaluate on perturbed data\n",
    "    perturbed_loss = evaluate_model(model, data_loader, criterion, complicated)\n",
    "    return original_loss, perturbed_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a58543-1e92-4ec5-a084-d56b53de7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and compare robustness\n",
    "simple_original_loss, simple_perturbed_loss = assess_robustness(model, train_loader, criterion, perturb_weights, 0.1)\n",
    "complex_original_loss, complex_perturbed_loss = assess_robustness(complicated_model, train_loader, criterion, perturb_weights, 0.1, complicated=True)\n",
    "\n",
    "print(f\"Simple Model - Original Loss: {simple_original_loss}, Perturbed Loss: {simple_perturbed_loss}\")\n",
    "print(f\"Complex Model - Original Loss: {complex_original_loss}, Perturbed Loss: {complex_perturbed_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63631a1a-4723-4674-a247-e1b1660cf747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
