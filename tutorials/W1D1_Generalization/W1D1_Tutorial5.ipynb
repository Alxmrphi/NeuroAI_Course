{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12bb9e08-6996-4d6b-8b80-a7a26191c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85fea2ec-e1ee-4ebe-9dff-d3b253c3ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for file and download URL\n",
    "fname = \"condsForSimJ2moMuscles.mat\"  # The name of the file to be downloaded\n",
    "url = \"https://osf.io/wak7e/download\" # URL from where the file will be downloaded\n",
    "expected_md5 = \"257d16c4d92759d615bf5cac75dd9a1f\" # MD5 hash for verifying file integrity\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        # Attempt to download the file\n",
    "        r = requests.get(url) # Make a GET request to the specified URL\n",
    "    except requests.ConnectionError:\n",
    "        # Handle connection errors during the download\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        # No connection errors, proceed to check the response\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            # Check if the HTTP response status code indicates a successful download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "            # Verify the integrity of the downloaded file using MD5 checksum\n",
    "            print(\"!!! Data download appears corrupted !!!\")\n",
    "        else:\n",
    "            # If download is successful and data is not corrupted, save the file\n",
    "            with open(fname, \"wb\") as fid:\n",
    "                fid.write(r.content) # Write the downloaded content to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c15d51-8e59-4305-a2bc-e270ac349085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before loading data: 316.7734375 MB\n",
      "Go Envelope Tensor Trimmed Shape: torch.Size([216, 226, 1])\n",
      "Plan Tensor Trimmed Shape: torch.Size([216, 226, 15])\n",
      "Muscle Tensor Trimmed Shape: torch.Size([216, 226, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before loading data, check memory usage\n",
    "import psutil\n",
    "print(f'Memory usage before loading data: {psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)} MB')\n",
    "\n",
    "# Load the .mat file\n",
    "data = scipy.io.loadmat('condsForSimJ2moMuscles.mat')\n",
    "\n",
    "# Extract condsForSim struct\n",
    "conds_for_sim = data['condsForSim']\n",
    "\n",
    "# Initialize lists to store data for all conditions\n",
    "go_envelope_all = []\n",
    "plan_all = []\n",
    "muscle_all = []\n",
    "\n",
    "# Get the number of conditions (rows) and delay durations (columns)\n",
    "num_conditions, num_delays = conds_for_sim.shape\n",
    "\n",
    "# Loop through each condition and extract data\n",
    "for i in range(num_conditions):  # 27 conditions\n",
    "    go_envelope_condition = []\n",
    "    plan_condition = []\n",
    "    muscle_condition = []\n",
    "\n",
    "    for j in range(num_delays):  # 8 delay durations\n",
    "        condition = conds_for_sim[i, j]\n",
    "\n",
    "        go_envelope = condition['goEnvelope']\n",
    "        plan = condition['plan']\n",
    "        muscle = condition['muscle']\n",
    "\n",
    "        # Select only muscles 5 and 6 \n",
    "        selected_muscle_data = muscle[:, [3, 4]]  # which show the nicest multiphasic activity\n",
    "\n",
    "        go_envelope_condition.append(go_envelope)\n",
    "        plan_condition.append(plan)\n",
    "        muscle_condition.append(selected_muscle_data)\n",
    "\n",
    "    # Convert lists of NumPy arrays to single NumPy arrays before conversion to tensors\n",
    "    go_envelope_np = np.array(go_envelope_condition)\n",
    "    plan_np = np.array(plan_condition)\n",
    "    muscle_np = np.array(muscle_condition)\n",
    "\n",
    "    # Convert the single NumPy arrays to PyTorch tensors\n",
    "    go_envelope_all.append(torch.tensor(go_envelope_np, dtype=torch.float32))\n",
    "    plan_all.append(torch.tensor(plan_np, dtype=torch.float32))\n",
    "    muscle_all.append(torch.tensor(muscle_np, dtype=torch.float32))\n",
    "\n",
    "# Stack data for all conditions\n",
    "go_envelope_tensor = torch.stack(go_envelope_all)\n",
    "plan_tensor = torch.stack(plan_all)\n",
    "muscle_tensor = torch.stack(muscle_all)\n",
    "\n",
    "# Reshape to merge the first two dimensions\n",
    "go_envelope_tensor = go_envelope_tensor.reshape(-1, *go_envelope_tensor.shape[2:])\n",
    "plan_tensor = plan_tensor.reshape(-1, *plan_tensor.shape[2:])\n",
    "muscle_tensor = muscle_tensor.reshape(-1, *muscle_tensor.shape[2:])\n",
    "\n",
    "# Remove the first 70 time steps\n",
    "go_envelope_tensor = go_envelope_tensor[:, 70:, :]\n",
    "plan_tensor = plan_tensor[:, 70:, :]\n",
    "muscle_tensor = muscle_tensor[:, 70:, :]\n",
    "\n",
    "# Let's print the new shapes to confirm the change\n",
    "print(f'Go Envelope Tensor Trimmed Shape: {go_envelope_tensor.shape}')\n",
    "print(f'Plan Tensor Trimmed Shape: {plan_tensor.shape}')\n",
    "print(f'Muscle Tensor Trimmed Shape: {muscle_tensor.shape}')\n",
    "\n",
    "# After tensors are created, we no longer need `data` and `conds_for_sim`\n",
    "del data\n",
    "del conds_for_sim\n",
    "# After tensors are reshaped and you no longer need the original ones\n",
    "del go_envelope_all\n",
    "del plan_all\n",
    "del muscle_all\n",
    "del go_envelope_np\n",
    "del plan_np\n",
    "del muscle_np\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "417463ca-29c8-4229-baed-58dcf390747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization and Standardization\n",
    "def normalize_and_standardize(tensor):\n",
    "\n",
    "    # Standardize: shift to zero mean and unit variance\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    tensor = (tensor - mean) / std\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# Apply normalization and standardization to go_envelope_tensor and plan_tensor\n",
    "normalized_go_envelope_tensor = normalize_and_standardize(go_envelope_tensor)\n",
    "normalized_plan_tensor = normalize_and_standardize(plan_tensor)\n",
    "normalized_muscle_tensor = muscle_tensor\n",
    "\n",
    "# Concatenate normalized tensors along the last dimension\n",
    "X_train = torch.cat((normalized_go_envelope_tensor, normalized_plan_tensor), dim=2)\n",
    "y_train = normalized_muscle_tensor\n",
    "\n",
    "batch_size = 256  # You can adjust this based on your data size and memory constraints\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b6a95-49a0-4f10-8f63-3b85a7f7d7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5519322156906128\n",
      "Epoch 2, Loss: 0.4986596703529358\n",
      "Epoch 3, Loss: 0.4689335823059082\n",
      "Epoch 4, Loss: 0.4492972493171692\n",
      "Epoch 5, Loss: 0.4316917061805725\n",
      "Epoch 6, Loss: 0.41479480266571045\n",
      "Epoch 7, Loss: 0.398833304643631\n",
      "Epoch 8, Loss: 0.3844679594039917\n",
      "Epoch 9, Loss: 0.3721160590648651\n",
      "Epoch 10, Loss: 0.36075031757354736\n",
      "Epoch 11, Loss: 0.3493914306163788\n",
      "Epoch 12, Loss: 0.33835771679878235\n",
      "Epoch 13, Loss: 0.32839053869247437\n",
      "Epoch 14, Loss: 0.31947383284568787\n",
      "Epoch 15, Loss: 0.3110644817352295\n",
      "Epoch 16, Loss: 0.3029980957508087\n",
      "Epoch 17, Loss: 0.29531052708625793\n",
      "Epoch 18, Loss: 0.287977933883667\n",
      "Epoch 19, Loss: 0.2811145782470703\n",
      "Epoch 20, Loss: 0.2747448980808258\n",
      "Epoch 21, Loss: 0.26856422424316406\n",
      "Epoch 22, Loss: 0.2624956965446472\n",
      "Epoch 23, Loss: 0.2566568851470947\n",
      "Epoch 24, Loss: 0.25110575556755066\n",
      "Epoch 25, Loss: 0.2458736151456833\n",
      "Epoch 26, Loss: 0.24088245630264282\n",
      "Epoch 27, Loss: 0.2360183298587799\n",
      "Epoch 28, Loss: 0.2313256710767746\n",
      "Epoch 29, Loss: 0.22685857117176056\n",
      "Epoch 30, Loss: 0.2225888967514038\n",
      "Epoch 31, Loss: 0.21851670742034912\n",
      "Epoch 32, Loss: 0.2145949900150299\n",
      "Epoch 33, Loss: 0.21078437566757202\n",
      "Epoch 34, Loss: 0.20711404085159302\n",
      "Epoch 35, Loss: 0.20359206199645996\n",
      "Epoch 36, Loss: 0.20020133256912231\n",
      "Epoch 37, Loss: 0.1969325840473175\n",
      "Epoch 38, Loss: 0.19376400113105774\n",
      "Epoch 39, Loss: 0.19071149826049805\n",
      "Epoch 40, Loss: 0.18777306377887726\n",
      "Epoch 41, Loss: 0.18495255708694458\n",
      "Epoch 42, Loss: 0.18222197890281677\n",
      "Epoch 43, Loss: 0.1795603632926941\n",
      "Epoch 44, Loss: 0.17696455121040344\n",
      "Epoch 45, Loss: 0.17446495592594147\n",
      "Epoch 46, Loss: 0.1720551699399948\n",
      "Epoch 47, Loss: 0.16972263157367706\n",
      "Epoch 48, Loss: 0.1674545854330063\n",
      "Epoch 49, Loss: 0.1652541160583496\n",
      "Epoch 50, Loss: 0.1631237417459488\n",
      "Epoch 51, Loss: 0.1610482931137085\n",
      "Epoch 52, Loss: 0.15902777016162872\n",
      "Epoch 53, Loss: 0.15706560015678406\n",
      "Epoch 54, Loss: 0.15515753626823425\n",
      "Epoch 55, Loss: 0.15330243110656738\n",
      "Epoch 56, Loss: 0.151495099067688\n",
      "Epoch 57, Loss: 0.1497366726398468\n",
      "Epoch 58, Loss: 0.14803031086921692\n",
      "Epoch 59, Loss: 0.14636950194835663\n",
      "Epoch 60, Loss: 0.14475306868553162\n",
      "Epoch 61, Loss: 0.14318086206912994\n",
      "Epoch 62, Loss: 0.1416507065296173\n",
      "Epoch 63, Loss: 0.14015871286392212\n",
      "Epoch 64, Loss: 0.13870401680469513\n",
      "Epoch 65, Loss: 0.13728773593902588\n",
      "Epoch 66, Loss: 0.1359080821275711\n",
      "Epoch 67, Loss: 0.13456253707408905\n",
      "Epoch 68, Loss: 0.1332491934299469\n",
      "Epoch 69, Loss: 0.13196663558483124\n",
      "Epoch 70, Loss: 0.13071230053901672\n",
      "Epoch 71, Loss: 0.12948517501354218\n",
      "Epoch 72, Loss: 0.1282859593629837\n",
      "Epoch 73, Loss: 0.12711575627326965\n",
      "Epoch 74, Loss: 0.12597249448299408\n",
      "Epoch 75, Loss: 0.12485451996326447\n",
      "Epoch 76, Loss: 0.12376077473163605\n",
      "Epoch 77, Loss: 0.12269100546836853\n",
      "Epoch 78, Loss: 0.12164456397294998\n",
      "Epoch 79, Loss: 0.12062013894319534\n",
      "Epoch 80, Loss: 0.11961705982685089\n",
      "Epoch 81, Loss: 0.118634894490242\n",
      "Epoch 82, Loss: 0.1176738515496254\n",
      "Epoch 83, Loss: 0.11673554033041\n",
      "Epoch 84, Loss: 0.11582382023334503\n",
      "Epoch 85, Loss: 0.11495845764875412\n",
      "Epoch 86, Loss: 0.1142244040966034\n",
      "Epoch 87, Loss: 0.11365008354187012\n",
      "Epoch 88, Loss: 0.1128997877240181\n",
      "Epoch 89, Loss: 0.11157360672950745\n",
      "Epoch 90, Loss: 0.11110398918390274\n",
      "Epoch 91, Loss: 0.11017798632383347\n",
      "Epoch 92, Loss: 0.10941486060619354\n",
      "Epoch 93, Loss: 0.10868389159440994\n",
      "Epoch 94, Loss: 0.1078869104385376\n",
      "Epoch 95, Loss: 0.10722575336694717\n",
      "Epoch 96, Loss: 0.10645269602537155\n",
      "Epoch 97, Loss: 0.1058158203959465\n",
      "Epoch 98, Loss: 0.10506423562765121\n",
      "Epoch 99, Loss: 0.1044677197933197\n",
      "Epoch 100, Loss: 0.10373218357563019\n",
      "Epoch 101, Loss: 0.1031772643327713\n",
      "Epoch 102, Loss: 0.1024479940533638\n",
      "Epoch 103, Loss: 0.10191866010427475\n",
      "Epoch 104, Loss: 0.10122103989124298\n",
      "Epoch 105, Loss: 0.10069246590137482\n",
      "Epoch 106, Loss: 0.1000451147556305\n",
      "Epoch 107, Loss: 0.09949776530265808\n",
      "Epoch 108, Loss: 0.09891969710588455\n",
      "Epoch 109, Loss: 0.09834467619657516\n",
      "Epoch 110, Loss: 0.09782713651657104\n",
      "Epoch 111, Loss: 0.0972403809428215\n",
      "Epoch 112, Loss: 0.09674455970525742\n",
      "Epoch 113, Loss: 0.0961964875459671\n",
      "Epoch 114, Loss: 0.09567713737487793\n",
      "Epoch 115, Loss: 0.09519167244434357\n",
      "Epoch 116, Loss: 0.09466236084699631\n",
      "Epoch 117, Loss: 0.09418143332004547\n",
      "Epoch 118, Loss: 0.09371201694011688\n",
      "Epoch 119, Loss: 0.09320862591266632\n",
      "Epoch 120, Loss: 0.0927414745092392\n",
      "Epoch 121, Loss: 0.09229478985071182\n",
      "Epoch 122, Loss: 0.09182220697402954\n",
      "Epoch 123, Loss: 0.09135950356721878\n",
      "Epoch 124, Loss: 0.09092652797698975\n",
      "Epoch 125, Loss: 0.09049703925848007\n",
      "Epoch 126, Loss: 0.09005860984325409\n",
      "Epoch 127, Loss: 0.08961967378854752\n",
      "Epoch 128, Loss: 0.08919590711593628\n",
      "Epoch 129, Loss: 0.08878778666257858\n",
      "Epoch 130, Loss: 0.08839017897844315\n",
      "Epoch 131, Loss: 0.08800011873245239\n",
      "Epoch 132, Loss: 0.08761876076459885\n",
      "Epoch 133, Loss: 0.08724765479564667\n",
      "Epoch 134, Loss: 0.0868886411190033\n",
      "Epoch 135, Loss: 0.08651196956634521\n",
      "Epoch 136, Loss: 0.08610555529594421\n",
      "Epoch 137, Loss: 0.08568298816680908\n",
      "Epoch 138, Loss: 0.08531112223863602\n",
      "Epoch 139, Loss: 0.0849844291806221\n",
      "Epoch 140, Loss: 0.08464300632476807\n",
      "Epoch 141, Loss: 0.0842752680182457\n",
      "Epoch 142, Loss: 0.08389715850353241\n",
      "Epoch 143, Loss: 0.08354433625936508\n",
      "Epoch 144, Loss: 0.08321848511695862\n",
      "Epoch 145, Loss: 0.08290661871433258\n",
      "Epoch 146, Loss: 0.08260558545589447\n",
      "Epoch 147, Loss: 0.08230576664209366\n",
      "Epoch 148, Loss: 0.08200815320014954\n",
      "Epoch 149, Loss: 0.08167603611946106\n",
      "Epoch 150, Loss: 0.08131074905395508\n",
      "Epoch 151, Loss: 0.08095855265855789\n",
      "Epoch 152, Loss: 0.08067390322685242\n",
      "Epoch 153, Loss: 0.08040428161621094\n",
      "Epoch 154, Loss: 0.08009021729230881\n",
      "Epoch 155, Loss: 0.07976528257131577\n",
      "Epoch 156, Loss: 0.07945775985717773\n",
      "Epoch 157, Loss: 0.07918529957532883\n",
      "Epoch 158, Loss: 0.0789315477013588\n",
      "Epoch 159, Loss: 0.07867088168859482\n",
      "Epoch 160, Loss: 0.07842285931110382\n",
      "Epoch 161, Loss: 0.07815377414226532\n",
      "Epoch 162, Loss: 0.07786113023757935\n"
     ]
    }
   ],
   "source": [
    "# Define a custom Rectified Tanh activation function\n",
    "def rectified_tanh(x):\n",
    "    return torch.where(x > 0, torch.tanh(x), 0) # was torch.where(x > 0, x, torch.tanh(x))\n",
    "def grad_rectified_tanh(x):\n",
    "    return torch.where(x > 0, 1 - torch.tanh(x)**2, 0)\n",
    "def grad_tanh(x):\n",
    "    return 1 - torch.tanh(x)**2\n",
    "    \n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, g, h, tau=50):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = tau  # Time constant\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size)  # New linear layer\n",
    "\n",
    "        # Weight initialization\n",
    "        self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, input_size) * (h / torch.sqrt(torch.tensor(input_size, dtype=torch.float)))) \n",
    "        self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # Nonlinearity\n",
    "        self.nonlinearity = rectified_tanh \n",
    "        \n",
    "        self.hidden_activations = []    \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        hidden_prev = hidden.clone()\n",
    "        timestep = self.tau / 10  # Timestep for Euler integration    \n",
    "        # Update hidden state\n",
    "        firing_rate = self.nonlinearity(hidden)        \n",
    "        hidden_update = torch.matmul(self.J, firing_rate.transpose(0, 1)) \n",
    "        input_update = torch.matmul(self.B, x.transpose(0, 1))        \n",
    "        new_hidden = hidden_update + input_update + self.bx.unsqueeze(1)\n",
    "        new_hidden = new_hidden.transpose(0, 1)    \n",
    "        # Euler integration for continuous-time update\n",
    "        hidden = hidden + (timestep / self.tau) * (-hidden_prev + new_hidden)    \n",
    "        # Output calculation\n",
    "        output = self.output_linear(firing_rate)    \n",
    "        # Regularization terms\n",
    "        firing_rate_reg = hidden.pow(2).sum()\n",
    "        dynamic_reg = torch.linalg.norm(torch.matmul(self.J, grad_rectified_tanh(hidden.transpose(0, 1))), ord='fro', dim=(-2, -1)).sum()\n",
    "        \n",
    "        self.hidden_activations.append(hidden.clone())\n",
    "        \n",
    "        return output, hidden, firing_rate_reg, dynamic_reg\n",
    "\n",
    "    def reset_activations(self):\n",
    "        self.hidden_activations = []\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state with batch dimension\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "def compute_l2_regularization(parameters, alpha):\n",
    "    l2_reg = sum(p.pow(2.0).sum() for p in parameters)\n",
    "    return alpha * l2_reg\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 16\n",
    "hidden_size = 150\n",
    "output_size = 2  # Number of muscles\n",
    "g = 1.5  # g value\n",
    "h_val = 1.0  # h value\n",
    "\n",
    "# Hyperparameters for regularization\n",
    "alpha = 1e-4  \n",
    "beta = 0.03\n",
    "gamma = 1e-4\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 500\n",
    "epoch_losses = [] \n",
    "\n",
    "# get available device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Model instantiation\n",
    "model = SimpleRNN(input_size, hidden_size, output_size, g, h_val)\n",
    "model.to(device)\n",
    "hidden_states_for_plot = []\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # MSE Loss for regression tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0) ### WD=0. Note: Paper uses Hessian-Free optimizer\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        h = model.init_hidden(batch_size).to(device) # Initialize hidden state with the current batch size\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_firing_rate_reg = 0\n",
    "        total_dynamic_reg = 0\n",
    "\n",
    "        for t in range(inputs.shape[1]):  # Iterate over time steps\n",
    "            output, h, firing_rate_reg, dynamic_reg = model(inputs[:, t, :], h)\n",
    "            hidden_states_for_plot.append(h.detach().cpu().numpy())\n",
    "            total_firing_rate_reg += firing_rate_reg\n",
    "            total_dynamic_reg += dynamic_reg\n",
    "            # Clear memory of intermediate tensors\n",
    "            del firing_rate_reg, dynamic_reg\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # Compute loss using the last output and include regularization terms\n",
    "        loss = criterion(output, targets[:, -1, :])\n",
    "        l2_reg = compute_l2_regularization(model.parameters(), alpha)\n",
    "        rfr_reg = beta * total_firing_rate_reg / inputs.shape[1] / hidden_size /num_conditions  # Average over time steps\n",
    "        rj_reg = gamma * total_firing_rate_reg / inputs.shape[1] /num_conditions # Average over time steps\n",
    "    \n",
    "        total_loss = loss + l2_reg + rfr_reg + rj_reg\n",
    "\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "    \n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "        # Clear memory at the end of each batch\n",
    "        del inputs, targets, h\n",
    "\n",
    "    avg_loss = running_loss / float(len(train_loader))\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss}')\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Clear cache after training\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# Monitor memory usage after training\n",
    "print(f'Memory usage after training: {psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)} MB')  \n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd9a5b-0e80-4464-aa8e-9ec54d225646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Custom activation function (remains unchanged)\n",
    "def rectified_tanh(x):\n",
    "    return torch.where(x > 0, torch.tanh(x), 0)\n",
    "\n",
    "# ComplicatedRNN class\n",
    "class ComplicatedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, g, h, tau=50):\n",
    "        super(ComplicatedRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = tau\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Weight initialization (unchanged)\n",
    "        self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, input_size) * (h / torch.sqrt(torch.tensor(input_size, dtype=torch.float)))) \n",
    "        self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # Nonlinearity (unchanged)\n",
    "        self.nonlinearity = rectified_tanh \n",
    "        \n",
    "        self.hidden_activations = []    \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Forward pass logic (same as SimpleRNN but without regularization terms)\n",
    "        hidden_prev = hidden.clone()\n",
    "        timestep = self.tau / 10\n",
    "\n",
    "        firing_rate = self.nonlinearity(hidden)\n",
    "        \n",
    "        hidden_update = torch.matmul(self.J, firing_rate.transpose(0, 1))\n",
    "        input_update = torch.matmul(self.B, x.transpose(0, 1))\n",
    "        \n",
    "        new_hidden = hidden_update + input_update + self.bx.unsqueeze(1)\n",
    "        new_hidden = new_hidden.transpose(0, 1)\n",
    "    \n",
    "        hidden = hidden + (timestep / self.tau) * (-hidden_prev + new_hidden)\n",
    "        output = self.output_linear(firing_rate)\n",
    "        \n",
    "        self.hidden_activations.append(hidden.clone())\n",
    "\n",
    "        return output, hidden\n",
    "        \n",
    "    def reset_activations(self):\n",
    "        self.hidden_activations = []\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "# Training loop\n",
    "# Hyperparameters \n",
    "input_size = 16\n",
    "hidden_size = 150\n",
    "output_size = 2\n",
    "g = 4\n",
    "h_val = 1.0\n",
    "num_epochs = 500\n",
    "epoch_losses = []\n",
    "\n",
    "# Device setup and model instantiation \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "complicated_model = ComplicatedRNN(input_size, hidden_size, output_size, g, h_val)\n",
    "complicated_model.to(device)\n",
    "hidden_states_for_plot_cm = []\n",
    "\n",
    "# Loss function and optimizer (no weight decay)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(complicated_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (updated to remove regularization terms)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        h = complicated_model.init_hidden(batch_size).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for t in range(inputs.shape[1]):\n",
    "            output, h = complicated_model(inputs[:, t, :], h)\n",
    "            hidden_states_for_plot_cm.append(h.detach().cpu().numpy())\n",
    "\n",
    "        loss = criterion(output, targets[:, -1, :])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(complicated_model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "    \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd598c-a42e-4219-aa9b-5b7e54710749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_psth(data, title, bin_size=10):\n",
    "    \"\"\"\n",
    "    Plot Peri-Stimulus Time Histogram (PSTH) for given data.\n",
    "    :param data: a tensor containing the neural data\n",
    "    :param title: a string for the plot title\n",
    "    :param bin_size: size of time bins for averaging\n",
    "    \"\"\"\n",
    "    # Averaging neural activity across trials for each time bin\n",
    "    mean_data = data.mean(dim=0)  # Mean across trials\n",
    "    n_bins = mean_data.shape[0] // bin_size\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    binned_data = mean_data.unfold(0, bin_size, bin_size).mean(dim=2)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(data.shape[2]):  # Iterate over each channel/neuron\n",
    "        plt.plot(binned_data[:, i], label=f'Channel {i+1}')\n",
    "    plt.xlabel('Time (bins)')\n",
    "    plt.ylabel('Average Neural Activity')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "plot_psth(normalized_muscle_tensor, \"PSTH for Arm Movement\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5362de-9acf-4380-af3b-68acded8eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert list of numpy arrays to a single numpy array\n",
    "hidden_states = np.array(hidden_states_for_plot)\n",
    "\n",
    "# Calculate the mean across all batches for each time step\n",
    "# Assuming hidden_states.shape is [num_samples, num_time_steps, hidden_size]\n",
    "mean_activations = np.mean(hidden_states, axis=0)\n",
    "\n",
    "# Plot the PSTHs for the first few neurons\n",
    "neurons_to_plot = 5  # Adjust this number as needed\n",
    "time_steps = mean_activations.shape[0]\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(min(neurons_to_plot, hidden_states.shape[2])):\n",
    "    plt.plot(range(time_steps), mean_activations[:, i], label=f'Neuron {i+1}')\n",
    "\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Average Activation')\n",
    "plt.title('PSTHs of Hidden Units in SimpleRNN')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b011e-0fa3-4e5f-a423-0074f069f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert list of numpy arrays to a single numpy array\n",
    "hidden_states_cm = np.array(hidden_states_for_plot_cm)\n",
    "\n",
    "# Calculate the mean across all batches for each time step\n",
    "# Assuming hidden_states.shape is [num_samples, num_time_steps, hidden_size]\n",
    "mean_activations_cm = np.mean(hidden_states_cm, axis=0)\n",
    "\n",
    "# Plot the PSTHs for the first few neurons\n",
    "neurons_to_plot = 5  # Adjust this number as needed\n",
    "time_steps_cm = mean_activations_cm.shape[0]\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(min(neurons_to_plot, hidden_states_cm.shape[2])):\n",
    "    plt.plot(range(time_steps), mean_activations_cm[:, i], label=f'Neuron {i+1}')\n",
    "\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Average Activation')\n",
    "plt.title('PSTHs of Hidden Units in ComplicatedRNN')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d4256-8aa3-423e-9e9a-24d3cb675808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def perturb_inputs(model, inputs, perturbation_strength):\n",
    "    # Perturb the inputs by adding random noise scaled by the perturbation strength\n",
    "    noise = torch.randn_like(inputs) * perturbation_strength\n",
    "    perturbed_inputs = inputs + noise\n",
    "    return perturbed_inputs\n",
    "\n",
    "def test_perturbed_inputs(model, perturbation_strengths, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    perturbation_errors = []\n",
    "\n",
    "    for strength in perturbation_strengths:\n",
    "        batch_errors = []  # Store errors for each batch in the test_loader\n",
    "        \n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            h = model.init_hidden(batch_size).to(device)  # Initialize hidden state\n",
    "            \n",
    "            perturbed_inputs = perturb_inputs(model, inputs, strength)  # Perturb inputs here\n",
    "\n",
    "            for t in range(inputs.shape[1]):  # Iterate over time steps\n",
    "                model_output = model(perturbed_inputs[:, t, :], h)\n",
    "                if len(model_output) == 4:  # If the model returns 4 outputs, unpack all (simple model)\n",
    "                    output, h, _, _ = model_output\n",
    "                else:  # If the model returns 2 outputs, unpack accordingly (complicated model)\n",
    "                    output, h = model_output\n",
    "            \n",
    "            # Compute loss for the entire sequence (last output)\n",
    "            loss = criterion(output, targets[:, -1, :]).item()\n",
    "            batch_errors.append(loss)  # Append the loss of this batch\n",
    "        \n",
    "        mean_error = np.mean(batch_errors)  # Calculate mean error for this perturbation strength\n",
    "        perturbation_errors.append(mean_error)\n",
    "        print(f'Perturbation strength: {strength}, Mean Error: {mean_error}')\n",
    "    \n",
    "    return perturbation_errors\n",
    "\n",
    "perturbation_strengths = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "simple_model_errors = test_perturbed_inputs(model, perturbation_strengths, train_loader, criterion, device)\n",
    "complex_model_errors = test_perturbed_inputs(complicated_model, perturbation_strengths, train_loader, criterion, device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert errors to percentages \n",
    "simple_model_errors_percent = [error * 100 for error in simple_model_errors]\n",
    "complex_model_errors_percent = [error * 100 for error in complex_model_errors]\n",
    "\n",
    "# Adjust the width of the bars here\n",
    "bar_width = 0.1  # Smaller values make narrower bars\n",
    "\n",
    "import numpy as np  # Import numpy for array manipulation\n",
    "\n",
    "# Define your perturbation strengths\n",
    "perturbation_strengths = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "bar_width = 0.2  # Adjust the bar width as needed\n",
    "\n",
    "# Create an array of indices for the x-axis ticks\n",
    "x_indices = np.arange(len(perturbation_strengths))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Subplot for the simple model\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x_indices, simple_model_errors_percent, width=bar_width, color='blue', label='Simple RNN')\n",
    "plt.xlabel('Perturbation Strength')\n",
    "plt.ylabel('Mean Error (%)')\n",
    "plt.title('Simple Model Robustness')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot for the complex model\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(x_indices, complex_model_errors_percent, width=bar_width, color='red', label='Complicated RNN')\n",
    "plt.xlabel('Perturbation Strength')\n",
    "plt.ylabel('Mean Error (%)')\n",
    "plt.title('Complex Model Robustness')\n",
    "plt.legend()\n",
    "\n",
    "# Set custom tick positions and labels on the x-axis\n",
    "plt.xticks(x_indices, perturbation_strengths)\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae5f60-f9a1-4ead-adcd-42493d32ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "import torch.backends.mps\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming train_loader is defined elsewhere and is a DataLoader instance\n",
    "# Assuming the models are instances of SimpleRNN or similar\n",
    "\n",
    "def perturb_recurrent_weights(model, perturbation_strength):\n",
    "    # Perturb the recurrent weight matrix J by adding Gaussian noise\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn_like(model.J) * perturbation_strength\n",
    "        perturbed_weights = model.J + noise\n",
    "        return perturbed_weights\n",
    "\n",
    "def test_perturbed_structure(model, perturbation_strengths, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    perturbation_errors = []\n",
    "    original_weights = model.J.data.clone()  # Save the original weights\n",
    "\n",
    "    for strength in perturbation_strengths:\n",
    "        batch_errors = []  # Store errors for each batch in the test_loader\n",
    "        # Perturb the recurrent weights of the model\n",
    "        perturbed_weights = perturb_recurrent_weights(model, strength)\n",
    "        model.J.data = perturbed_weights.data\n",
    "\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            h = model.init_hidden(batch_size).to(device)  # Initialize hidden state\n",
    "            \n",
    "            for t in range(inputs.shape[1]):  # Iterate over time steps\n",
    "                model_output = model(inputs[:, t, :], h)\n",
    "                if len(model_output) == 4:  # If the model returns 4 outputs, unpack all (simple model)\n",
    "                    output, h, _, _ = model_output\n",
    "                else:  # If the model returns 2 outputs, unpack accordingly (complicated model)\n",
    "                    output, h = model_output\n",
    "            \n",
    "            # Compute loss for the entire sequence (last output)\n",
    "            loss = criterion(output, targets[:, -1, :]).item()\n",
    "            batch_errors.append(loss)  # Append the loss of this batch\n",
    "        \n",
    "        # Restore the original weights before the next iteration\n",
    "        model.J.data = original_weights.data\n",
    "        \n",
    "        mean_error = np.mean(batch_errors)  # Calculate mean error for this perturbation strength\n",
    "        perturbation_errors.append(mean_error)\n",
    "        print(f'Perturbation strength: {strength}, Mean Error: {mean_error}')\n",
    "    \n",
    "    return perturbation_errors\n",
    "\n",
    "# Define your perturbation strengths\n",
    "perturbation_strengths = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "# Get errors for simple and complex models\n",
    "simple_model_structural_errors = test_perturbed_structure(model, perturbation_strengths, train_loader, criterion, device)\n",
    "complex_model_structural_errors = test_perturbed_structure(complicated_model, perturbation_strengths, train_loader, criterion, device)\n",
    "\n",
    "# Convert errors to percentages \n",
    "simple_model_errors_percent = [error * 100 for error in simple_model_structural_errors]\n",
    "complex_model_errors_percent = [error * 100 for error in complex_model_structural_errors]\n",
    "\n",
    "# Define your perturbation strengths\n",
    "perturbation_strengths = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "# Create an array of indices for the x-axis ticks\n",
    "x_indices = np.arange(len(perturbation_strengths))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# For the simple model\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x_indices, simple_model_errors_percent, color='blue', label='Simple RNN')\n",
    "plt.xlabel('Perturbation Strength (log scale)')\n",
    "plt.ylabel('Mean Error (%)')\n",
    "plt.title('Simple Model Robustness')\n",
    "plt.legend()\n",
    "\n",
    "# For the complex model\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(x_indices, complex_model_errors_percent, color='red', label='Complicated RNN')\n",
    "plt.xlabel('Perturbation Strength (log scale)')\n",
    "plt.ylabel('Mean Error (%)')\n",
    "plt.title('Complex Model Robustness')\n",
    "plt.legend()\n",
    "\n",
    "# Set custom tick positions and labels on the x-axis\n",
    "plt.xticks(x_indices, perturbation_strengths)\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257161c9-9155-4dfd-b089-b12ac2725dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
