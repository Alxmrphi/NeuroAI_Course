{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb9e08-6996-4d6b-8b80-a7a26191c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fea2ec-e1ee-4ebe-9dff-d3b253c3ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for file and download URL\n",
    "fname = \"condsForSimJ2moMuscles.mat\"  # The name of the file to be downloaded\n",
    "url = \"https://osf.io/wak7e/download\" # URL from where the file will be downloaded\n",
    "expected_md5 = \"257d16c4d92759d615bf5cac75dd9a1f\" # MD5 hash for verifying file integrity\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        # Attempt to download the file\n",
    "        r = requests.get(url) # Make a GET request to the specified URL\n",
    "    except requests.ConnectionError:\n",
    "        # Handle connection errors during the download\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        # No connection errors, proceed to check the response\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            # Check if the HTTP response status code indicates a successful download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "            # Verify the integrity of the downloaded file using MD5 checksum\n",
    "            print(\"!!! Data download appears corrupted !!!\")\n",
    "        else:\n",
    "            # If download is successful and data is not corrupted, save the file\n",
    "            with open(fname, \"wb\") as fid:\n",
    "                fid.write(r.content) # Write the downloaded content to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c15d51-8e59-4305-a2bc-e270ac349085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy.io\n",
    "\n",
    "# Load the .mat file\n",
    "data = scipy.io.loadmat('condsForSimJ2moMuscles.mat')\n",
    "\n",
    "# Extract condsForSim struct\n",
    "conds_for_sim = data['condsForSim']\n",
    "\n",
    "# Initialize lists to store data for all conditions\n",
    "go_envelope_all = []\n",
    "plan_all = []\n",
    "muscle_all = []\n",
    "\n",
    "# Get the number of conditions (rows) and delay durations (columns)\n",
    "num_conditions, num_delays = conds_for_sim.shape\n",
    "\n",
    "# Loop through each condition and extract data\n",
    "for i in range(num_conditions):  # 27 conditions\n",
    "    go_envelope_condition = []\n",
    "    plan_condition = []\n",
    "    muscle_condition = []\n",
    "\n",
    "    for j in range(num_delays):  # 8 delay durations\n",
    "        condition = conds_for_sim[i, j]\n",
    "\n",
    "        go_envelope = condition['goEnvelope']\n",
    "        plan = condition['plan']\n",
    "        muscle = condition['muscle']\n",
    "\n",
    "        # Select only muscles 5 and 6 \n",
    "        selected_muscle_data = muscle[:, [3, 4]]  # which show the nicest multiphasic activity\n",
    "\n",
    "        go_envelope_condition.append(go_envelope)\n",
    "        plan_condition.append(plan)\n",
    "        muscle_condition.append(selected_muscle_data)\n",
    "\n",
    "    # Convert lists of NumPy arrays to single NumPy arrays before conversion to tensors\n",
    "    go_envelope_np = np.array(go_envelope_condition)\n",
    "    plan_np = np.array(plan_condition)\n",
    "    muscle_np = np.array(muscle_condition)\n",
    "\n",
    "    # Convert the single NumPy arrays to PyTorch tensors\n",
    "    go_envelope_all.append(torch.tensor(go_envelope_np, dtype=torch.float32))\n",
    "    plan_all.append(torch.tensor(plan_np, dtype=torch.float32))\n",
    "    muscle_all.append(torch.tensor(muscle_np, dtype=torch.float32))\n",
    "\n",
    "# Stack data for all conditions\n",
    "go_envelope_tensor = torch.stack(go_envelope_all)\n",
    "plan_tensor = torch.stack(plan_all)\n",
    "muscle_tensor = torch.stack(muscle_all)\n",
    "\n",
    "# Print the shapes to confirm\n",
    "print(f'Go Envelope Tensor: {go_envelope_tensor.shape}')\n",
    "print(f'Plan Tensor: {plan_tensor.shape}')\n",
    "print(f'Muscle Tensor: {muscle_tensor.shape}')\n",
    "\n",
    "# Clean up\n",
    "del data, conds_for_sim, go_envelope_all, plan_all, muscle_all\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd6d286-fc8b-4b1c-babe-c58279781637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization and Standardization Function\n",
    "def normalize_and_standardize(tensor):\n",
    "    # Normalize\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Standardize\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    standardized_normalized_tensor = (tensor - mean) / std\n",
    "    \n",
    "    return standardized_normalized_tensor\n",
    "\n",
    "plan_tensor = normalize_and_standardize(plan_tensor)\n",
    "\n",
    "# Concatenate Plan and Go Envelope Tensors\n",
    "concatenated_tensor = torch.cat([plan_tensor, go_envelope_tensor], dim=3)  # Resulting shape: [27, 8, 296, 16]\n",
    "\n",
    "# Normalize and Standardize\n",
    "normalised_inputs = concatenated_tensor\n",
    "\n",
    "\n",
    "# Averaging across conditions and delays\n",
    "avg_go_envelope = normalised_inputs.mean(dim=[0, 1]).squeeze()\n",
    "avg_plan = plan_tensor.mean(dim=[0, 1])\n",
    "avg_muscle = muscle_tensor.mean(dim=[0, 1])\n",
    "\n",
    "# For plan and muscle\n",
    "# individual features for plotting (for example, the first feature)\n",
    "feature_idx = 7 #max14\n",
    "muscle_idx = 1 #max1\n",
    "\n",
    "plan_feature = avg_plan[:, feature_idx]\n",
    "muscle_feature = avg_muscle[:, muscle_idx]\n",
    "# Time steps\n",
    "timesteps = np.arange(296)\n",
    "\n",
    "# Plotting Go Envelope\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(timesteps, avg_go_envelope, label='Go Envelope')\n",
    "plt.title('Go Envelope over Time')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Average Value')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Plan Feature\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(timesteps, plan_feature, label=f'Plan Feature {feature_idx}')\n",
    "plt.title(f'Plan Feature {feature_idx} over Time')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Average Value')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Muscle Feature\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(timesteps, muscle_feature, label=f'Muscle n°{muscle_idx}')\n",
    "plt.title(f'Muscle n°{muscle_idx} over Time')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Average Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417463ca-29c8-4229-baed-58dcf390747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplicatedTimeseriesDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, delay_idx):\n",
    "        \"\"\"\n",
    "        inputs: Tensor of shape [27, 8, 296, input_features]\n",
    "        targets: Tensor of shape [27, 8, 296, output_features]\n",
    "        delay_idx: Fixed index of the delay to be used\n",
    "        \"\"\"\n",
    "        self.inputs = inputs[:, delay_idx]\n",
    "        self.targets = targets[:, delay_idx]\n",
    "        self.num_conditions = inputs.shape[0]\n",
    "\n",
    "        print(\"Shape of inputs - ComplicatedRNN\", self.inputs.shape)\n",
    "        print(\"Shape of targets - ComplicatedRNN\", self.targets.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_conditions\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.inputs[idx]\n",
    "        target_seq = self.targets[idx]\n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Choose the delay index\n",
    "fixed_delay_idx = 3\n",
    "\n",
    "# Concatenate Plan and Go Envelope Tensors\n",
    "concatenated_tensor = torch.cat([plan_tensor, go_envelope_tensor], dim=3)  # Resulting shape: [27, 8, 296, 16]\n",
    "\n",
    "# Normalize and Standardize\n",
    "normalized_muscle_tensor = muscle_tensor\n",
    "\n",
    "# Create the dataset with the fixed delay\n",
    "complicated_dataset = ComplicatedTimeseriesDataset(normalised_inputs, normalized_muscle_tensor, fixed_delay_idx)\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.7 * len(complicated_dataset))\n",
    "val_size = int(0.15 * len(complicated_dataset))\n",
    "test_size = len(complicated_dataset) - train_size - val_size\n",
    "\n",
    "complicated_train_dataset, complicated_val_dataset, complicated_test_dataset = random_split(complicated_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 1\n",
    "complicated_train_loader = DataLoader(complicated_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "complicated_val_loader = DataLoader(complicated_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "complicated_test_loader = DataLoader(complicated_test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec07a1-d43a-4ba4-ac21-3bb34a35490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the first two dimensions (conditions and delays)\n",
    "flattened_inputs = concatenated_tensor.view(-1, *concatenated_tensor.shape[2:])\n",
    "flattened_targets = muscle_tensor.view(-1, *muscle_tensor.shape[2:])\n",
    "\n",
    "class SimpleTimeseriesDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        \n",
    "        print(\"Shape of inputs - SimpleRNN\", self.inputs.shape)\n",
    "        print(\"Shape of targets - SimpleRNN\", self.targets.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.inputs[idx]\n",
    "        target_seq = self.targets[idx]\n",
    "        return input_seq, target_seq\n",
    "        \n",
    "# Normalize and Standardize the flattened tensors\n",
    "normalised_flattened_inputs = flattened_inputs\n",
    "normalized_flattened_targets = flattened_targets\n",
    "\n",
    "# Create the SimpleRNN dataset\n",
    "simple_dataset = SimpleTimeseriesDataset(normalised_flattened_inputs, normalized_flattened_targets)\n",
    "\n",
    "# Split the dataset\n",
    "simple_train_size = int(0.7 * len(simple_dataset))\n",
    "simple_val_size = int(0.15 * len(simple_dataset))\n",
    "simple_test_size = len(simple_dataset) - simple_train_size - simple_val_size\n",
    "\n",
    "simple_train_dataset, simple_val_dataset, simple_test_dataset = random_split(simple_dataset, [simple_train_size, simple_val_size, simple_test_size])\n",
    "\n",
    "batch_size = 31\n",
    "\n",
    "# Create DataLoaders\n",
    "simple_train_loader = DataLoader(simple_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "simple_val_loader = DataLoader(simple_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "simple_test_loader = DataLoader(simple_test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c841c9a-27b3-414d-b406-56547c285e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            h = model.init_hidden(batch_size).to(device)\n",
    "\n",
    "            for t in range(inputs.shape[1]):\n",
    "                # Capture any additional outputs in 'rest' \n",
    "                output, h, *rest = model(inputs[:, t, :], h)\n",
    "            \n",
    "            loss = criterion(output, targets[:, -1, :])\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_test_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            h = model.init_hidden(batch_size).to(device)\n",
    "\n",
    "            for t in range(inputs.shape[1]):\n",
    "                # Capture any additional outputs in 'rest' \n",
    "                output, h, *rest = model(inputs[:, t, :], h)\n",
    "            \n",
    "            loss = criterion(output, targets[:, -1, :])\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    return avg_test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b6a95-49a0-4f10-8f63-3b85a7f7d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.autograd import profiler\n",
    "\n",
    "# Define a custom Rectified Tanh activation function\n",
    "def rectified_tanh(x):\n",
    "    return torch.where(x > 0, torch.tanh(x), 0) # was torch.where(x > 0, x, torch.tanh(x))\n",
    "def grad_rectified_tanh(x):\n",
    "    return torch.where(x > 0, 1 - torch.tanh(x)**2, 0)\n",
    "def grad_tanh(x):\n",
    "    return 1 - torch.tanh(x)**2\n",
    "    \n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, g, h, tau=50):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = tau  # Time constant\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size)  \n",
    "\n",
    "        # Weight initialization\n",
    "        self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, input_size) * (h / torch.sqrt(torch.tensor(input_size, dtype=torch.float)))) \n",
    "        self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # Nonlinearity\n",
    "        self.nonlinearity = rectified_tanh \n",
    "         \n",
    "    def forward(self, x, hidden):\n",
    "        hidden_prev = hidden.clone()\n",
    "        timestep = self.tau / 10  # Timestep for Euler integration    \n",
    "        # Update hidden state\n",
    "        firing_rate = self.nonlinearity(hidden)        \n",
    "        hidden_update = torch.matmul(self.J, firing_rate.transpose(0, 1)) \n",
    "        input_update = torch.matmul(self.B, x.transpose(0, 1))        \n",
    "        new_hidden = hidden_update + input_update + self.bx.unsqueeze(1)\n",
    "        new_hidden = new_hidden.transpose(0, 1)    \n",
    "        # Euler integration for continuous-time update\n",
    "        hidden = hidden + (timestep / self.tau) * (-hidden_prev + new_hidden)    \n",
    "        # Output calculation\n",
    "        output = self.output_linear(firing_rate)    \n",
    "        # Regularization terms\n",
    "        firing_rate_reg = hidden.pow(2).sum()\n",
    "        dynamic_reg = torch.linalg.norm(torch.matmul(self.J, grad_rectified_tanh(hidden.transpose(0, 1))), ord='fro', dim=(-2, -1)).sum()\n",
    " \n",
    "        return output, hidden, firing_rate_reg, dynamic_reg\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state with batch dimension\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "def compute_l2_regularization(parameters, alpha):\n",
    "    l2_reg = sum(p.pow(2.0).sum() for p in parameters)\n",
    "    return alpha * l2_reg\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 16 # Features + Go Cue\n",
    "hidden_size = 150\n",
    "output_size = 2  # Number of muscles\n",
    "g = 1.5  # g value\n",
    "h_val = 1.0  # h value\n",
    "\n",
    "# Hyperparameters for regularization\n",
    "alpha = 1e-4  \n",
    "beta = 0.03\n",
    "gamma = 1e-4\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 200\n",
    "epoch_losses = [] \n",
    "val_losses = []\n",
    "\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "# get available device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Model instantiation\n",
    "model = SimpleRNN(input_size, hidden_size, output_size, g, h_val)\n",
    "model.to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # MSE Loss for regression tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0) ### WD=0. Note: Paper uses Hessian-Free optimizer\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    # Initialize hidden states \n",
    "    hidden_states_for_plot = []\n",
    "\n",
    "    for inputs, targets in simple_train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        print(inputs.shape)\n",
    "        h = model.init_hidden(batch_size).to(device)\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        total_firing_rate_reg = 0\n",
    "        total_dynamic_reg = 0\n",
    "\n",
    "        with autocast():  # Enable automatic mixed precision\n",
    "            for t in range(inputs.shape[1]):\n",
    "                output, h, firing_rate_reg, dynamic_reg = model(inputs[:, t, :], h)\n",
    "                hidden_states_for_plot.append(h.detach().cpu().numpy())\n",
    "                total_firing_rate_reg += firing_rate_reg\n",
    "                total_dynamic_reg += dynamic_reg\n",
    "\n",
    "            # Compute loss and regularization terms\n",
    "            loss = criterion(output, targets[:, -1, :])\n",
    "            l2_reg = compute_l2_regularization(model.parameters(), alpha)\n",
    "            rfr_reg = beta * total_firing_rate_reg / inputs.shape[1] / hidden_size / num_conditions\n",
    "            rj_reg = gamma * total_dynamic_reg / inputs.shape[1] / num_conditions\n",
    "            total_loss = loss + l2_reg + rfr_reg + rj_reg\n",
    "\n",
    "        scaler.scale(total_loss).backward()  # Scale loss and perform backward pass\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        scaler.step(optimizer)  # Update optimizer\n",
    "        scaler.update()  # Update scaler\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(simple_train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {avg_loss}')\n",
    "\n",
    "    # Validation phase after completing the training for one epoch\n",
    "    val_loss = validate_model(model, simple_val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered at epoch\", epoch + 1)\n",
    "        early_stop = True\n",
    "        break\n",
    "\n",
    "    # Clear CUDA cache if needed\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Check if training was stopped by early stopping\n",
    "if early_stop:\n",
    "    print('Training stopped due to early stopping at epoch', epoch + 1)\n",
    "else:\n",
    "    print('Finished Training')\n",
    "# Testing phase\n",
    "test_loss = test_model(model, simple_test_loader, criterion, device)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# Clear cache after training\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# Determine the number of epochs for which you have loss data\n",
    "actual_num_epochs = len(epoch_losses)  # This will be less than num_epochs if early stopping was triggered\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, actual_num_epochs + 1), epoch_losses, label='Training Loss')\n",
    "plt.plot(range(1, actual_num_epochs + 1), val_losses, label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss (SimpleRNN)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd9a5b-0e80-4464-aa8e-9ec54d225646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Define a custom Rectified Tanh activation function\n",
    "def rectified_tanh(x):\n",
    "    return torch.where(x > 0, torch.tanh(x), 0) # was torch.where(x > 0, x, torch.tanh(x))\n",
    "def grad_rectified_tanh(x):\n",
    "    return torch.where(x > 0, 1 - torch.tanh(x)**2, 0)\n",
    "def grad_tanh(x):\n",
    "    return 1 - torch.tanh(x)**2\n",
    "\n",
    "# ComplicatedRNN class\n",
    "class ComplicatedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, g, h, tau=50):\n",
    "        super(ComplicatedRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = tau\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Weight initialization (unchanged)\n",
    "        self.J = nn.Parameter(torch.randn(hidden_size, hidden_size) * (g / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float))))\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, input_size) * (h / torch.sqrt(torch.tensor(input_size, dtype=torch.float)))) \n",
    "        self.bx = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # Nonlinearity (unchanged)\n",
    "        self.nonlinearity = rectified_tanh \n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Forward pass logic (same as SimpleRNN but without regularization terms)\n",
    "        hidden_prev = hidden.clone()\n",
    "        timestep = self.tau / 10\n",
    "        #Update hidden state\n",
    "        firing_rate = self.nonlinearity(hidden)        \n",
    "        hidden_update = torch.matmul(self.J, firing_rate.transpose(0, 1))\n",
    "        input_update = torch.matmul(self.B, x.transpose(0, 1))     \n",
    "        new_hidden = hidden_update + input_update + self.bx.unsqueeze(1)\n",
    "        new_hidden = new_hidden.transpose(0, 1)\n",
    "        # Euler integration for continuous-time update\n",
    "        hidden = hidden + (timestep / self.tau) * (-hidden_prev + new_hidden)\n",
    "        output = self.output_linear(firing_rate)\n",
    "        \n",
    "        return output, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "# Training loop\n",
    "# Hyperparameters\n",
    "input_size = 16\n",
    "hidden_size = 150\n",
    "output_size = 2  # Number of muscles\n",
    "g = 4  # g value\n",
    "h_val = 1.0  # h value\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 300\n",
    "epoch_losses = [] \n",
    "val_losses = []\n",
    "\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "# get available device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Model instantiation\n",
    "complicated_model = ComplicatedRNN(input_size, hidden_size, output_size, g, h_val)\n",
    "complicated_model.to(device)\n",
    "\n",
    "# Loss function and optimizer (no weight decay)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(complicated_model.parameters(), lr=0.001, weight_decay=0)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop \n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    hidden_states_for_plot_cm = []\n",
    "\n",
    "    complicated_model.train()  # Set the model to training mode\n",
    "    for inputs, targets in complicated_train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        h = complicated_model.init_hidden(batch_size).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # Apply automatic mixed precision\n",
    "            for t in range(inputs.shape[1]):\n",
    "                output, h = complicated_model(inputs[:, t, :], h)\n",
    "                hidden_states_for_plot_cm.append(h.detach().cpu().numpy())\n",
    "\n",
    "            loss = criterion(output, targets[:, -1, :])\n",
    "\n",
    "        scaler.scale(loss).backward()  # Scale loss for backward pass\n",
    "        scaler.step(optimizer)  # Update optimizer with scaled gradients\n",
    "        scaler.update()  # Update the scaler\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(complicated_train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {avg_loss}')\n",
    "\n",
    "    # Validation phase after completing the training for one epoch\n",
    "    val_loss = validate_model(complicated_model, complicated_val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered at epoch\", epoch + 1)\n",
    "        early_stop = True\n",
    "        break\n",
    "\n",
    "    # Clear CUDA cache if needed\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Check if training was stopped by early stopping\n",
    "if early_stop:\n",
    "    print('Training stopped due to early stopping at epoch', epoch + 1)\n",
    "else:\n",
    "    print('Finished Training')\n",
    "# Testing phase\n",
    "test_loss = test_model(complicated_model, complicated_test_loader, criterion, device)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# Clear cache after training\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# Determine the number of epochs for which you have loss data\n",
    "actual_num_epochs = len(epoch_losses)  # This will be less than num_epochs if early stopping was triggered\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, actual_num_epochs + 1), epoch_losses, label='Training Loss')\n",
    "plt.plot(range(1, actual_num_epochs + 1), val_losses, label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss (ComplicatedRNN)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd598c-a42e-4219-aa9b-5b7e54710749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_psth(data, title, bin_size=10):\n",
    "    \"\"\"\n",
    "    Plot Peri-Stimulus Time Histogram (PSTH) for given data.\n",
    "    :param data: a tensor containing the neural data of shape [conditions, delays, time, features]\n",
    "    :param title: a string for the plot title\n",
    "    :param bin_size: size of time bins for averaging\n",
    "    \"\"\"\n",
    "    # Averaging neural activity across conditions, delays for each time bin\n",
    "    mean_data = data.mean(dim=(0, 1))  # Mean across conditions and delays\n",
    "\n",
    "    # Number of bins\n",
    "    n_bins = mean_data.shape[0] // bin_size\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    binned_data = mean_data[:n_bins*bin_size].unfold(0, bin_size, bin_size).mean(dim=2)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(binned_data.shape[1]):  # Iterate over each feature/channel\n",
    "        plt.plot(binned_data[:, i], label=f'Feature {i+1}')\n",
    "    plt.xlabel('Time (bins)')\n",
    "    plt.ylabel('Average Activity')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_psth(normalized_muscle_tensor, \"PSTH for Arm Movement\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5362de-9acf-4380-af3b-68acded8eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Slicing to take only the last 296 timesteps\n",
    "last_hidden_states = hidden_states_for_plot[-296:]\n",
    "\n",
    "# Apply the nonlinearity to each hidden state before averaging\n",
    "rectified_tanh = lambda x: np.where(x > 0, np.tanh(x), 0)\n",
    "hidden_states = rectified_tanh(np.array(last_hidden_states))\n",
    "\n",
    "# Calculate the mean across all batches for each time step\n",
    "mean_activations = np.mean(hidden_states, axis=1)\n",
    "\n",
    "# Plot the PSTHs for the first few neurons\n",
    "neurons_to_plot = 5  \n",
    "time_steps = mean_activations.shape[0]\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(min(neurons_to_plot, hidden_states.shape[2])):\n",
    "    plt.plot(range(time_steps), mean_activations[:, i], label=f'Neuron {i+1}')\n",
    "\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Average Activation')\n",
    "plt.title('PSTHs of Hidden Units in SimpleRNN')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b011e-0fa3-4e5f-a423-0074f069f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Slicing to take only the last 296 timesteps\n",
    "last_hidden_states = hidden_states_for_plot_cm[-296:]\n",
    "\n",
    "# Apply the nonlinearity to each hidden state before averaging\n",
    "rectified_tanh = lambda x: np.where(x > 0, np.tanh(x), 0)\n",
    "hidden_states = rectified_tanh(np.array(last_hidden_states))\n",
    "\n",
    "# Calculate the mean across all batches for each time step\n",
    "mean_activations = np.mean(hidden_states, axis=1)\n",
    "\n",
    "# Plot the PSTHs for the first few neurons\n",
    "neurons_to_plot = 5  \n",
    "time_steps = mean_activations.shape[0]\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(min(neurons_to_plot, hidden_states.shape[2])):\n",
    "    plt.plot(range(time_steps), mean_activations[:, i], label=f'Neuron {i+1}')\n",
    "\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Average Activation')\n",
    "plt.title('PSTHs of Hidden Units in ComplicatedRNN')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee0281-a790-4cce-b4f2-2bd0e8da93a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def perturb_inputs(model, inputs, perturbation_strength):\n",
    "    # Perturb the inputs by adding random noise scaled by the perturbation strength\n",
    "    noise = torch.randn_like(inputs) * perturbation_strength\n",
    "    perturbed_inputs = inputs + noise\n",
    "    return perturbed_inputs\n",
    "\n",
    "def compute_loss(model, inputs, targets, criterion, device):\n",
    "    batch_size = inputs.size(0)\n",
    "    h = model.init_hidden(batch_size).to(device)  # Initialize hidden state\n",
    "    for t in range(inputs.shape[1]):  # Iterate over time steps\n",
    "        model_output = model(inputs[:, t, :], h)\n",
    "        output, h, *rest = model_output\n",
    "    # Compute loss for the entire sequence \n",
    "    mean_targets = torch.mean(targets, dim=1)\n",
    "    loss = criterion(output, mean_targets).item()\n",
    "    return loss\n",
    "\n",
    "def test_perturbed_inputs(model, perturbation_strengths, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    perturbation_errors_diffs = []\n",
    "\n",
    "    for strength in perturbation_strengths:\n",
    "        batch_errors_diffs = []  # Store differences in errors for each batch in the test_loader\n",
    "        \n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Compute error for original inputs\n",
    "            original_loss = compute_loss(model, inputs, targets, criterion, device)\n",
    "            \n",
    "            # Compute error for perturbed inputs\n",
    "            perturbed_inputs = perturb_inputs(model, inputs, strength)\n",
    "            perturbed_loss = compute_loss(model, perturbed_inputs, targets, criterion, device)\n",
    "            \n",
    "            # Calculate the difference in errors and store it\n",
    "            error_diff = abs(perturbed_loss - original_loss)\n",
    "            batch_errors_diffs.append(error_diff)\n",
    "        \n",
    "        mean_error_diff = np.mean(batch_errors_diffs)  # Calculate mean difference in error for this perturbation strength\n",
    "        perturbation_errors_diffs.append(mean_error_diff)\n",
    "        print(f'Perturbation strength: {strength}, Mean Error Difference: {mean_error_diff}, Model: {model}')\n",
    "    \n",
    "    return perturbation_errors_diffs\n",
    "\n",
    "perturbation_strengths = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "simple_model_errors = test_perturbed_inputs(model, perturbation_strengths, simple_train_loader, criterion, device)\n",
    "complex_model_errors = test_perturbed_inputs(complicated_model, perturbation_strengths, complicated_train_loader, criterion, device)\n",
    "\n",
    "print(\"simple model errors\", simple_model_errors)\n",
    "print(\"complex model errors\", complex_model_errors)\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.35  # Adjust this as needed\n",
    "\n",
    "# Create an array of indices for the x-axis ticks\n",
    "x_indices = np.arange(len(perturbation_strengths))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot bars for the simple model\n",
    "plt.bar(x_indices - bar_width/2, simple_model_errors, width=bar_width, color='blue', label='Simple RNN')\n",
    "\n",
    "# Plot bars for the complex model\n",
    "plt.bar(x_indices + bar_width/2, complex_model_errors, width=bar_width, color='red', label='Complicated RNN')\n",
    "\n",
    "# Labeling and aesthetics\n",
    "plt.xlabel('Perturbation Strength')\n",
    "plt.ylabel('Unnormalized Error')\n",
    "plt.title('Mean difference in error between original and perturbed inputs')\n",
    "\n",
    "# Set custom tick positions and labels on the x-axis\n",
    "plt.xticks(x_indices, perturbation_strengths)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the mean and standard deviation of simple_model_errors\n",
    "simple_mean = np.mean(simple_model_errors)\n",
    "simple_std = np.std(simple_model_errors)\n",
    "\n",
    "# Normalize and standardize simple_model_errors\n",
    "normalized_simple_errors = (simple_model_errors - np.min(simple_model_errors)) / (np.max(simple_model_errors) - np.min(simple_model_errors))\n",
    "standardized_simple_errors = (simple_model_errors - simple_mean) / simple_std\n",
    "\n",
    "# Calculate the mean and standard deviation of complex_model_errors\n",
    "complex_mean = np.mean(complex_model_errors)\n",
    "complex_std = np.std(complex_model_errors)\n",
    "\n",
    "# Normalize and standardize complex_model_errors\n",
    "normalized_complex_errors = (complex_model_errors - np.min(complex_model_errors)) / (np.max(complex_model_errors) - np.min(complex_model_errors))\n",
    "standardized_complex_errors = (complex_model_errors - complex_mean) / complex_std\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot bars for the simple model\n",
    "plt.bar(x_indices - bar_width/2, normalized_simple_errors, width=bar_width, color='blue', label='Simple RNN')\n",
    "\n",
    "# Plot bars for the complex model\n",
    "plt.bar(x_indices + bar_width/2, normalized_complex_errors, width=bar_width, color='red', label='Complicated RNN')\n",
    "\n",
    "# Labeling and aesthetics\n",
    "plt.xlabel('Perturbation Strength')\n",
    "plt.ylabel('Normalized and Standardized Error')\n",
    "plt.title('Mean difference in error between original and perturbed inputs')\n",
    "\n",
    "# Set custom tick positions and labels on the x-axis\n",
    "plt.xticks(x_indices, perturbation_strengths)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73366bc5-4ce8-4ad0-8eef-9da6044deef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def perturb_recurrent_weights(model, perturbation_strength):\n",
    "    # Perturb the recurrent weight matrix J by adding Gaussian noise\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn_like(model.J) * perturbation_strength\n",
    "        perturbed_weights = model.J + noise\n",
    "        return perturbed_weights\n",
    "\n",
    "def test_perturbed_structure(model, perturbation_strengths, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    perturbation_errors = []\n",
    "    original_weights = model.J.data.clone()  # Save the original weights\n",
    "\n",
    "    for strength in perturbation_strengths:\n",
    "        batch_errors = []  # Store errors for each batch in the test_loader\n",
    "        # Perturb the recurrent weights of the model\n",
    "        perturbed_weights = perturb_recurrent_weights(model, strength)\n",
    "        model.J.data = perturbed_weights.data\n",
    "\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            h = model.init_hidden(batch_size).to(device)  # Initialize hidden state\n",
    "            \n",
    "            for t in range(inputs.shape[1]):  # Iterate over time steps\n",
    "                model_output = model(inputs[:, t, :], h)\n",
    "                if len(model_output) == 4:  # If the model returns 4 outputs, unpack all (simple model)\n",
    "                    output, h, _, _ = model_output\n",
    "                else:  # If the model returns 2 outputs, unpack accordingly (complicated model)\n",
    "                    output, h = model_output\n",
    "            \n",
    "            # Compute loss for the entire sequence (last output)\n",
    "            loss = criterion(output, targets[:, -1, :]).item()\n",
    "            batch_errors.append(loss)  # Append the loss of this batch\n",
    "        \n",
    "        # Restore the original weights before the next iteration\n",
    "        model.J.data = original_weights.data\n",
    "        \n",
    "        mean_error = np.mean(batch_errors)  # Calculate mean error for this perturbation strength\n",
    "        perturbation_errors.append(mean_error)\n",
    "        print(f'Perturbation strength: {strength}, Mean Error: {mean_error}')\n",
    "    \n",
    "    return perturbation_errors\n",
    "\n",
    "# Define your perturbation strengths\n",
    "perturbation_strengths = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "# Get errors for simple and complex models\n",
    "simple_model_structural_errors = test_perturbed_structure(model, perturbation_strengths, simple_train_loader, criterion, device)\n",
    "complex_model_structural_errors = test_perturbed_structure(complicated_model, perturbation_strengths, complicated_train_loader, criterion, device)\n",
    "\n",
    "# Normalize the errors by dividing by the maximum error in each group\n",
    "max_simple_error = max(simple_model_structural_errors)\n",
    "max_complex_error = max(complex_model_structural_errors)\n",
    "simple_model_normalized_errors = [error / max_simple_error for error in simple_model_structural_errors]\n",
    "complex_model_normalized_errors = [error / max_complex_error for error in complex_model_structural_errors]\n",
    "\n",
    "# Calculate the mean and standard deviation of simple_model_normalized_errors\n",
    "simple_normalized_mean = np.mean(simple_model_normalized_errors)\n",
    "simple_normalized_std = np.std(simple_model_normalized_errors)\n",
    "\n",
    "# Calculate the mean and standard deviation of complex_model_normalized_errors\n",
    "complex_normalized_mean = np.mean(complex_model_normalized_errors)\n",
    "complex_normalized_std = np.std(complex_model_normalized_errors)\n",
    "\n",
    "# Standardize simple_model_normalized_errors\n",
    "standardized_simple_normalized_errors = [(error - simple_normalized_mean) / simple_normalized_std for error in simple_model_normalized_errors]\n",
    "\n",
    "# Standardize complex_model_normalized_errors\n",
    "standardized_complex_normalized_errors = [(error - complex_normalized_mean) / complex_normalized_std for error in complex_model_normalized_errors]\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.35  # Adjust this as needed\n",
    "\n",
    "# Create an array of indices for the x-axis ticks\n",
    "x_indices = np.arange(len(perturbation_strengths))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size\n",
    "\n",
    "# Plot bars for the simple model\n",
    "plt.bar(x_indices - bar_width/2, simple_model_normalized_errors, width=bar_width, color='blue', label='Simple RNN')\n",
    "\n",
    "# Plot bars for the complex model\n",
    "plt.bar(x_indices + bar_width/2, complex_model_normalized_errors, width=bar_width, color='red', label='Complicated RNN')\n",
    "\n",
    "# Labeling \n",
    "plt.xlabel('Perturbation Strength (log scale)')\n",
    "plt.ylabel('Normalized Error')\n",
    "plt.title('Perturbation test of the weights')\n",
    "\n",
    "# Set custom tick positions and labels on the x-axis\n",
    "plt.xticks(x_indices, perturbation_strengths)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257161c9-9155-4dfd-b089-b12ac2725dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
