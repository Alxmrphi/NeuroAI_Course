{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d207f0be-cbd8-4b93-a823-61af51421e2a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Generalization in AI\n",
    "\n",
    "**Week 1, Day 1: Generalization**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Names & Surnames\n",
    "\n",
    "__Content reviewers:__ Names & Surnames\n",
    "\n",
    "__Production editors:__ Names & Surnames\n",
    "\n",
    "<br>\n",
    "\n",
    "Acknowledgments: [ACKNOWLEDGMENT_INFORMATION]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d71fc6-0728-41bd-b84e-ac78a622ece4",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: [insert estimated duration of whole tutorial in minutes]*\n",
    "\n",
    "In this tutorial, you will investigate the problem of generalization in artificial intelligence, neuroscience and cognitive science. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff267ac-6f23-4ef3-bb42-098ce864d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "## Uncomment the code below to test your function\n",
    "\n",
    "#from IPython.display import IFrame\n",
    "#link_id = \"<YOUR_LINK_ID_HERE>\"\n",
    "\n",
    "print(\"If you want to download the slides: 'Link to the slides'\")\n",
    "      # Example: https://osf.io/download/{link_id}/\n",
    "\n",
    "#IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee444563-8ef0-4b20-b8d7-92eeace85488",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57acdfc5-c864-40a0-b648-be385d5c3eb5",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown\n",
    "\n",
    "#!pip install numpy Pillow matplotlib torch torchvision transformers gradio sentencepiece protobuf\n",
    "#!pip install git+https://github.com/Belval/TextRecognitionDataGenerator#egg=trdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40270953",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown\n",
    "\n",
    "# Standard Libraries for file and operating system operations, security, and web requests\n",
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "import logging\n",
    "import io\n",
    "\n",
    "# Core python data science and image processing libraries\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep Learning and model specific libraries\n",
    "import torch\n",
    "import transformers\n",
    "from torchvision import transforms\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import google.protobuf\n",
    "\n",
    "# Utility and interface libraries\n",
    "import gradio as gr\n",
    "from IPython.display import IFrame\n",
    "import trdg\n",
    "from trdg.generators import GeneratorFromStrings\n",
    "import sentencepiece\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa95f5",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf34b9a-1dd5-458a-b390-0fa12609d532",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "# @markdown\n",
    "\n",
    "def display_image(image_path):\n",
    "    \"\"\"Display an image from a given file path.\n",
    "\n",
    "    Inputs:\n",
    "    - image_path (str): The path to the image file.\n",
    "    \"\"\"\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.show()\n",
    "\n",
    "def display_transformed_images(image, transformations):\n",
    "    \"\"\"\n",
    "    Apply a list of transformations to an image and display them.\n",
    "\n",
    "    Inputs:\n",
    "    - image (Tensor): The input image as a tensor.\n",
    "    - transformations (list): A list of torchvision transformations to apply.\n",
    "    \"\"\"\n",
    "    # Convert tensor image to PIL Image for display\n",
    "    pil_image = transforms.ToPILImage()(image)\n",
    "\n",
    "    fig, axs = plt.subplots(len(transformations) + 1, 1, figsize=(5, 15))\n",
    "    axs[0].imshow(pil_image)\n",
    "    axs[0].set_title('Original')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, transform in enumerate(transformations):\n",
    "        # Apply transformation if it's not the placeholder\n",
    "        if transform != \"Custom ElasticTransform Placeholder\":\n",
    "            transformed_image = transform(image)\n",
    "            # Convert transformed tensor image to PIL Image for display\n",
    "            display_image = transforms.ToPILImage()(transformed_image)\n",
    "            axs[i+1].imshow(display_image)\n",
    "            axs[i+1].set_title(transform.__class__.__name__)\n",
    "            axs[i+1].axis('off')\n",
    "        else:\n",
    "            axs[i+1].text(0.5, 0.5, 'ElasticTransform Placeholder', ha='center')\n",
    "            axs[i+1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_original_and_transformed_images(original_tensor, transformed_tensor):\n",
    "    \"\"\"\n",
    "    Display the original and transformed images side by side.\n",
    "\n",
    "    Inputs:\n",
    "    - original_tensor (Tensor): The original image as a tensor.\n",
    "    - transformed_tensor (Tensor): The transformed image as a tensor.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Display original image\n",
    "    original_image = original_tensor.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    axs[0].imshow(original_image)\n",
    "    axs[0].set_title('Original')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Display transformed image\n",
    "    transformed_image = transformed_tensor.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    axs[1].imshow(transformed_image)\n",
    "    axs[1].set_title('Transformed')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def display_generated_images(generator):\n",
    "    \"\"\"\n",
    "    Display images generated from strings.\n",
    "\n",
    "    Inputs:\n",
    "    - generator (GeneratorFromStrings): A generator that produces images from strings.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, (text_img, lbl) in enumerate(generator, 1):\n",
    "        ax = plt.subplot(1, len(generator.strings) * generator.count // len(generator.strings), i)\n",
    "        plt.imshow(text_img)\n",
    "        plt.title(f\"Example {i}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a34ac-fa41-4e90-ab45-82c14384a83e",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "# @markdown\n",
    "\n",
    "def download_file(fname, url, expected_md5):\n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL and saves it locally.\n",
    "\n",
    "    Inputs:\n",
    "    - fname (str): The local filename/path to save the downloaded file.\n",
    "    - url (str): The URL from which to download the file.\n",
    "    - expected_md5 (str): The expected MD5 checksum to verify the integrity of the downloaded data.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url)  # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content)  # Write the downloaded content to a file\n",
    "                print(f\"{fname} has been downloaded successfully.\")\n",
    "\n",
    "# Variables for the font file and download URL\n",
    "fname = \"Dancing_Script.zip\"\n",
    "url = \"https://osf.io/32yed/download\"\n",
    "expected_md5 = \"d59bd3201b58a37d0d3b4cd0b0ec7400\"\n",
    "\n",
    "# Download the font file\n",
    "download_file(fname, url, expected_md5)\n",
    "\n",
    "def extract_zip(zip_fname):\n",
    "    \"\"\"\n",
    "    Extracts a ZIP file to the current directory.\n",
    "\n",
    "    Inputs:\n",
    "    - zip_fname (str): The filename/path of the ZIP file to be extracted.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_fname, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "\n",
    "# Extract the downloaded ZIP file\n",
    "extract_zip(fname)\n",
    "\n",
    "def download_image(fname, url, expected_md5):\n",
    "    \"\"\"\n",
    "    Downloads an image file from the given URL and saves it locally.\n",
    "\n",
    "    Inputs:\n",
    "    - fname (str): The local filename/path to save the downloaded image.\n",
    "    - url (str): The URL from which to download the image.\n",
    "    - expected_md5 (str): The expected MD5 checksum to verify the integrity of the downloaded data.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url) # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content) # Write the downloaded content to a file\n",
    "\n",
    "# Variables for file and download URL\n",
    "fnames = [\"img_1235.jpg\", \"image_augmentation.png\"]  # The names of the files to be downloaded\n",
    "urls = [\"https://osf.io/kv5bx/download\", \"https://osf.io/fqwsr/download\"]  # URLs from where the files will be downloaded\n",
    "expected_md5s = [\"920ae567f707bfee0be29dc854f804ed\", \"f4f1ebee1470a7e2d7662eec1d193ba2\"] # MD5 hashes for verifying files integrity\n",
    "\n",
    "for fname, url, expected_md5 in zip(fnames, urls, expected_md5s):\n",
    "    download_image(fname, url, expected_md5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3430cd5-ec37-4300-94e8-8bf7ffb82017",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Generalization in AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d91aa-c728-4b35-8920-97f6142ba268",
   "metadata": {},
   "source": [
    "## Tutorial-specific learning Objective: \n",
    "\n",
    "1. Describe some common goals of developers of deployed AI systems:\n",
    "- Performance, latency, SWaP-C\n",
    "- Explanation/understanding\n",
    "2. Describe (at least) 3 ways of tackling generalization with AI, including the most currently popular trends of fitting generic models on large-scale datasets (i.e. bitter lesson)\n",
    "3. Get hands-on experience in the basics of deep learning and PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02494b53-3c84-4c46-9e31-bcab2eda70b9",
   "metadata": {},
   "source": [
    "## Video and background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616cb60-a9ae-466d-a112-f6d114d92e3e",
   "metadata": {},
   "source": [
    "### Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ca8c3-c1a3-45e2-8100-d636f8921030",
   "metadata": {},
   "source": [
    "## Activity 1: Introducing the TrOCR (Transformer-based Optical Character Recognition) Model\n",
    "In this section, we introduce TrOCR, a transformer-based model developed by Microsoft, designed for Optical Character Recognition (OCR). The goal of TrOCR is to accurately convert images of text into machine-readable text. This model is an example of how AI can bridge the gap between visual data and text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09a332-2ddd-47aa-9541-30e0302f84b1",
   "metadata": {},
   "source": [
    "### Loading \n",
    "\n",
    "We start by loading the pre-trained TrOCR model and its associated processor. The model VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-handwritten\") is specifically trained to recognize handwritten text, while the TrOCRProcessor is used for preparing the input images and decoding the model's predictions into human-readable text. This demonstrates the ease of accessing and utilizing advanced machine learning models with just a few lines of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00326ca-807d-460f-adf4-767e94bc0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained TrOCR model and processor\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-handwritten\")\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-handwritten\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f33da0-8340-431b-913a-fa1f38ff3da6",
   "metadata": {},
   "source": [
    "Okay, the model is loaded! Now, we need a \"recognize_text\" function. This is a crucial component that showcases how to apply the loaded model to perform OCR on an input image. It uses a pre-trained model for Optical Character Recognition (OCR) to convert an image to text. It preprocesses the image to the model's input specifications, predicts the text as token IDs, decodes these tokens into human-readable text while skipping special tokens, and returns the recognized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1debc6-b3b1-4173-839c-f2a4240efb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to recognize text from an image\n",
    "def recognize_text(image):\n",
    "    \"\"\"\n",
    "    This function takes an image as input and uses a pre-trained language model to generate text from the image.\n",
    "\n",
    "    Inputs:\n",
    "    - image (PIL Image or Tensor): The input image containing text to be recognized.\n",
    "\n",
    "    Outputs:\n",
    "    - text (str): The recognized text extracted from the input image.\n",
    "    \"\"\"\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bba7aed-2887-49c8-ac28-c6f704e89cb8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we employ Gradio to create an interactive web interface for our TrOCR demo. Gradio is a Python library that simplifies creating UIs for machine learning models. This interface allows users to upload images and see the recognized text, demonstrating the model's capabilities interactively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b46ff0-958b-41f1-b00a-2fd0464b3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=recognize_text,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=gr.Textbox(),\n",
    "    title=\"Interactive demo: TrOCR\",\n",
    "    description=\"Demo for Microsoft’s TrOCR, an encoder-decoder model for OCR on single-text line images.\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2bc209-67df-410d-a2c4-d5855a1421c8",
   "metadata": {},
   "source": [
    "## Activity 2: Inspecting the Model's Encoder and Decoder\n",
    "By inspecting the model's encoder and decoder, we gain insight into the inner workings of the TrOCR model. The encoder processes the input images, and the decoder generates text predictions. This step is crucial for understanding the model architecture and the roles of its components in the text recognition process. Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cd9c9-95fb-4541-9ece-78620745ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the encoder of the model\n",
    "model.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afead452-953b-4114-8525-09071dcbe85b",
   "metadata": {},
   "source": [
    "### Coding exercise\n",
    "\n",
    "In the cell below, inspect the decoder of the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1706d-16d6-41a1-a685-a23741c0b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f50d0-4fc9-4162-83e9-5e7a83168aec",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Follow-up coding exercise \n",
    "\n",
    "Now, we want to create a count_parameters function that calculates the total number of parameters in the model, providing a sense of the model's complexity. Counting parameters in the encoder and decoder separately helps appreciate the distribution of model complexity and its implications on training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df898317-31e6-4b28-9175-c1206b255a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the parameters of the model\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    This function calculates the total number of parameters in a given PyTorch model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (torch.nn.Module): The PyTorch model for which parameters are to be counted.\n",
    "\n",
    "    Outputs:\n",
    "    - num_parameters (int): The total number of parameters in the specified model.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Count parameters in the encoder\n",
    "encoder_params = count_parameters(model.encoder)\n",
    "\n",
    "# Count parameters in the decoder\n",
    "decoder_params = count_parameters(model.decoder)\n",
    "\n",
    "encoder_params, decoder_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d7a60-21cd-4556-a0dc-43bc3ba9f478",
   "metadata": {},
   "source": [
    "### Written exercise\n",
    "\n",
    "Look at the available variants of this model (i.e. small, base, large). How do these models trade off in terms of accuracy vs. inference latency? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50088f7-6e0a-4dc9-bfd5-6072ed94a7b7",
   "metadata": {},
   "source": [
    "### Discussion point\n",
    "\n",
    "What kinds of inductive biases are embedded inside a model such as this one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa316df-4675-44be-a3b8-49c60c469ed5",
   "metadata": {},
   "source": [
    "## Activity 3: generalization, part I: transfer learning \n",
    "\n",
    "In this section, we take a look at how much data is distilled inside the model, focusing on the decoder. We want to calculate how long it would take to write a certain number of words, comparing human writing capabilities with the model's text generation speed. This will prompt us to discuss the efficiency of machine learning models in automating tasks traditionally performed by humans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ada4c5-ea4f-4feb-917d-1f9b43b11865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_writing_time(total_words, words_per_day, days_per_week, weeks_per_year, average_human_lifespan):\n",
    "    \"\"\"\n",
    "    Calculate the time required to write a given number of words in lifetimes.\n",
    "\n",
    "    Inputs:\n",
    "    - total_words (int): total number of words to be written.\n",
    "    - words_per_day (int): number of words written per day.\n",
    "    - days_per_week (int): number of days dedicated to writing per week.\n",
    "    - weeks_per_year (int): number of weeks dedicated to writing per year.\n",
    "    - average_human_lifespan (int): average lifespan of a human in years.\n",
    "\n",
    "    Outpus:\n",
    "    - time_to_write_lifetimes (float): time to write the given words in lifetimes.\n",
    "    \"\"\"\n",
    "\n",
    "    #################################################\n",
    "    ## TODO for students: fill in the missing variables ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "    #################################################\n",
    "\n",
    "    words_per_year = words_per_day * days_per_week * weeks_per_year\n",
    "\n",
    "    # Calculate the time to write in years\n",
    "    time_to_write_years = total_words / ...\n",
    "\n",
    "    # Calculate the time to write in lifetimes\n",
    "    time_to_write_lifetimes = time_to_write_years / average_human_lifespan\n",
    "\n",
    "    return time_to_write_lifetimes\n",
    "\n",
    "# Example values\n",
    "total_words = 5e9\n",
    "words_per_day = 1500\n",
    "days_per_week = 6\n",
    "weeks_per_year = 50\n",
    "average_human_lifespan = 80\n",
    "\n",
    "# Uncomment the code below to test your function\n",
    "\n",
    "# Test the function\n",
    "#time_to_write_lifetimes_roberta = calculate_writing_time(\n",
    "    #total_words,\n",
    "    #words_per_day,\n",
    "    #days_per_week,\n",
    "    #weeks_per_year,\n",
    "    #average_human_lifespan\n",
    "#)\n",
    "\n",
    "# Print the result\n",
    "#print(f\"Time to write {total_words} words in lifetimes: {time_to_write_lifetimes_roberta} lifetimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6127df-6dde-445f-8b49-3df6ba9064fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def calculate_writing_time(total_words, words_per_day, days_per_week, weeks_per_year, average_human_lifespan):\n",
    "    \"\"\"\n",
    "    Calculate the time required to write a given number of words in lifetimes.\n",
    "\n",
    "    Inputs:\n",
    "    - total_words (int): total number of words to be written.\n",
    "    - words_per_day (int): number of words written per day.\n",
    "    - days_per_week (int): number of days dedicated to writing per week.\n",
    "    - weeks_per_year (int): number of weeks dedicated to writing per year.\n",
    "    - average_human_lifespan (int): average lifespan of a human in years.\n",
    "\n",
    "    Outpus:\n",
    "    - time_to_write_lifetimes (float): time to write the given words in lifetimes.\n",
    "    \"\"\"\n",
    "\n",
    "    words_per_year = words_per_day * days_per_week * weeks_per_year\n",
    "\n",
    "    # Calculate the time to write in years\n",
    "    time_to_write_years = total_words / words_per_year\n",
    "\n",
    "    # Calculate the time to write in lifetimes\n",
    "    time_to_write_lifetimes = time_to_write_years / average_human_lifespan\n",
    "\n",
    "    return time_to_write_lifetimes\n",
    "\n",
    "# Example values\n",
    "total_words = 5e9\n",
    "words_per_day = 1500\n",
    "days_per_week = 6\n",
    "weeks_per_year = 50\n",
    "average_human_lifespan = 80\n",
    "\n",
    "# Uncomment the code below to test your function\n",
    "\n",
    "# Test the function\n",
    "#time_to_write_lifetimes_roberta = calculate_writing_time(\n",
    "    #total_words,\n",
    "    #words_per_day,\n",
    "    #days_per_week,\n",
    "    #weeks_per_year,\n",
    "    #average_human_lifespan\n",
    "#)\n",
    "\n",
    "# Print the result\n",
    "#print(f\"Time to write {total_words} words in lifetimes: {time_to_write_lifetimes_roberta} lifetimes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e55acc-2fab-4df6-8298-f4f41f868205",
   "metadata": {},
   "source": [
    "### Exploring LLaMA 2\n",
    "\n",
    "RoBERTa is a pretty tiny model by modern standards. A more modern LLM like Llama 2 is trained on 2 trillion tokens. How much time would it take to generate that much text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19006eb-4600-4aa4-9c99-b85b5e4a87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring Llama 2\n",
    "total_tokens_llama2 = 2e12\n",
    "total_words_llama2 = 2e12 / 1.5 #assuming 1.5 words per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785a098-d6ff-48a8-8b57-3e0e9e436ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to generate text\n",
    "time_to_write_lifetimes_llama = calculate_writing_time(total_words_llama2, words_per_day, days_per_week, weeks_per_year, average_human_lifespan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc491eab-ca40-46ee-9e66-dadb13240d69",
   "metadata": {},
   "source": [
    "Assuming 1.5 tokens/word, it would take ~=37,000 lifetimes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce0081-636c-4f40-bc2f-fc8f2d22dee7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Activity 3: generalization, part II: augmentation\n",
    "\n",
    "This section introduces the concept of image augmentation, a technique used to increase the diversity of training data and improve model generalization. When data is not abundant, we can improve generalization by augmenting the existing dataset with variants of the same data. Thus, we take an expressive model with few built-in inductive biases, and through demonstrations, let it learn invariances and equivariances in the data, encouraging generalization.\n",
    "\n",
    "By applying various transformations to images and displaying the results, you can visually understand how augmentation works and its impact on model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4dcd8-f11a-447a-bf33-564f90239456",
   "metadata": {},
   "source": [
    "Let's start with loading and visualizing our chosen image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78127c97-955e-46e0-869d-7f7285449f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "image_path = 'img_1235.jpg'\n",
    "display_image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68adf7a5-9cf0-457d-b645-38a26cddec10",
   "metadata": {},
   "source": [
    "Now, we will apply a few transformations to this image. You can play around with the input values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0a320-46f3-4f84-8c51-c8cd07cea776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PIL Image to Tensor\n",
    "image = Image.open(image_path)\n",
    "image = transforms.ToTensor()(image)\n",
    "\n",
    "# Define each transformation separately\n",
    "# RandomAffine: applies rotations, translations, scaling. Here, rotates by up to ±15 degrees,\n",
    "affine = transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1))\n",
    "\n",
    "# ElasticTransform: applies elastic distortions to the image. The 'alpha' parameter controls\n",
    "# the intensity of the distortion.\n",
    "elastic = transforms.ElasticTransform(alpha=50.0)\n",
    "\n",
    "# RandomPerspective: applies random perspective transformations with a specified distortion scale.\n",
    "perspective = transforms.RandomPerspective(distortion_scale=0.2, p=1.0)\n",
    "\n",
    "# RandomErasing: randomly erases a rectangle area in the image.\n",
    "erasing = transforms.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random', inplace=False)\n",
    "\n",
    "# GaussianBlur: applies gaussian blur with specified kernel size and sigma range.\n",
    "gaussian_blur = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ee77d-988c-474a-bc76-582b7315e436",
   "metadata": {},
   "source": [
    "Let's now combine them in a single list and display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e691d06-e1bc-45fc-b289-89827eaf0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of all transformations for iteration\n",
    "transformations = [affine, elastic, perspective, erasing, gaussian_blur]\n",
    "\n",
    "# Display\n",
    "display_transformed_images(image, transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ed9ac-5bb8-4e07-8bb1-6a71442a6244",
   "metadata": {},
   "source": [
    "We can also use transforms.Compose from PyTorch to apply all of these transformations simultaneously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c000dc-2c1e-41e6-a988-ab75819b0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the image\n",
    "image_path = 'image_augmentation.png'\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Convert PIL Image to Tensor\n",
    "image_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "# Define transformations here\n",
    "affine = transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1))\n",
    "elastic = transforms.ElasticTransform(alpha=90.0)\n",
    "perspective = transforms.RandomPerspective(distortion_scale=0.2, p=1.0)\n",
    "erasing = transforms.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random', inplace=False)\n",
    "gaussian_blur = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "\n",
    "# Combine all the transformations\n",
    "all_transforms = transforms.Compose([\n",
    "    affine,\n",
    "    elastic,\n",
    "    perspective,\n",
    "    erasing,\n",
    "    gaussian_blur\n",
    "])\n",
    "\n",
    "# Apply combined transformation\n",
    "augmented_image_tensor = all_transforms(image_tensor)\n",
    "\n",
    "display_original_and_transformed_images(image_tensor, augmented_image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb05cb5-9d0c-427d-a246-f80c09bea0ec",
   "metadata": {},
   "source": [
    "## Activity 3: generalization, part III: synthetic data\n",
    "\n",
    "When augmentation is not enough, we can further improve generalization by training on synthetic data. This allows us to stretch our data even further. Here, we will define strings and create a generator to generate a synthetic version of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1a69c-005a-41c1-8014-8fe9cb4c8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your strings\n",
    "strings = ['Hello', 'This is Patrick', 'From NMA']\n",
    "\n",
    "# Specify font path\n",
    "font_path = \"DancingScript-VariableFont_wght.ttf\"  # Ensure this path is correct\n",
    "\n",
    "# Create a generator with the specified parameters\n",
    "generator = GeneratorFromStrings(\n",
    "    strings=strings,\n",
    "    fonts=[font_path],\n",
    "    space_width=2,\n",
    "    skewing_angle=8,\n",
    "    count=3\n",
    ")\n",
    "\n",
    "# Define the desired size\n",
    "desired_size = (500, 300)  # Width, Height in pixels\n",
    "\n",
    "# Function to resize images\n",
    "def resize_image(image, new_size):\n",
    "    return image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "for img, lbl in generator:\n",
    "    # Resize the image before showing it\n",
    "    img = resize_image(img, desired_size)\n",
    "    img.show()\n",
    "\n",
    "# Call the function with the generator\n",
    "display_generated_images(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e6f2f-672d-41dc-9020-16d9f12259ff",
   "metadata": {},
   "source": [
    "### Discussion point\n",
    "\n",
    "What does this type of synthetic data capture that wouldn’t be easy to capture through data augmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7071b-4230-481f-9abd-b2967f53f279",
   "metadata": {},
   "source": [
    "### Generating handwriting style data\n",
    "\n",
    "We can take this idea further and generate handwriting style data. Use an embedded calligrapher.ai model to generate new snippets of writing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca19de-44b0-492d-af5b-8fa78fc5e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"https://www.calligrapher.ai/\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abdb822-ef4b-4c82-8ecf-dd1c3119be3a",
   "metadata": {},
   "source": [
    "### Discussion point\n",
    "\n",
    "What kinds of variation does this synthetic data capture that wouldn’t be easy to reproduce to all our other data generation pipelines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2b94c-e1b6-412b-a227-6345fd758b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D1_Tutorial1",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
