{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6221f0d3-4a18-416b-b548-3d9a5b82dedf",
   "metadata": {
    "id": "6221f0d3-4a18-416b-b548-3d9a5b82dedf"
   },
   "source": [
    "# Tutorial 2: Contrastive learning for object recognition\n",
    "\n",
    "**Week 1, Day 2: Comparing Tasks**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Andrew F. Luo, Leila Wehbe\n",
    "\n",
    "__Content reviewers:__ Names & Surnames\n",
    "\n",
    "__Production editors:__ Names & Surnames\n",
    "\n",
    "<br>\n",
    "\n",
    "Acknowledgments: [ACKNOWLEDGMENT_INFORMATION]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734dc52-6dc9-4669-a7c7-99ef87e47b8b",
   "metadata": {
    "id": "2734dc52-6dc9-4669-a7c7-99ef87e47b8b"
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 20 minutes*\n",
    "\n",
    "By the end of this tutorial, participants will be able to:\n",
    "1. Understand why we want to do contrastive learning.\n",
    "2. Understand the losses in contrastive learning.\n",
    "3. Run an example on contrastive learning using MNIST.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aceddeef-ea13-4224-8ca8-70563edbcf00",
   "metadata": {
    "id": "aceddeef-ea13-4224-8ca8-70563edbcf00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to download the slides: 'Link to the slides'\n"
     ]
    }
   ],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "\n",
    "## Uncomment the code below to test your function\n",
    "\n",
    "#from IPython.display import IFrame\n",
    "#link_id = \"<YOUR_LINK_ID_HERE>\"\n",
    "\n",
    "print(\"If you want to download the slides: 'Link to the slides'\")\n",
    "      # Example: https://osf.io/download/{link_id}/\n",
    "\n",
    "#IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ae64d-bc44-4e0e-b077-487da8391334",
   "metadata": {
    "id": "273ae64d-bc44-4e0e-b077-487da8391334"
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec851343-ee02-4f52-85da-a361297a5188",
   "metadata": {
    "id": "ec851343-ee02-4f52-85da-a361297a5188"
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "# !pip3 install vibecheck datatops --quiet\n",
    "\n",
    "# from vibecheck import DatatopsContentReviewContainer\n",
    "# def content_review(notebook_section: str):\n",
    "#     return DatatopsContentReviewContainer(\n",
    "#         \"\",  # No text prompt - leave this as is\n",
    "#         notebook_section,\n",
    "#         {\n",
    "#             \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "#             \"name\": \"sciencematch_sm\", # change the name of the course : neuromatch_dl, climatematch_ct, etc\n",
    "#             \"user_key\": \"y1x3mpx5\",\n",
    "#         },\n",
    "#     ).render()\n",
    "\n",
    "# feedback_prefix = \"W1D2_T1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3eeb07c-5de3-428e-a422-3a58f1124b43",
   "metadata": {
    "id": "f3eeb07c-5de3-428e-a422-3a58f1124b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: torch in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (0.17.2)\n",
      "Requirement already satisfied: tqdm in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (4.66.2)\n",
      "Requirement already satisfied: ipysankeywidget in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (0.5.0)\n",
      "Requirement already satisfied: ipywidgets in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (8.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (6.1.2)\n",
      "Requirement already satisfied: filelock in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: decorator in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.0.9)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown\n",
    "\n",
    "!pip install numpy matplotlib torch torchvision tqdm ipysankeywidget ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56685b9-3f17-4a55-acca-73dad5623992",
   "metadata": {
    "id": "b56685b9-3f17-4a55-acca-73dad5623992"
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebc5798-a8dd-4985-b0ec-d5facf4ee700",
   "metadata": {
    "id": "cebc5798-a8dd-4985-b0ec-d5facf4ee700"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perform high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "598cc45f-c4b7-406c-a80b-6adfae387583",
   "metadata": {
    "id": "598cc45f-c4b7-406c-a80b-6adfae387583"
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "# @markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386b49a0-9409-496a-8656-579ca3e4af5f",
   "metadata": {
    "id": "386b49a0-9409-496a-8656-579ca3e4af5f"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "# @markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pIBjtdIDeeVg",
   "metadata": {
    "id": "pIBjtdIDeeVg"
   },
   "source": [
    "## Overview of the tutorial\n",
    "\n",
    "To begin, we will start by importing all the necessary packages that we'll need throughout our session. This initial step ensures that all the tools and functions required for our computations are readily available.\n",
    "\n",
    "### Speeding up training and inference.\n",
    "\n",
    "Next, let's discuss the 'allow tf32' settings in PyTorch, which are designed to enhance computational speed at the expense of precision. This setting is particularly relevant for operations involving tensors. PyTorch automatically enables this for CuDNN-based convolution operations to accelerate processing times. However, it's important to note that for matrix multiplication tasks, this option remains disabled by default to maintain higher numerical precision.\n",
    "\n",
    "### Analysis of the results\n",
    "\n",
    "As we move forward, we'll employ PCA (Principal Component Analysis) and t-SNE (t-Distributed Stochastic Neighbor Embedding) as our primary tools for visualizing data. These techniques are instrumental in reducing the dimensionality of the data, allowing us to observe patterns and relationships that are otherwise difficult to discern in high-dimensional spaces. By visualizing data in this way, we can gain insightful perspectives that are crucial for understanding complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb74907a-f462-4acf-835d-340a899a3663",
   "metadata": {
    "id": "bb74907a-f462-4acf-835d-340a899a3663"
   },
   "source": [
    "### What is contrastive learning?\n",
    "\n",
    "Contrastive learning is often referred to as \"self-supervised learning (SSL)\" and has historically been known as \"metric learning.\" The essence of contrastive/metric learning is that instead of outputting a classification one-hot/softmax vector, or a regression value, you directly output a high-dimensional embedding.\n",
    "\n",
    "Here is an example: Given multiple data points from a single class (for example, three photos of you from different viewpoints) and different classes (for example, 10 photos from one or multiple people who are not you), you want the three embeddings from your photos to be closer to each other while being farther away from the ten embeddings from the different classes.\n",
    "\n",
    "Hence the name \"metric learning,\" where you seek to learn a metric/distance that fits the constraints of the data.\n",
    "\n",
    "\n",
    "### Why contrastive learning?\n",
    "\n",
    "It may not be immediately obvious why you would want to engage in contrastive/metric learning. Can't you just use a giant 1000-class ImageNet-trained classifier and recognize every image? However, metric learning proves useful when the number of classes is not known ahead of time. For example, if I wanted a network to recognize human faces, there are 7 billion people on this planet, which makes it impossible for you to train a classification network with 7 billion output neurons. Nevertheless, I can train a network that outputs a high-dimensional embedding for each image. Now, given a reference image of a person, your network can decide if the new photo is close to or farther away from the reference image.\n",
    "\n",
    "\n",
    "### Terms defined and other stuff\n",
    "\n",
    "* Positive pair—This refers to two data points that should be close together in embedding space. For example, two photos of you in different lighting conditions.\n",
    "* Negative pair—This refers to two data points that should be far apart in embedding space. For example, a photo of you versus a photo of a dog (assuming you are not a dog). Note that positive pairs/negative pairs don't have to be images. You could have a picture and the matching text be a positive pair as well. Recent work has also moved to positive pairs defined using an older version of the encoder (Google Momentum contrast or EMA contrastive).\n",
    "* Pretext task—In computer vision, this refers to how you augment images to get positive pairs.\n",
    "Hard positive/negative mining—This refers to a practice where positive/negative pairs where the network struggles are used to train the network with more loss in some way.\n",
    "* InfoNCE—This is one of the most common contrastive losses [1,2,3,4], which was proposed in similar ways multiple times by different authors. It is a cross-entropy loss of classifying the correct positive pair out from a pool of pairs. Note there are variants like MIL-NCE, which allow for multiple positive pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AJoELbcmeO_n",
   "metadata": {
    "id": "AJoELbcmeO_n"
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "First, we'll start by outlining the network blocks that we plan to incorporate into our model. This involves defining each component as a class in PyTorch, which requires inheriting from the torch.nn.Module class. It's crucial to remember that after defining your class, you must initialize it properly by calling super().__init__(). This step is essential as it ensures that all network sub-modules are registered correctly. Additionally, PyTorch provides several useful functions such as ModuleList, register_parameter, register_module, and register_buffer to help manage these components effectively.\n",
    "\n",
    "### Mini residual block\n",
    "\n",
    "Our initial focus will be on creating a mini_residual block. This block adopts a modern approach to the residual design, featuring a prenormalization step as suggested by Kaiming He. We will also incorporate the LeakyReLU activation function. LeakyReLU is particularly favored in generative adversarial networks (GANs) due to its ability to maintain non-zero gradients, which helps in the training process by avoiding the vanishing gradient problem.\n",
    "\n",
    "### Full model construction\n",
    "\n",
    "Following the mini_residual block, we will construct the full model. This model will consist of a series of residual blocks stacked together. In PyTorch, the components of a model are organized in a sequence using nn.Sequential, which executes the blocks from the first to the last. This sequential arrangement simplifies the process of defining forward pass operations, ensuring that data flows through the blocks in the intended order. By stacking these blocks, the model can learn complex patterns from the data, enhancing its predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c432bf-1de5-43f6-b887-087d17e9ec0f",
   "metadata": {
    "id": "d7c432bf-1de5-43f6-b887-087d17e9ec0f"
   },
   "outputs": [],
   "source": [
    "class mini_residual(nn.Module):\n",
    "    # Follows \"Identity Mappings in Deep Residual Networks\", uses layernorm instead of batchnorm, uses leakyReLU instead of ReLU\n",
    "    def __init__(self, feat_in=128, feat_out=128, feat_hidden=256, use_norm=True):\n",
    "        super().__init__()\n",
    "        if use_norm:\n",
    "            self.block = nn.Sequential(nn.LayerNorm(feat_in), nn.LeakyReLU(negative_slope=0.1),\n",
    "                                      nn.Linear(feat_in, feat_hidden), nn.LayerNorm(feat_hidden),\n",
    "                                      nn.LeakyReLU(negative_slope=0.1), nn.Linear(feat_hidden, feat_out))\n",
    "        else:\n",
    "            self.block = nn.Sequential(nn.LeakyReLU(negative_slope=0.1),nn.Linear(feat_in, feat_hidden),\n",
    "                                      nn.LeakyReLU(negative_slope=0.1), nn.Linear(feat_hidden, feat_out))\n",
    "        if feat_in!=feat_out:\n",
    "            self.bypass = nn.Linear(feat_in, feat_out)\n",
    "        else:\n",
    "            self.bypass = nn.Identity()\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        return self.block(input_data) + self.bypass(input_data)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim, num_blocks=4):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(in_dim, hidden_dim)\n",
    "        self.hidden = nn.Sequential(*[mini_residual(feat_in=hidden_dim, feat_out=hidden_dim, feat_hidden=hidden_dim) for i in range(num_blocks)])\n",
    "        self.out = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_proj_out = self.in_proj(x)\n",
    "        hidden_out = self.hidden(in_proj_out)\n",
    "        return self.out(hidden_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C29eZ1BkfuIr",
   "metadata": {
    "id": "C29eZ1BkfuIr"
   },
   "source": [
    "Now, let's move on to defining the loss function for our model, using an approach extracted from the PyTorch metric learning package for better clarity. We will be implementing a variant of the InfoNCE loss function, which is widely recognized as one of the most effective contrastive or metric learning losses. It has been prominently used in various models, including OpenAI's CLIP, due to its ability to enhance feature discrimination by contrasting positive pairs against negative pairs.\n",
    "\n",
    "InfoNCE typically requires substantial batch sizes—commonly 128 or even larger—to perform optimally. This requirement is due to the need for diverse negative samples in the batch to effectively learn the contrasts. However, large batch sizes can be impractical in resource-constrained settings or when data availability is limited.\n",
    "\n",
    "To address this, we will implement a modified version of InfoNCE as described in the \"Decoupled Contrastive Learning\" paper. This variant adapts the loss to be more suitable for smaller batch sizes by modifying the denominator of the InfoNCE formula. Specifically, it removes the positive example from the denominator, which reduces the computational demand and stabilizes training when fewer examples are available. This adjustment not only makes the loss function more flexible but also maintains robustness in learning discriminative features even with smaller batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f70265-2738-4ec3-b07f-b1d06507cc07",
   "metadata": {
    "id": "09f70265-2738-4ec3-b07f-b1d06507cc07"
   },
   "outputs": [],
   "source": [
    "# This is code from the pytorch metric learning package\n",
    "# Extracted out so it is clear what it is doing\n",
    "\n",
    "def neg_inf(dtype):\n",
    "    return torch.finfo(dtype).min\n",
    "\n",
    "\n",
    "def small_val(dtype):\n",
    "    return torch.finfo(dtype).tiny\n",
    "\n",
    "def to_dtype(x, tensor=None, dtype=None):\n",
    "    if not torch.is_autocast_enabled():\n",
    "        dt = dtype if dtype is not None else tensor.dtype\n",
    "        if x.dtype != dt:\n",
    "            x = x.type(dt)\n",
    "    return x\n",
    "\n",
    "def get_matches_and_diffs(labels, ref_labels=None):\n",
    "    if ref_labels is None:\n",
    "        ref_labels = labels\n",
    "    labels1 = labels.unsqueeze(1)\n",
    "    labels2 = ref_labels.unsqueeze(0)\n",
    "    matches = (labels1 == labels2).byte()\n",
    "    diffs = matches ^ 1\n",
    "    if ref_labels is labels:\n",
    "        matches.fill_diagonal_(0)\n",
    "    return matches, diffs\n",
    "\n",
    "def get_all_pairs_indices(labels, ref_labels=None):\n",
    "    \"\"\"\n",
    "    Given a tensor of labels, this will return 4 tensors.\n",
    "    The first 2 tensors are the indices which form all positive pairs\n",
    "    The second 2 tensors are the indices which form all negative pairs\n",
    "    \"\"\"\n",
    "    matches, diffs = get_matches_and_diffs(labels, ref_labels)\n",
    "    a1_idx, p_idx = torch.where(matches)\n",
    "    a2_idx, n_idx = torch.where(diffs)\n",
    "    return a1_idx, p_idx, a2_idx, n_idx\n",
    "\n",
    "def cos_sim(input_embeddings):\n",
    "    # batch, dim\n",
    "    normed_embeddings = torch.nn.functional.normalize(input_embeddings, dim=-1)\n",
    "    return normed_embeddings@normed_embeddings.t()\n",
    "\n",
    "def dcl_loss(pos_pairs, neg_pairs, indices_tuple,temperature=0.07):\n",
    "    # This is the modified InfoNCE loss called \"Decoupled Contrastive Learning\" for small batch sizes\n",
    "    # Basically You remove the numerator from the sum to the denominator\n",
    "\n",
    "    a1, p, a2, _ = indices_tuple\n",
    "\n",
    "    if len(a1) > 0 and len(a2) > 0:\n",
    "        dtype = neg_pairs.dtype\n",
    "        pos_pairs = pos_pairs.unsqueeze(1) / temperature\n",
    "        neg_pairs = neg_pairs / temperature\n",
    "        n_per_p = to_dtype(a2.unsqueeze(0) == a1.unsqueeze(1), dtype=dtype)\n",
    "        neg_pairs = neg_pairs * n_per_p\n",
    "        neg_pairs[n_per_p == 0] = neg_inf(dtype)\n",
    "\n",
    "        max_val = torch.max(\n",
    "            pos_pairs, torch.max(neg_pairs, dim=1, keepdim=True)[0]\n",
    "        ).detach()\n",
    "        numerator = torch.exp(pos_pairs - max_val).squeeze(1)\n",
    "        denominator = torch.sum(torch.exp(neg_pairs - max_val), dim=1)\n",
    "        log_exp = torch.log((numerator / denominator) + small_val(dtype))\n",
    "        return -log_exp\n",
    "    return 0\n",
    "\n",
    "def pair_based_loss(mat, indices_tuple, lossfunc):\n",
    "    a1, p, a2, n = indices_tuple\n",
    "    pos_pair, neg_pair = [], []\n",
    "    if len(a1) > 0:\n",
    "        pos_pair = mat[a1, p]\n",
    "    if len(a2) > 0:\n",
    "        neg_pair = mat[a2, n]\n",
    "    return lossfunc(pos_pair, neg_pair, indices_tuple)\n",
    "\n",
    "# dummy_labels = torch.from_numpy(np.array([1,1,2,3,2,4]))\n",
    "# demo_matches, demo_diffs = get_matches_and_diffs(labels=dummy_labels)\n",
    "# results = get_all_pairs_indices(labels=dummy_labels)\n",
    "# final_loss = pair_based_loss(torch.randn(6,6),results, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LAajOxFCgYFK",
   "metadata": {
    "id": "LAajOxFCgYFK"
   },
   "source": [
    "Now we will make the Pytorch dataset object, this defines how data is loaded from disk for each batch, and what transform you want to apply. Note that you do not have to use torchvision transforms. It is very common to write your own transform code in the dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d72d9231-35a5-405e-b2f6-135e134e0ae9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965
    },
    "id": "d72d9231-35a5-405e-b2f6-135e134e0ae9",
    "outputId": "d67b7409-5a40-40c7-992d-494ee7919c96"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 575,
       "width": 580
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "mnist_transforms=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "test_dset = torchvision.datasets.MNIST(\"./\", train=False, transform=mnist_transforms, download=True)\n",
    "\n",
    "\n",
    "height = int(784**0.5)\n",
    "width = height\n",
    "idx = 0\n",
    "data_point = test_dset[idx]\n",
    "plt.imshow(data_point[0][0].numpy())\n",
    "plt.show()\n",
    "print(data_point[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CkhXcVrGhUM5",
   "metadata": {
    "id": "CkhXcVrGhUM5"
   },
   "source": [
    "Now we will make the model using the definition we wrote previously. And we will move it to the device you want. Note that in pytorch, calling `.to(device)` on a module acts on the module itself, as in it is an inplace operation. However for pytorch tensors directly (if you don't call this function on a module) it is not inplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd3ab89-ffd9-408c-8396-25b88e0930fb",
   "metadata": {
    "id": "ffd3ab89-ffd9-408c-8396-25b88e0930fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "mynet = Model(in_dim=784, out_dim=128, hidden_dim=256)\n",
    "\n",
    "# Automatically select the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Output the device that will be used\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "_ = mynet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pi1Seq52ho57",
   "metadata": {
    "id": "pi1Seq52ho57"
   },
   "source": [
    "Let us create a test dataloader, and see what the untrained network gives us in terms of representations for each number. We will compute the cosine similarity for each written character WITHIN the same class (we set the diagonal to np.nan to avoid comparing a written character to itself).\n",
    "\n",
    "We will also compute the cosine similarity for each written character across the classes.\n",
    "\n",
    "You should have a habit of calling `network.eval()` before evaluating a network, this is also an inplace operation. This will tell pytorch to freeze some buffers (like in batchnorm) and disable dropout.\n",
    "\n",
    "We use `torch.inference_mode()` here, this disables gradient computation and speeds up the testing process. However this may break some features, if it fails you can replace it with `torch.no_grad()`. Note that `inference_mode` does not automatically enable `eval`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9122b-ce83-464f-b6c9-5af2feceac62",
   "metadata": {
    "id": "b2f9122b-ce83-464f-b6c9-5af2feceac62"
   },
   "outputs": [],
   "source": [
    "# First try with untrained network, find the cosine similarities within a class and across classes\n",
    "test_loader = DataLoader(test_dset, batch_size=50, shuffle=False) # enable persistent_workers=True if more than 1 worker to save CPU\n",
    "mynet.eval()\n",
    "test_embeddings = []\n",
    "test_labels = []\n",
    "sim_matrix = np.zeros((10,10))\n",
    "with torch.inference_mode():\n",
    "    for data_batch in test_loader:\n",
    "        test_img, test_label = data_batch\n",
    "        batch_size = test_img.shape[0]\n",
    "        flat = test_img.reshape(batch_size,-1).to(device, non_blocking=True)\n",
    "        pred_embeddings = mynet(flat).cpu().numpy().tolist()\n",
    "        test_embeddings.extend(pred_embeddings)\n",
    "        test_labels.extend(test_label.numpy().tolist())\n",
    "\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "test_embeddings_normed = test_embeddings/np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "test_labels = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GtbiyBRmi-e2",
   "metadata": {
    "id": "GtbiyBRmi-e2"
   },
   "source": [
    "### Visualizing the cosine similarity of embeddings within the same class and across different classes before training\n",
    "\n",
    "Ideally, you should see a very high cosine similarity for images within the same class (the diagonal), but very low cosine similarity for images not within the same class (the non-diagonal).\n",
    "\n",
    "But since our network is untrained, *you* will see there is not much difference, this is expected as the network is untrained. If you look at the plot, there is no clear structure to the similarities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0JcR1VwliL1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0JcR1VwliL1f",
    "outputId": "6e666e37-74f8-4757-a303-5b7cfd44c5e3"
   },
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "for i in [0,1,2,3,4,5,6,7,8,9]:\n",
    "    embeddings[i] = test_embeddings_normed[test_labels==i]\n",
    "\n",
    "# Within class cosine similarity:\n",
    "for i in [0,1,2,3,4,5,6,7,8,9]:\n",
    "    sims = embeddings[i]@embeddings[i].T\n",
    "    np.fill_diagonal(sims, np.nan)\n",
    "    cur_sim = np.nanmean(sims)\n",
    "    sim_matrix[i,i] =  cur_sim\n",
    "\n",
    "    print(\"Within class {} cosine similarity\".format(cur_sim))\n",
    "\n",
    "print(\"==================\")\n",
    "for i in [0,1,2,3,4,5,6,7,8,9]:\n",
    "    for j in [0,1,2,3,4,5,6,7,8,9]:\n",
    "        if i==j:\n",
    "            pass\n",
    "        elif i>j:\n",
    "            pass\n",
    "        else:\n",
    "            sims = embeddings[i]@embeddings[j].T\n",
    "            cur_sim = np.mean(sims)\n",
    "            sim_matrix[i,j] =  cur_sim\n",
    "            sim_matrix[j,i] =  cur_sim\n",
    "            print(\"{} and {} cosine similarity {}\".format(i,j, np.nanmean(sims)))\n",
    "\n",
    "plt.imshow(sim_matrix, vmin=0.0, vmax=1.0)\n",
    "plt.title(\"untrained network\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iAScidA3iXpZ",
   "metadata": {
    "id": "iAScidA3iXpZ"
   },
   "source": [
    "Now we will train the network!\n",
    "\n",
    "Note how we decay the learning rate, so the final learning rate will be half that of the inital learning rate. AdamW is the Adam optimizer with decoupled weight decay. A learning rate of 3e-4 and a weight decay of 1e-2 in AdamW are pretty typical. Note that weight decay in AdamW and SGD work differently in the pytorch implementations. In pytorch, the adamw weight decay is further scaled by learning rate (real weight decay = weight decay * lr) but in SGD, it is not scaled by learning rate. So in AdamW, it is common to use higher weight decay values than SGD.\n",
    "\n",
    "Also note how we call `mynet.train()` before we start training. That sets mynet to training mode, and enables the buffers and dropout layers (if they were present in the network architecture.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b62ecc-c0c4-4b2e-8d93-ab4ea9a952d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5b62ecc-c0c4-4b2e-8d93-ab4ea9a952d0",
    "outputId": "4624457d-2133-4a26-8147-ca70d19cf1b4"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "# Automatically select the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Output the device that will be used\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_dset = torchvision.datasets.MNIST(\"./\", train=True, transform=mnist_transforms)\n",
    "train_loader = DataLoader(train_dset, batch_size=50, shuffle=True) # enable persistent_workers=True if more than 1 worker to save CPU\n",
    "\n",
    "try:\n",
    "    del optimizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # Nuke the optimizer from memory if students try re-running this block\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del mynet\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # Nuke the network from memory if students try re-running this block\n",
    "except:\n",
    "    pass\n",
    "\n",
    "mynet = Model(in_dim=784, out_dim=128, hidden_dim=256)\n",
    "_ = mynet.to(device)\n",
    "\n",
    "# This enables training mode, which may affect dropout and stuff\n",
    "mynet.train(mode=True)\n",
    "print(\"Is the network in training mode?\", mynet.training)\n",
    "init_lr = 3e-4\n",
    "lr_decay_factor = 0.5\n",
    "optimizer = torch.optim.AdamW(mynet.parameters(), lr=init_lr, weight_decay=1e-2)\n",
    "\n",
    "loss_tracker = []\n",
    "for epoch_id in range(1, epochs+1):\n",
    "    loss_epoch_tracker = 0\n",
    "    batch_counter = 0\n",
    "    # decay lr to half that of initial by the end of training\n",
    "    new_lrate = init_lr * (lr_decay_factor ** (epoch_id / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lrate\n",
    "\n",
    "    batches_in_epoch = len(train_loader)\n",
    "    for data_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        train_img, train_label = data_batch\n",
    "        batch_size = train_img.shape[0]\n",
    "        flat = train_img.reshape(batch_size,-1).to(device, non_blocking=True)\n",
    "        train_label = train_label.to(device, non_blocking=True)\n",
    "        predicted_results = mynet(flat)\n",
    "        # Now to compute loss\n",
    "        similarities = cos_sim(predicted_results)\n",
    "        label_pos_neg = get_all_pairs_indices(train_label)\n",
    "        final_loss = torch.mean(pair_based_loss(similarities, label_pos_neg, dcl_loss))\n",
    "\n",
    "        # Compute gradients from the loss to the parameters that require a gradient\n",
    "        final_loss.backward()\n",
    "\n",
    "        # Now we use the optimizer and the gradients to change the original parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # .item() converts the CUDA tensor to a single CPU scalar.\n",
    "        # Note this only works on tensors with a single value\n",
    "        # Avoid calling .item() too frequently, as it involves a GPU -> CPU transfer it is slow!\n",
    "        loss_cpu_number = final_loss.item()\n",
    "\n",
    "        # We keep track of the losses here\n",
    "        # This is just for human visualization, doesn't really affect training\n",
    "        loss_epoch_tracker+=loss_cpu_number\n",
    "        batch_counter +=1\n",
    "\n",
    "        # Every 500 batches, we print the current epoch, batches seen for the current epoch, and the current batch loss\n",
    "        if batch_counter%500 == 0:\n",
    "            print(\"Epoch {}, Batch {}/{}, loss: {}\".format(epoch_id, batch_counter, batches_in_epoch, loss_cpu_number))\n",
    "\n",
    "    # Every epoch we print out the average loss over the epoch\n",
    "    print(\"Epoch average loss {}\".format(loss_epoch_tracker/batch_counter))\n",
    "# Test mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uvQ4MdHKjY1t",
   "metadata": {
    "id": "uvQ4MdHKjY1t"
   },
   "source": [
    "Let us now extract the features from the trained network!\n",
    "\n",
    "Again, please make it a habit to set the network into eval mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f11e3-07e8-4104-9ae4-6a9fd6f037eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d4f11e3-07e8-4104-9ae4-6a9fd6f037eb",
    "outputId": "3831ef0a-7e30-4b2e-fbf8-c1ee50788ba4"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dset, batch_size=50, shuffle=False) # enable persistent_workers=True if more than 1 worker to save CPU\n",
    "mynet.eval()\n",
    "test_embeddings = []\n",
    "test_labels = []\n",
    "with torch.inference_mode():\n",
    "    for data_batch in test_loader:\n",
    "        test_img, test_label = data_batch\n",
    "        batch_size = test_img.shape[0]\n",
    "        flat = test_img.reshape(batch_size,-1).to(device, non_blocking=True)\n",
    "        pred_embeddings = mynet(flat).cpu().numpy().tolist()\n",
    "        test_embeddings.extend(pred_embeddings)\n",
    "        test_labels.extend(test_label.numpy().tolist())\n",
    "test_labels = np.array(test_labels)\n",
    "print(\"Feature extraction done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4GGxdeiWjkIr",
   "metadata": {
    "id": "4GGxdeiWjkIr"
   },
   "source": [
    "As the network was trained using infoNCE, we will normalize each feature to unit norm. PCA further expects the features to be centered and 1 std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e0f93-3f68-4db0-9edf-4891a3858f63",
   "metadata": {
    "id": "767e0f93-3f68-4db0-9edf-4891a3858f63"
   },
   "outputs": [],
   "source": [
    "test_embeddings = np.array(test_embeddings)\n",
    "test_embeddings_normed = test_embeddings/np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "test_embeddings_normed = test_embeddings_normed-np.mean(test_embeddings_normed, axis=1, keepdims=True)\n",
    "test_embeddings_normed = test_embeddings_normed/np.std(test_embeddings_normed, axis=1, keepdims=True)\n",
    "pca = PCA(n_components=2)\n",
    "pca_embeddings = pca.fit_transform(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8UXxSAmfjoB8",
   "metadata": {
    "id": "8UXxSAmfjoB8"
   },
   "source": [
    "For TSNE, we just normalize each feature to unit norm due to infoNCE. We will not further center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cdd26a-15c9-4c5f-82f7-4a65ca0bd5f7",
   "metadata": {
    "id": "d4cdd26a-15c9-4c5f-82f7-4a65ca0bd5f7"
   },
   "outputs": [],
   "source": [
    "test_embeddings = np.array(test_embeddings)\n",
    "test_embeddings_normed = test_embeddings/np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_embeddings = tsne.fit_transform(test_embeddings)\n",
    "# This takes like a minute, go grab a coffee or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de073f-d514-4156-98e2-d7aac5620ca6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8de073f-d514-4156-98e2-d7aac5620ca6",
    "outputId": "1e832641-36de-485c-d801-00d20cbe09bb"
   },
   "outputs": [],
   "source": [
    "test_labels.shape, tsne_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eqJJWmx7j1kF",
   "metadata": {
    "id": "eqJJWmx7j1kF"
   },
   "source": [
    "Look at how the features for each number are distributed! Note how well separated all the embeddings for different characters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe3c0d-7eed-4b2b-bdaa-bb258fc92d10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "2afe3c0d-7eed-4b2b-bdaa-bb258fc92d10",
    "outputId": "9c3b8d21-6799-4e6d-e367-046f0d0f26d6"
   },
   "outputs": [],
   "source": [
    "my_embeddings = tsne_embeddings\n",
    "# TSNE or PCA? TSNE is nicer to look at.\n",
    "\n",
    "num = 0\n",
    "plt.scatter(my_embeddings[test_labels==num, 0], my_embeddings[test_labels==num, 1], c=\"red\")\n",
    "num = 1\n",
    "plt.scatter(my_embeddings[test_labels==num, 0], my_embeddings[test_labels==num, 1], c=\"green\")\n",
    "num = 2\n",
    "plt.scatter(my_embeddings[test_labels==num, 0], my_embeddings[test_labels==num, 1], c=\"blue\")\n",
    "num = 3\n",
    "plt.scatter(my_embeddings[test_labels==num, 0], my_embeddings[test_labels==num, 1], c=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0KXB1wpmMp1",
   "metadata": {
    "id": "d0KXB1wpmMp1"
   },
   "source": [
    "### Visualizing the cosine similarity AFTER training\n",
    "\n",
    "Note now that diagonal is strongly more positive than the off-diagonal, this means within the same class, the similarity is much stronger than outside a class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a8b40-faa7-4071-b070-0ca192e06d55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e88a8b40-faa7-4071-b070-0ca192e06d55",
    "outputId": "f5ae1a4e-106c-46fc-ab7f-c2a91d042e1f"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dset, batch_size=50, shuffle=False) # enable persistent_workers=True if more than 1 worker to save CPU\n",
    "mynet.eval()\n",
    "test_embeddings = []\n",
    "test_labels = []\n",
    "sim_matrix = np.zeros((10,10))\n",
    "with torch.inference_mode():\n",
    "    for data_batch in test_loader:\n",
    "        test_img, test_label = data_batch\n",
    "        batch_size = test_img.shape[0]\n",
    "        flat = test_img.reshape(batch_size,-1).to(device, non_blocking=True)\n",
    "        pred_embeddings = mynet(flat).cpu().numpy().tolist()\n",
    "        test_embeddings.extend(pred_embeddings)\n",
    "        test_labels.extend(test_label.numpy().tolist())\n",
    "\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "test_embeddings_normed = test_embeddings/np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "test_labels = np.array(test_labels)\n",
    "embeddings = {}\n",
    "for i in [0,1,2,3,4,5,6,7,8,9]:\n",
    "    embeddings[i] =test_embeddings_normed[test_labels==i]\n",
    "\n",
    "\n",
    "\n",
    "# Within class cosine similarity:\n",
    "for i in [0,1,2,3,4,5,6,7,8,9]:\n",
    "    sims = embeddings[i]@embeddings[i].T\n",
    "    np.fill_diagonal(sims, np.nan)\n",
    "    cur_sim = np.nanmean(sims)\n",
    "    sim_matrix[i,i] =  cur_sim\n",
    "\n",
    "    print(\"Within class {} cosine similarity\".format(cur_sim))\n",
    "\n",
    "print(\"==================\")\n",
    "for i in [0,1,2,3,4,5,6,7,8,9]:\n",
    "    for j in [0,1,2,3,4,5,6,7,8,9]:\n",
    "        if i==j:\n",
    "            pass\n",
    "        elif i>j:\n",
    "            pass\n",
    "        else:\n",
    "            sims = embeddings[i]@embeddings[j].T\n",
    "            cur_sim = np.mean(sims)\n",
    "            sim_matrix[i,j] =  cur_sim\n",
    "            sim_matrix[j,i] =  cur_sim\n",
    "            print(\"{} and {} cosine similarity {}\".format(i,j, np.nanmean(sims)))\n",
    "plt.imshow(sim_matrix, vmin=0.0, vmax=1.0)\n",
    "plt.title(\"trained network\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8wRjpsrmva7",
   "metadata": {
    "id": "a8wRjpsrmva7"
   },
   "source": [
    "### Using the network to identify nearest neighbors in the test set.\n",
    "\n",
    "But how do people actually use a contrastive learned network? In Person Re-ID (reidentification), you will use a network to compute the embeddings for two images, and check if the embeddings have cosine/euclidean similarity above some threshold to decide if they are the same person.\n",
    "\n",
    "In foundation model training (CLIP), people typically fine tune the entire network or train a linear probe or small network on the last layer outputs.\n",
    "\n",
    "Here, we will follow the person Re-ID setup, and try to find the most similar image in a test set and decide if they represent the same character!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013D0NbXowjj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "013D0NbXowjj",
    "outputId": "dbfa188c-d3bd-4d0c-9589-18e681356e4f"
   },
   "outputs": [],
   "source": [
    "sims_all = test_embeddings_normed@test_embeddings_normed.T\n",
    "np.fill_diagonal(sims_all, -1000.0)\n",
    "# Set to a small value so it doesn't give us the same number for argmax\n",
    "\n",
    "idx_to_check = 3029\n",
    "best_idx = np.argmax(sims_all[idx_to_check])\n",
    "\n",
    "plt.imshow(test_dset[idx_to_check][0][0].cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(test_dset[best_idx][0][0].cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444cc4a-fd26-471c-ac34-7e515dbe946a",
   "metadata": {
    "id": "8444cc4a-fd26-471c-ac34-7e515dbe946a"
   },
   "source": [
    "### How is contrastive learning used in practice?\n",
    "\n",
    "Nearly all vision foundation models (DINO and DINOv2, CLIP and all CLIP derivatives including OpenCLIP/EVA-CLIP) are trained using contrastive losses. DINO/v2 is trained on images alone, while CLIP is trained on a combination of images and text.\n",
    "\n",
    "When only images are used, the contrastive learning loss is applied to augmentations of the same image. Examples include crops/flips/rotations of images -- when used in the way, the augmentations are called a \"pretext task\". Typically, augmentations of the same image are treated as instances where the embeddings should be the same. So for example, a network should recognize a photo of you, and a photo of you flipped (or brightness changed, or noise added, or changed to black and white) as a photo of the same person.\n",
    "\n",
    "When images and text are used together as in CLIP, you have images and captions of those images -- for example the caption \"A photo of a dog\" may be matched to a picture of a blue heeler puppy. These captions are typicalled scraped from online sources, and collected into datasets like LAION-2B/COYO-700M/CommonCrawl. While these captions are typically not high quality, having billions of them do seem to mitigate this issue. When contrastive learning is used in this case, typically a dual encoder is used -- one for text, and one for the image. The network is trained using a loss which minimizes the distance between the correct text and image pair, while distances between incorrect text and image pairs are maximized. For example -- the caption \"A photo of a dog\" and the picture of the blue heeler puppy should have embeddings that are close, while the caption should be far away from a picture of a cat. Often times, normalized dot-product (cosine similarity), angular distance (Universal Sentence Encoder) euclidean distance, or squared euclidean distance are applied to compute the \"distance\" of the embeddings.\n",
    "\n",
    "\n",
    "Referencess:\n",
    "\n",
    "[1] Unsupervised feature learning via non-parametric\n",
    "instance discrimination (2018)\n",
    "\n",
    "[2] Representation learning with contrastive predictive coding (2018)\n",
    "\n",
    "[3] A simple framework for contrastive learning of visual representations (2020)\n",
    "\n",
    "[4] Improved Deep Metric Learning with Multi-class N-pair Loss Objective (2016)\n",
    "\n",
    "[4] Noise-contrastive estimation: A new estimation principle for unnormalize\n",
    "statistical models (2010)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
