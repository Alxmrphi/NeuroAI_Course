{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ed61a3-87d2-4e76-83f6-4b786c101af2",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# W2D5Tutorial1\n",
    "\n",
    "**Week 2, Day 5: Mysteries**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Names & Surnames\n",
    "\n",
    "__Content reviewers:__ Names & Surnames\n",
    "\n",
    "__Production editors:__ Names & Surnames\n",
    "\n",
    "<br>\n",
    "\n",
    "Acknowledgments: [ACKNOWLEDGMENT_INFORMATION]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366c740-80c7-4545-b688-d547ef79613e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: [insert estimated duration of whole tutorial in minutes]*\n",
    "\n",
    "In this tutorial, you will observe how performance degrades as testing data distribution strays from training distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4fa9f7-d9cc-4594-be41-b31f02af1bd4",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to download the slides: 'Link to the slides'\n"
     ]
    }
   ],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "\n",
    "## Uncomment the code below to test your function\n",
    "\n",
    "#from IPython.display import IFrame\n",
    "#link_id = \"<YOUR_LINK_ID_HERE>\"\n",
    "\n",
    "print(\"If you want to download the slides: 'Link to the slides'\")\n",
    "      # Example: https://osf.io/download/{link_id}/\n",
    "\n",
    "#IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d1a7f-452c-436d-90a8-409571405e62",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4762c382-3622-4178-8e19-da783bac0a57",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: Pillow in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (10.2.0)\n",
      "Requirement already satisfied: torch in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: torchvision in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: transformers in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (4.38.1)\n",
      "Requirement already satisfied: ipywidgets in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (8.1.2)\n",
      "Requirement already satisfied: gradio in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (4.19.2)\n",
      "Requirement already satisfied: trdg in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (1.8.0)\n",
      "Requirement already satisfied: scikit-learn in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: networkx in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (3.2.1)\n",
      "Requirement already satisfied: pickleshare in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (0.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from matplotlib) (6.1.2)\n",
      "Requirement already satisfied: filelock in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: jinja2 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (5.2.0)\n",
      "Requirement already satisfied: fastapi in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (0.110.0)\n",
      "Requirement already satisfied: ffmpy in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==0.10.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (0.10.1)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (3.9.15)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (2.2.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (2.6.3)\n",
      "Requirement already satisfied: pydub in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (0.2.2)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.9.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio) (0.27.1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from gradio-client==0.10.1->gradio) (11.0.3)\n",
      "Requirement already satisfied: opencv-python>=4.2.0.32 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from trdg) (4.9.0.80)\n",
      "Requirement already satisfied: wikipedia>=1.4.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from trdg) (1.4.0)\n",
      "Requirement already satisfied: diffimg==0.2.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from trdg) (0.2.3)\n",
      "Requirement already satisfied: arabic-reshaper==2.1.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from trdg) (2.1.3)\n",
      "Requirement already satisfied: python-bidi==0.4.2 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from trdg) (0.4.2)\n",
      "Requirement already satisfied: future in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from arabic-reshaper==2.1.3->trdg) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from arabic-reshaper==2.1.3->trdg) (69.1.1)\n",
      "Requirement already satisfied: six in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from python-bidi==0.4.2->trdg) (1.16.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio) (4.21.1)\n",
      "Requirement already satisfied: toolz in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: anyio in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from httpx>=0.24.1->gradio) (4.3.0)\n",
      "Requirement already satisfied: certifi in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from httpx>=0.24.1->gradio) (1.0.4)\n",
      "Requirement already satisfied: idna in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from httpx>=0.24.1->gradio) (3.6)\n",
      "Requirement already satisfied: sniffio in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: decorator in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.0.9)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from wikipedia>=1.4.0->trdg) (4.12.3)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from fastapi->gradio) (0.36.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.33.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from beautifulsoup4->wikipedia>=1.4.0->trdg) (2.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/samuele/virtualenvs/neuroaienv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown\n",
    "\n",
    "!pip install numpy matplotlib Pillow torch torchvision transformers ipywidgets gradio trdg scikit-learn networkx pickleshare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "299b676d-d8de-41ad-80c6-3516e25f0fba",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown\n",
    "\n",
    "# Standard libraries for basic operations and file handling\n",
    "import random\n",
    "import pickleshare\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Image processing libraries for handling and manipulating image data\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Deep learning libraries for model building, training, and evaluation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Utility libraries for creating interactive elements and interfaces\n",
    "import ipywidgets as widgets\n",
    "import gradio as gr\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Libraries for graph analysis\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0960e64d-fe33-4276-8da0-e450e2649bcc",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734a181-77f8-4e19-8674-10904df84761",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 1: Recurrent Independent Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b2098-9d42-4060-a470-fad0d748faac",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The crucial idea behind this section is that machine learning aims to capture the modular structure of the physical world, where complexity emerges from simpler, independently evolving subsystems. This concept aligns with causal inference, suggesting that understanding and modeling the world involves identifying and integrating these autonomous mechanisms. These mechanisms, which interact sparsely, maintain their functionality even amidst changes in others, highlighting their robustness. Recurrent Independent Mechanisms (RIMs) embody this principle by operating mostly independently, occasionally interacting through an attention-based mechanism for efficient and dynamic information processing. This approach suggests a preference for models that can capture the independence and sparse interactions of mechanisms, potentially leading to more adaptable and generalizable AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5851e83-da49-48b7-94f3-832f366424f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'RIMs-Sequential-MNIST' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "## This will take five minutes, as the repository contains a torch model that is quite heavy\n",
    "\n",
    "# URL of the repository to clone\n",
    "!git clone https://github.com/SamueleBolotta/RIMs-Sequential-MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae3e1d3-7be8-460b-ad89-17c96274cb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/samuele/Documenti/GitHub/NeuroAI_Course/tutorials/W2D5_Mysteries/RIMs-Sequential-MNIST\n"
     ]
    }
   ],
   "source": [
    "%cd RIMs-Sequential-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "649857ca-ad57-42de-bb48-756ad6c16a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM model already exists. No download needed.\n",
      "RIM model already exists. No download needed.\n"
     ]
    }
   ],
   "source": [
    "from data import MnistData\n",
    "from networks import MnistModel, LSTM\n",
    "import requests\n",
    "\n",
    "# Function to download files \n",
    "def download_file(url, destination):\n",
    "    print(f\"Starting to download {url} to {destination}\")\n",
    "    response = requests.get(url, allow_redirects=True)\n",
    "    open(destination, 'wb').write(response.content)\n",
    "    print(f\"Successfully downloaded {url} to {destination}\")\n",
    "\n",
    "# Path of the models\n",
    "model_path = {\n",
    "    'LSTM': 'lstm_model_dir/lstm_best_model.pt',\n",
    "    'RIM': 'rim_model_dir/best_model.pt'\n",
    "}\n",
    "\n",
    "import os\n",
    "\n",
    "# URLs of the models\n",
    "model_urls = {\n",
    "    'LSTM': 'https://osf.io/4gajq/download',\n",
    "    'RIM': 'https://osf.io/3squn/download'\n",
    "}\n",
    "\n",
    "# Check if model files exist, if not, download them\n",
    "for model_key, model_url in model_urls.items():\n",
    "    if not os.path.exists(model_path[model_key]):\n",
    "        download_file(model_url, model_path[model_key])\n",
    "        print(f\"{model_key} model downloaded.\")\n",
    "    else:\n",
    "        print(f\"{model_key} model already exists. No download needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85190a-f57e-41c8-8d4a-3ddc658ae045",
   "metadata": {},
   "source": [
    "# RIMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad179715-7c6f-455b-aea4-7b7860430c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RIM, Device: cuda\n",
      "Configuration: {'cuda': True, 'epochs': 200, 'batch_size': 64, 'hidden_size': 600, 'input_size': 1, 'model': 'RIM', 'train': False, 'num_units': 6, 'rnn_cell': 'LSTM', 'key_size_input': 64, 'value_size_input': 400, 'query_size_input': 64, 'num_input_heads': 1, 'num_comm_heads': 4, 'input_dropout': 0.1, 'comm_dropout': 0.1, 'key_size_comm': 32, 'value_size_comm': 100, 'query_size_comm': 32, 'k': 4, 'size': 14, 'loadsaved': 1, 'log_dir': 'rim_model_dir'}\n",
      "Total validation samples: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:46<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total validation samples: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:29<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total validation samples: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:20<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set 1 Accuracy: 72.90%\n",
      "Validation Set 2 Accuracy: 80.65%\n",
      "Validation Set 3 Accuracy: 92.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "config = {\n",
    "    'cuda': True,\n",
    "    'epochs': 200,\n",
    "    'batch_size': 64,\n",
    "    'hidden_size': 600,\n",
    "    'input_size': 1,\n",
    "    'model': 'RIM', # Or 'RIM' for the MnistModel\n",
    "    'train': False, # Set to False to load the saved model\n",
    "    'num_units': 6,\n",
    "    'rnn_cell': 'LSTM',\n",
    "    'key_size_input': 64,\n",
    "    'value_size_input': 400,\n",
    "    'query_size_input': 64,\n",
    "    'num_input_heads': 1,\n",
    "    'num_comm_heads': 4,\n",
    "    'input_dropout': 0.1,\n",
    "    'comm_dropout': 0.1,\n",
    "    'key_size_comm': 32,\n",
    "    'value_size_comm': 100,\n",
    "    'query_size_comm': 32,\n",
    "    'k': 4,\n",
    "    'size': 14,\n",
    "    'loadsaved': 1, # Ensure this is 1 to load saved model\n",
    "    'log_dir': 'rim_model_dir'\n",
    "}\n",
    "\n",
    "# Choose the model\n",
    "model = MnistModel(config)  # Instantiating MnistModel (RIM) with config\n",
    "model_directory = model_path['RIM']\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Eval\n",
    "saved = torch.load(model_directory)\n",
    "model.load_state_dict(saved['net'])\n",
    "\n",
    "# Data\n",
    "data = MnistData(config['batch_size'], (config['size'], config['size']), config['k'])\n",
    "\n",
    "# Evaluation function\n",
    "def test_model(model, loader, func):\n",
    "    accuracy = 0\n",
    "    loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Total validation samples: {loader.val_len()}\")  # Print total number of validation samples\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(loader.val_len())):\n",
    "            test_x, test_y = func(i)\n",
    "            test_x = model.to_device(test_x)\n",
    "            test_y = model.to_device(test_y).long()\n",
    "            probs  = model(test_x)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            correct = preds == test_y\n",
    "            accuracy += correct.sum().item()\n",
    "            \n",
    "    accuracy /= loader.val_len()  # Use the total number of items in the validation set for accuracy calculation\n",
    "    return accuracy\n",
    "\n",
    "    \n",
    "# Evaluate on all three validation sets\n",
    "validation_functions = [data.val_get1, data.val_get2, data.val_get3]\n",
    "validation_accuracies = []\n",
    "\n",
    "print(f\"Model: {config['model']}, Device: {device}\")\n",
    "print(f\"Configuration: {config}\")\n",
    "\n",
    "for func in validation_functions:\n",
    "    accuracy = test_model(model, data, func)\n",
    "    validation_accuracies.append(accuracy)\n",
    "\n",
    "# Print accuracies for all validation sets\n",
    "for i, accuracy in enumerate(validation_accuracies, 1):\n",
    "    print(f'Validation Set {i} Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6303ab3d-10e3-4f17-98cd-d1cc8bec2244",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08ceb84b-e550-48ed-a612-a5e5155ebf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LSTM, Device: cuda\n",
      "Configuration: {'cuda': True, 'epochs': 200, 'batch_size': 64, 'hidden_size': 600, 'input_size': 1, 'model': 'LSTM', 'train': False, 'num_units': 6, 'rnn_cell': 'LSTM', 'key_size_input': 64, 'value_size_input': 400, 'query_size_input': 64, 'num_input_heads': 1, 'num_comm_heads': 4, 'input_dropout': 0.1, 'comm_dropout': 0.1, 'key_size_comm': 32, 'value_size_comm': 100, 'query_size_comm': 32, 'k': 4, 'size': 14, 'loadsaved': 1, 'log_dir': 'rim_model_dir'}\n",
      "Total validation samples: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:02<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total validation samples: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:01<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total validation samples: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:01<00:00, 15.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set 1 Accuracy: 56.60%\n",
      "Validation Set 2 Accuracy: 71.95%\n",
      "Validation Set 3 Accuracy: 404.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "config = {\n",
    "    'cuda': True,\n",
    "    'epochs': 200,\n",
    "    'batch_size': 64,\n",
    "    'hidden_size': 600,\n",
    "    'input_size': 1,\n",
    "    'model': 'LSTM', \n",
    "    'train': False, # Set to False to load the saved model\n",
    "    'num_units': 6,\n",
    "    'rnn_cell': 'LSTM',\n",
    "    'key_size_input': 64,\n",
    "    'value_size_input': 400,\n",
    "    'query_size_input': 64,\n",
    "    'num_input_heads': 1,\n",
    "    'num_comm_heads': 4,\n",
    "    'input_dropout': 0.1,\n",
    "    'comm_dropout': 0.1,\n",
    "    'key_size_comm': 32,\n",
    "    'value_size_comm': 100,\n",
    "    'query_size_comm': 32,\n",
    "    'k': 4,\n",
    "    'size': 14,\n",
    "    'loadsaved': 1, # Ensure this is 1 to load saved model\n",
    "    'log_dir': 'rim_model_dir'\n",
    "}\n",
    "\n",
    "model = LSTM(config)  # Instantiating LSTM with config\n",
    "model_directory = model_path['LSTM']\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Eval\n",
    "saved = torch.load(model_directory)\n",
    "model.load_state_dict(saved['net'])\n",
    "\n",
    "# Data\n",
    "data = MnistData(config['batch_size'], (config['size'], config['size']), config['k'])\n",
    "\n",
    "# Evaluation function\n",
    "def test_model(model, loader, func):\n",
    "    accuracy = 0\n",
    "    loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Total validation samples: {loader.val_len()}\")  # Print total number of validation samples\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(loader.val_len())):\n",
    "            test_x, test_y = func(i)\n",
    "            test_x = model.to_device(test_x)\n",
    "            test_y = model.to_device(test_y).long()\n",
    "            probs  = model(test_x)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            correct = preds == test_y\n",
    "            accuracy += correct.sum().item()\n",
    "            \n",
    "    accuracy /= loader.val_len()  # Use the total number of items in the validation set for accuracy calculation\n",
    "    return accuracy\n",
    "\n",
    "    \n",
    "# Evaluate on all three validation sets\n",
    "validation_functions = [data.val_get1, data.val_get2, data.val_get3]\n",
    "validation_accuracies = []\n",
    "\n",
    "print(f\"Model: {config['model']}, Device: {device}\")\n",
    "print(f\"Configuration: {config}\")\n",
    "\n",
    "for func in validation_functions:\n",
    "    accuracy = test_model(model, data, func)\n",
    "    validation_accuracies.append(accuracy)\n",
    "\n",
    "# Print accuracies for all validation sets\n",
    "for i, accuracy in enumerate(validation_accuracies, 1):\n",
    "    print(f'Validation Set {i} Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45788f3e-bcba-4a40-8836-833d1d06803d",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Global Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ea49e-f052-42c0-8b1a-c37135584646",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As we have seen, deep learning has shifted towards structured models with specialized modules that enhance scalability and generalization. But we can go one step further. Inspired by the 1980s AI focus on modular architectures and the Global Workspace Theory from cognitive neuroscience, the approach we are going to analyse in this section employs a shared global workspace for module coordination. It promotes flexibility and systematic generalization by allowing dynamic interactions among specialized modules. This model emphasizes the importance of having a number of sparsely communicating specialist modules interact via a shared working memory, aiming to achieve coherent and efficient behavior across the system. \n",
    "\n",
    "RIMs leverage a self-attention mechanism to enable information sharing among specialist modules, traditionally through pairwise interactions where each module attends to every other. This new approach, however, introduces a shared workspace with limited capacity to streamline this process. At each computational step, specialist modules compete for the opportunity to write to this shared workspace. Subsequently, the information stored in the workspace is broadcasted to all specialists simultaneously, enhancing coordination and information flow among the modules without the need for direct pairwise communication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d22f777-19ce-4858-a1dd-93d98bbf9035",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise: Creating a Shared Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf09def-0d69-4c45-b418-0d956789de07",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Specialists compete to write their information into the shared workspace. This process is guided by a key-query-value attention mechanism, where the competition is realized through attention scores determining which specialists' information is most critical to be updated in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2380f39e-ec9f-4981-b0c0-2531f1730db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)  # Ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73b534-831e-4e4f-b268-02077f8599bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedWorkspace(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_specialists, hidden_dim, num_memory_slots, memory_slot_dim):\n",
    "        #################################################\n",
    "        ## TODO for students: fill in the missing variables ##\n",
    "        # Fill out function and remove\n",
    "        raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "        #################################################\n",
    "        super().__init__()\n",
    "        self.num_specialists = num_specialists\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_memory_slots = num_memory_slots\n",
    "        self.memory_slot_dim = memory_slot_dim\n",
    "        self.workspace_memory = nn.Parameter(torch.randn(num_memory_slots, memory_slot_dim))\n",
    "        \n",
    "        # Attention mechanism components for writing to the workspace\n",
    "        self.key = ...\n",
    "        self.query = ...\n",
    "        self.value = nn.Linear(hidden_dim, memory_slot_dim)\n",
    "    \n",
    "    def write_to_workspace(self, specialists_states):\n",
    "        #################################################\n",
    "        ## TODO for students: fill in the missing variables ##\n",
    "        # Fill out function and remove\n",
    "        raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "        #################################################\n",
    "        # Flatten specialists' states if they're not already\n",
    "        specialists_states = specialists_states.view(-1, self.hidden_dim)\n",
    "        \n",
    "        # Compute key, query, and value\n",
    "        keys = self.key(specialists_states)\n",
    "        query = self.query(self.workspace_memory)\n",
    "        values = self.value(specialists_states)\n",
    "        \n",
    "        # Compute attention scores and apply softmax\n",
    "        attention_scores = torch.matmul(query, keys.transpose(-2, -1)) / (self.memory_slot_dim ** 0.5)\n",
    "        attention_probs = ...\n",
    "        \n",
    "        # Update workspace memory with weighted sum of values\n",
    "        updated_memory = torch.matmul(attention_probs, values)\n",
    "        self.workspace_memory = nn.Parameter(updated_memory)\n",
    "        \n",
    "        return self.workspace_memory\n",
    "\n",
    "    def forward(self, specialists_states):\n",
    "        #################################################\n",
    "        ## TODO for students: fill in the missing variables ##\n",
    "        # Fill out function and remove\n",
    "        raise NotImplementedError(\"Student exercise: fill in the missing variables\")\n",
    "        #################################################\n",
    "        updated_memory = ...\n",
    "        return updated_memory\n",
    "\n",
    "# Example parameters\n",
    "num_specialists = 5\n",
    "hidden_dim = 10\n",
    "num_memory_slots = 4\n",
    "memory_slot_dim = 6\n",
    "\n",
    "# Generate deterministic specialists' states\n",
    "specialists_states = torch.randn(num_specialists, hidden_dim)\n",
    "\n",
    "# Uncomment the code below to test your function\n",
    "# workspace = SharedWorkspace(num_specialists, hidden_dim, num_memory_slots, memory_slot_dim)\n",
    "# expected_output = workspace.forward(specialists_states)\n",
    "# print(\"Expected Output:\", expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d1740-cc8b-4246-b566-e725e52e54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove solution\n",
    "\n",
    "class SharedWorkspace(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_specialists, hidden_dim, num_memory_slots, memory_slot_dim):\n",
    "        super().__init__()\n",
    "        self.num_specialists = num_specialists\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_memory_slots = num_memory_slots\n",
    "        self.memory_slot_dim = memory_slot_dim\n",
    "        self.workspace_memory = nn.Parameter(torch.randn(num_memory_slots, memory_slot_dim))\n",
    "\n",
    "        # Attention mechanism components for writing to the workspace\n",
    "        self.key = nn.Linear(hidden_dim, memory_slot_dim)\n",
    "        self.query = nn.Linear(memory_slot_dim, memory_slot_dim)\n",
    "        self.value = nn.Linear(hidden_dim, memory_slot_dim)\n",
    "\n",
    "    def write_to_workspace(self, specialists_states):\n",
    "        # Flatten specialists' states if they're not already\n",
    "        specialists_states = specialists_states.view(-1, self.hidden_dim)\n",
    "\n",
    "        # Compute key, query, and value\n",
    "        keys = self.key(specialists_states)\n",
    "        query = self.query(self.workspace_memory)\n",
    "        values = self.value(specialists_states)\n",
    "\n",
    "        # Compute attention scores and apply softmax\n",
    "        attention_scores = torch.matmul(query, keys.transpose(-2, -1)) / (self.memory_slot_dim ** 0.5)\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Update workspace memory with weighted sum of values\n",
    "        updated_memory = torch.matmul(attention_probs, values)\n",
    "        self.workspace_memory = nn.Parameter(updated_memory)\n",
    "\n",
    "        return self.workspace_memory\n",
    "\n",
    "    def forward(self, specialists_states):\n",
    "        updated_memory = self.write_to_workspace(specialists_states)\n",
    "        return updated_memory\n",
    "\n",
    "# Example parameters\n",
    "num_specialists = 5\n",
    "hidden_dim = 10\n",
    "num_memory_slots = 4\n",
    "memory_slot_dim = 6\n",
    "\n",
    "# Generate deterministic specialists' states\n",
    "specialists_states = torch.randn(num_specialists, hidden_dim)\n",
    "\n",
    "# Uncomment the code below to test your function\n",
    "# workspace = SharedWorkspace(num_specialists, hidden_dim, num_memory_slots, memory_slot_dim)\n",
    "# expected_output = workspace.forward(specialists_states)\n",
    "# print(\"Expected Output:\", expected_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed9ace-b3de-4fdc-8996-664dd8e4b3a1",
   "metadata": {
    "execution": {}
   },
   "source": [
    "After updating the shared workspace with the most critical signals, this information is then broadcast back to all specialists. Each specialist updates its state using this broadcast information, which can involve an attention mechanism for consolidation and an update function (like an LSTM or GRU step) based on the new combined state. Let's add this method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03836253-ec99-490b-9921-db1395e5c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_from_workspace(self, specialists_states):\n",
    "    # Broadcast updated memory to specialists\n",
    "    broadcast_query = self.query(specialists_states).view(self.num_specialists, -1, self.memory_slot_dim)\n",
    "    broadcast_keys = self.key(self.workspace_memory).unsqueeze(0).repeat(self.num_specialists, 1, 1)\n",
    "\n",
    "    # Compute attention scores for broadcasting\n",
    "    broadcast_attention_scores = torch.matmul(broadcast_query, broadcast_keys.transpose(-2, -1)) / (self.memory_slot_dim ** 0.5)\n",
    "    broadcast_attention_probs = F.softmax(broadcast_attention_scores, dim=-1)\n",
    "\n",
    "    # Update specialists' states with attention-weighted memory information\n",
    "    broadcast_values = self.value(self.workspace_memory).unsqueeze(0).repeat(self.num_specialists, 1, 1)\n",
    "    updated_states = torch.matmul(broadcast_attention_probs, broadcast_values)\n",
    "\n",
    "    return updated_states.view_as(specialists_states)\n",
    "\n",
    "# Assign the method to the class\n",
    "SharedWorkspace.broadcast_from_workspace = broadcast_from_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc221d3-530e-4835-b542-905ce3e281bf",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This approach modularizes the shared workspace functionality, ensuring the specialists' states are first aggregated in a competitive manner into the workspace, followed by an efficient distribution of this consolidated information. This mechanism allows for dynamic filtering based on the current context and enhances the model's ability to generalize from past experiences by focusing on the most relevant signals at each computational step. To integrate this into a full system, you would need to instantiate this SharedWorkspace within your RIM architecture, ensuring that the initial representations of specialists are processed (Step 1), passed to the SharedWorkspace for competition and update (Step 2), and then the updated information is broadcast back to the specialists (Step3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f71074b-70b5-4446-bf86-00e9f781ecd7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: a toy model for illustrating GNW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070b46f-51c0-4416-80ee-728a32f75cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGNWModel:\n",
    "    def __init__(self, num_nodes=5):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.network = nx.erdos_renyi_graph(n=num_nodes, p=0.5)\n",
    "        self.activations = {node: False for node in self.network.nodes}\n",
    "\n",
    "    def activate_node(self):\n",
    "        selected_node = random.choice(list(self.network.nodes))\n",
    "        self.activations[selected_node] = True\n",
    "\n",
    "        # Simulate global broadcast\n",
    "        for neighbor in self.network.neighbors(selected_node):\n",
    "            self.activations[neighbor] = True\n",
    "\n",
    "    def reset_activations(self):\n",
    "        self.activations = {node: False for node in self.network.nodes}\n",
    "\n",
    "    def draw_network(self):\n",
    "        color_map = ['green' if self.activations[node] else 'red' for node in self.network.nodes]\n",
    "        nx.draw(self.network, node_color=color_map, with_labels=True, node_size=700)\n",
    "        plt.show()\n",
    "\n",
    "# Create a GNW model instance\n",
    "gnw_model = SimpleGNWModel()\n",
    "\n",
    "# Button to activate a node\n",
    "activate_button = widgets.Button(description='Activate Node')\n",
    "\n",
    "# Button to reset activations\n",
    "reset_button = widgets.Button(description='Reset')\n",
    "\n",
    "# Output area for the network graph\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_activate_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output(wait=True)\n",
    "        gnw_model.activate_node()\n",
    "        gnw_model.draw_network()\n",
    "\n",
    "def on_reset_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output(wait=True)\n",
    "        gnw_model.reset_activations()\n",
    "        gnw_model.draw_network()\n",
    "\n",
    "activate_button.on_click(on_activate_clicked)\n",
    "reset_button.on_click(on_reset_clicked)\n",
    "\n",
    "display(widgets.VBox([activate_button, reset_button, output_area]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa4601-c636-4999-9b2d-70f1d85057ab",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 4: TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343057cd-59dd-4410-86ba-b3b7fa1a719a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D5_Tutorial1",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
